<!DOCTYPE html><html><head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Robotics Dataset Comparison - Cyber Nachos</title>
<style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }/*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{border:0 solid #e5e7eb;box-sizing:border-box}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-tap-highlight-color:transparent}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-size:1em;font-variation-settings:normal}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{color:inherit;font-family:inherit;font-feature-settings:inherit;font-size:100%;font-variation-settings:inherit;font-weight:inherit;letter-spacing:inherit;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]:where(:not([hidden=until-found])){display:none}:root{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--primary:221.2 83.2% 53.3%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--ring:221.2 83.2% 53.3%;--radius:.5rem}.dark{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--primary:217.2 91.2% 59.8%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--ring:224.3 76.3% 48%}*{border-color:hsl(var(--border))}body{background-color:hsl(var(--background));color:hsl(var(--foreground))}.container{margin-left:auto;margin-right:auto;padding-left:2rem;padding-right:2rem;width:100%}@media (min-width:1400px){.container{max-width:1400px}}.sr-only{height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border-width:0;white-space:nowrap}.pointer-events-none{pointer-events:none}.pointer-events-auto{pointer-events:auto}.visible{visibility:visible}.\!collapse{visibility:collapse!important}.collapse{visibility:collapse}.static{position:static}.fixed{position:fixed}.absolute{position:absolute}.relative{position:relative}.sticky{position:sticky}.inset-0{top:0;right:0;bottom:0;left:0}.inset-x-0{left:0;right:0}.inset-y-0{bottom:0;top:0}.-top-12{top:-3rem}.bottom-0{bottom:0}.left-0{left:0}.left-1\/2{left:50%}.right-0{right:0}.right-1{right:.25rem}.right-2{right:.5rem}.right-3{right:.75rem}.right-4{right:1rem}.top-0{top:0}.top-1{top:.25rem}.top-1\/2{top:50%}.top-2{top:.5rem}.top-3{top:.75rem}.top-4{top:1rem}.top-\[102px\]{top:102px}.top-\[60\%\]{top:60%}.top-\[90px\]{top:90px}.top-full{top:100%}.top-px{top:1px}.z-10{z-index:10}.z-30{z-index:30}.z-40{z-index:40}.z-50{z-index:50}.z-\[100\]{z-index:100}.z-\[1\]{z-index:1}.order-first{order:-9999}.order-last{order:9999}.col-span-2{grid-column:span 2/span 2}.m-0{margin:0}.-mx-1{margin-left:-.25rem;margin-right:-.25rem}.-mx-4{margin-left:-1rem;margin-right:-1rem}.mx-0\.5{margin-left:.125rem;margin-right:.125rem}.mx-2{margin-left:.5rem;margin-right:.5rem}.mx-3\.5{margin-left:.875rem;margin-right:.875rem}.mx-4{margin-left:1rem;margin-right:1rem}.mx-auto{margin-left:auto;margin-right:auto}.my-8{margin-bottom:2rem;margin-top:2rem}.-ml-2{margin-left:-.5rem}.mb-0{margin-bottom:0}.mb-1{margin-bottom:.25rem}.mb-2{margin-bottom:.5rem}.mb-3{margin-bottom:.75rem}.mb-4{margin-bottom:1rem}.mb-5{margin-bottom:1.25rem}.mb-6{margin-bottom:1.5rem}.mb-\[2px\]{margin-bottom:2px}.ml-1{margin-left:.25rem}.ml-2{margin-left:.5rem}.ml-3{margin-left:.75rem}.ml-4{margin-left:1rem}.ml-6{margin-left:1.5rem}.ml-auto{margin-left:auto}.mr-1{margin-right:.25rem}.mr-1\.5{margin-right:.375rem}.mr-2{margin-right:.5rem}.mr-3{margin-right:.75rem}.mr-4{margin-right:1rem}.mr-auto{margin-right:auto}.mt-0{margin-top:0}.mt-0\.5{margin-top:.125rem}.mt-1{margin-top:.25rem}.mt-1\.5{margin-top:.375rem}.mt-16{margin-top:4rem}.mt-2{margin-top:.5rem}.mt-3{margin-top:.75rem}.mt-4{margin-top:1rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.inline-flex{display:inline-flex}.table{display:table}.grid{display:grid}.contents{display:contents}.hidden{display:none}.aspect-\[4\/3\]{aspect-ratio:4/3}.size-10{height:2.5rem;width:2.5rem}.size-16{height:4rem;width:4rem}.size-2{height:.5rem;width:.5rem}.size-3{height:.75rem;width:.75rem}.size-3\.5{height:.875rem;width:.875rem}.size-32{height:8rem;width:8rem}.size-4{height:1rem;width:1rem}.size-5{height:1.25rem;width:1.25rem}.size-6{height:1.5rem;width:1.5rem}.size-8{height:2rem;width:2rem}.size-full{height:100%;width:100%}.h-1\.5{height:.375rem}.h-10{height:2.5rem}.h-11{height:2.75rem}.h-12{height:3rem}.h-14{height:3.5rem}.h-2{height:.5rem}.h-2\.5{height:.625rem}.h-3{height:.75rem}.h-4{height:1rem}.h-48{height:12rem}.h-5{height:1.25rem}.h-6{height:1.5rem}.h-7{height:1.75rem}.h-8{height:2rem}.h-9{height:2.25rem}.h-\[--radix-navigation-menu-viewport-height\]{height:var(--radix-navigation-menu-viewport-height)}.h-\[calc\(100vh-3\.5rem\)\]{height:calc(100vh - 3.5rem)}.h-\[calc\(100vh-6\.5rem\)\]{height:calc(100vh - 6.5rem)}.h-full{height:100%}.h-px{height:1px}.h-svh{height:100svh}.max-h-screen{max-height:100vh}.min-h-4{min-height:1rem}.min-h-5{min-height:1.25rem}.min-h-6{min-height:1.5rem}.min-h-screen{min-height:100vh}.w-10{width:2.5rem}.w-2\.5{width:.625rem}.w-24{width:6rem}.w-3{width:.75rem}.w-3\/4{width:75%}.w-4{width:1rem}.w-44{width:11rem}.w-5{width:1.25rem}.w-6{width:1.5rem}.w-72{width:18rem}.w-8{width:2rem}.w-9{width:2.25rem}.w-\[200px\]{width:200px}.w-\[23rem\]{width:23rem}.w-\[250px\]{width:250px}.w-\[var\(--dropdown-width\)\]{width:var(--dropdown-width)}.w-auto{width:auto}.w-fit{width:-moz-fit-content;width:fit-content}.w-full{width:100%}.w-max{width:-moz-max-content;width:max-content}.w-px{width:1px}.min-w-0{min-width:0}.min-w-4{min-width:1rem}.min-w-5{min-width:1.25rem}.min-w-6{min-width:1.5rem}.max-w-2xl{max-width:42rem}.max-w-\[750px\]{max-width:750px}.max-w-\[980px\]{max-width:980px}.max-w-lg{max-width:32rem}.max-w-max{max-width:-moz-max-content;max-width:max-content}.max-w-md{max-width:28rem}.max-w-screen-2xl{max-width:1536px}.flex-1{flex:1 1 0%}.shrink{flex-shrink:1}.shrink-0{flex-shrink:0}.flex-grow,.grow{flex-grow:1}.basis-1\/3{flex-basis:33.333333%}.-translate-x-1\/2{--tw-translate-x:-50%}.-translate-x-1\/2,.-translate-y-1\/2{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.-translate-y-1\/2{--tw-translate-y:-50%}.-translate-y-4{--tw-translate-y:-1rem}.-translate-y-4,.translate-y-0{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.translate-y-0{--tw-translate-y:0px}.translate-y-12{--tw-translate-y:3rem}.translate-y-12,.translate-y-8{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.translate-y-8{--tw-translate-y:2rem}.-rotate-90{--tw-rotate:-90deg}.-rotate-90,.rotate-0{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.rotate-0{--tw-rotate:0deg}.rotate-180{--tw-rotate:180deg}.rotate-180,.rotate-45{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.rotate-45{--tw-rotate:45deg}.rotate-90{--tw-rotate:90deg}.rotate-90,.scale-0{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.scale-0{--tw-scale-x:0;--tw-scale-y:0}.scale-100{--tw-scale-x:1;--tw-scale-y:1}.scale-100,.transform{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@keyframes bounce{0%,to{animation-timing-function:cubic-bezier(.8,0,1,1);transform:translateY(-25%)}50%{animation-timing-function:cubic-bezier(0,0,.2,1);transform:none}}.animate-bounce{animation:bounce 1s infinite}@keyframes pulse{50%{opacity:.5}}.animate-pulse{animation:pulse 2s cubic-bezier(.4,0,.6,1) infinite}@keyframes spin{to{transform:rotate(1turn)}}.animate-spin{animation:spin 1s linear infinite}.cursor-default{cursor:default}.cursor-pointer{cursor:pointer}.touch-none{touch-action:none}.select-none{-webkit-user-select:none;-moz-user-select:none;user-select:none}.resize{resize:both}.scroll-m-20{scroll-margin:5rem}.list-decimal{list-style-type:decimal}.list-disc{list-style-type:disc}.list-none{list-style-type:none}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.grid-cols-5{grid-template-columns:repeat(5,minmax(0,1fr))}.grid-cols-\[repeat\(auto-fit\,_minmax\(270px\,_1fr\)\)\]{grid-template-columns:repeat(auto-fit,minmax(270px,1fr))}.flex-row{flex-direction:row}.flex-col{flex-direction:column}.flex-col-reverse{flex-direction:column-reverse}.flex-wrap{flex-wrap:wrap}.place-items-center{place-items:center}.items-start{align-items:flex-start}.items-end{align-items:flex-end}.items-center{align-items:center}.justify-start{justify-content:flex-start}.justify-end{justify-content:flex-end}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.gap-1{gap:.25rem}.gap-1\.5{gap:.375rem}.gap-2{gap:.5rem}.gap-3{gap:.75rem}.gap-4{gap:1rem}.gap-5{gap:1.25rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.gap-x-1{-moz-column-gap:.25rem;column-gap:.25rem}.gap-y-1\.5{row-gap:.375rem}.gap-y-2{row-gap:.5rem}.space-x-2>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(.5rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(.5rem*var(--tw-space-x-reverse))}.space-x-4>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(1rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(1rem*var(--tw-space-x-reverse))}.space-y-1>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.25rem*var(--tw-space-y-reverse));margin-top:calc(.25rem*(1 - var(--tw-space-y-reverse)))}.space-y-1\.5>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.375rem*var(--tw-space-y-reverse));margin-top:calc(.375rem*(1 - var(--tw-space-y-reverse)))}.space-y-10>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(2.5rem*var(--tw-space-y-reverse));margin-top:calc(2.5rem*(1 - var(--tw-space-y-reverse)))}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.5rem*var(--tw-space-y-reverse));margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)))}.space-y-3>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.75rem*var(--tw-space-y-reverse));margin-top:calc(.75rem*(1 - var(--tw-space-y-reverse)))}.space-y-4>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(1rem*var(--tw-space-y-reverse));margin-top:calc(1rem*(1 - var(--tw-space-y-reverse)))}.space-y-6>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(1.5rem*var(--tw-space-y-reverse));margin-top:calc(1.5rem*(1 - var(--tw-space-y-reverse)))}.divide-x>:not([hidden])~:not([hidden]){--tw-divide-x-reverse:0;border-left-width:calc(1px*(1 - var(--tw-divide-x-reverse)));border-right-width:calc(1px*var(--tw-divide-x-reverse))}.divide-y>:not([hidden])~:not([hidden]){--tw-divide-y-reverse:0;border-bottom-width:calc(1px*var(--tw-divide-y-reverse));border-top-width:calc(1px*(1 - var(--tw-divide-y-reverse)))}.self-center{align-self:center}.overflow-hidden{overflow:hidden}.overflow-clip{overflow:clip}.overflow-x-auto{overflow-x:auto}.overflow-y-auto{overflow-y:auto}.overflow-x-hidden{overflow-x:hidden}.truncate{overflow:hidden;text-overflow:ellipsis}.truncate,.whitespace-nowrap{white-space:nowrap}.text-nowrap{text-wrap:nowrap}.break-words{overflow-wrap:break-word}.rounded{border-radius:.25rem}.rounded-\[inherit\]{border-radius:inherit}.rounded-full{border-radius:9999px}.rounded-lg{border-radius:var(--radius)}.rounded-md{border-radius:calc(var(--radius) - 2px)}.rounded-none{border-radius:0}.rounded-sm{border-radius:calc(var(--radius) - 4px)}.rounded-t-none{border-top-left-radius:0;border-top-right-radius:0}.rounded-tl-sm{border-top-left-radius:calc(var(--radius) - 4px)}.border{border-width:1px}.border-2{border-width:2px}.border-b{border-bottom-width:1px}.border-b-2{border-bottom-width:2px}.border-l{border-left-width:1px}.border-l-2{border-left-width:2px}.border-r{border-right-width:1px}.border-t{border-top-width:1px}.border-t-2{border-top-width:2px}.border-none{border-style:none}.border-\[\#adfa1d\]{--tw-border-opacity:1;border-color:rgb(173 250 29/var(--tw-border-opacity,1))}.border-amber-200\/50{border-color:#fde68b80}.border-amber-500{--tw-border-opacity:1;border-color:rgb(245 158 11/var(--tw-border-opacity,1))}.border-amber-600{--tw-border-opacity:1;border-color:rgb(217 119 6/var(--tw-border-opacity,1))}.border-blue-700{--tw-border-opacity:1;border-color:rgb(29 78 216/var(--tw-border-opacity,1))}.border-border{border-color:hsl(var(--border))}.border-border\/30{border-color:hsl(var(--border)/.3)}.border-border\/40{border-color:hsl(var(--border)/.4)}.border-border\/50{border-color:hsl(var(--border)/.5)}.border-destructive{border-color:hsl(var(--destructive))}.border-destructive\/50{border-color:hsl(var(--destructive)/.5)}.border-green-500{--tw-border-opacity:1;border-color:rgb(34 197 94/var(--tw-border-opacity,1))}.border-green-600{--tw-border-opacity:1;border-color:rgb(22 163 74/var(--tw-border-opacity,1))}.border-input{border-color:hsl(var(--input))}.border-primary{border-color:hsl(var(--primary))}.border-primary\/20{border-color:hsl(var(--primary)/.2)}.border-primary\/30{border-color:hsl(var(--primary)/.3)}.border-red-500{--tw-border-opacity:1;border-color:rgb(239 68 68/var(--tw-border-opacity,1))}.border-red-600{--tw-border-opacity:1;border-color:rgb(220 38 38/var(--tw-border-opacity,1))}.border-sky-500{--tw-border-opacity:1;border-color:rgb(14 165 233/var(--tw-border-opacity,1))}.border-sky-600{--tw-border-opacity:1;border-color:rgb(2 132 199/var(--tw-border-opacity,1))}.border-transparent{border-color:transparent}.border-violet-600{--tw-border-opacity:1;border-color:rgb(124 58 237/var(--tw-border-opacity,1))}.border-b-transparent{border-bottom-color:transparent}.border-l-transparent{border-left-color:transparent}.border-t-transparent{border-top-color:transparent}.bg-\[\#adfa1d\]{--tw-bg-opacity:1;background-color:rgb(173 250 29/var(--tw-bg-opacity,1))}.bg-accent{background-color:hsl(var(--accent))}.bg-amber-500{--tw-bg-opacity:1;background-color:rgb(245 158 11/var(--tw-bg-opacity,1))}.bg-background{background-color:hsl(var(--background))}.bg-background\/80{background-color:hsl(var(--background)/.8)}.bg-black\/50{background-color:#00000080}.bg-black\/80{background-color:#000c}.bg-border{background-color:hsl(var(--border))}.bg-card{background-color:hsl(var(--card))}.bg-destructive{background-color:hsl(var(--destructive))}.bg-green-100{--tw-bg-opacity:1;background-color:rgb(220 252 231/var(--tw-bg-opacity,1))}.bg-green-500{--tw-bg-opacity:1;background-color:rgb(34 197 94/var(--tw-bg-opacity,1))}.bg-muted{background-color:hsl(var(--muted))}.bg-muted\/30{background-color:hsl(var(--muted)/.3)}.bg-muted\/40{background-color:hsl(var(--muted)/.4)}.bg-muted\/50{background-color:hsl(var(--muted)/.5)}.bg-popover{background-color:hsl(var(--popover))}.bg-primary{background-color:hsl(var(--primary))}.bg-primary\/10{background-color:hsl(var(--primary)/.1)}.bg-primary\/5{background-color:hsl(var(--primary)/.05)}.bg-primary\/80{background-color:hsl(var(--primary)/.8)}.bg-red-100{--tw-bg-opacity:1;background-color:rgb(254 226 226/var(--tw-bg-opacity,1))}.bg-red-500{--tw-bg-opacity:1;background-color:rgb(239 68 68/var(--tw-bg-opacity,1))}.bg-secondary{background-color:hsl(var(--secondary))}.bg-sky-500{--tw-bg-opacity:1;background-color:rgb(14 165 233/var(--tw-bg-opacity,1))}.bg-transparent{background-color:transparent}.bg-white{--tw-bg-opacity:1;background-color:rgb(255 255 255/var(--tw-bg-opacity,1))}.bg-white\/40{background-color:#fff6}.bg-zinc-950{--tw-bg-opacity:1;background-color:rgb(9 9 11/var(--tw-bg-opacity,1))}.bg-gradient-to-b{background-image:linear-gradient(to bottom,var(--tw-gradient-stops))}.bg-gradient-to-r{background-image:linear-gradient(to right,var(--tw-gradient-stops))}.from-amber-50{--tw-gradient-from:#fffbeb var(--tw-gradient-from-position);--tw-gradient-to:rgba(255,251,235,0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.from-background{--tw-gradient-from:hsl(var(--background)) var(--tw-gradient-from-position);--tw-gradient-to:hsl(var(--background)/0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.from-muted{--tw-gradient-from:hsl(var(--muted)) var(--tw-gradient-from-position);--tw-gradient-to:hsl(var(--muted)/0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.from-primary{--tw-gradient-from:hsl(var(--primary)) var(--tw-gradient-from-position);--tw-gradient-to:hsl(var(--primary)/0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.from-primary\/10{--tw-gradient-from:hsl(var(--primary)/.1) var(--tw-gradient-from-position);--tw-gradient-to:hsl(var(--primary)/0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.from-primary\/5{--tw-gradient-from:hsl(var(--primary)/.05) var(--tw-gradient-from-position);--tw-gradient-to:hsl(var(--primary)/0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.to-amber-100\/50{--tw-gradient-to:hsla(48,96%,89%,.5) var(--tw-gradient-to-position)}.to-background{--tw-gradient-to:hsl(var(--background)) var(--tw-gradient-to-position)}.to-muted\/30{--tw-gradient-to:hsl(var(--muted)/.3) var(--tw-gradient-to-position)}.to-muted\/40{--tw-gradient-to:hsl(var(--muted)/.4) var(--tw-gradient-to-position)}.to-muted\/50{--tw-gradient-to:hsl(var(--muted)/.5) var(--tw-gradient-to-position)}.to-primary\/90{--tw-gradient-to:hsl(var(--primary)/.9) var(--tw-gradient-to-position)}.object-cover{-o-object-fit:cover;object-fit:cover}.object-\[50\%_53\%\]{-o-object-position:50% 53%;object-position:50% 53%}.object-center{-o-object-position:center;object-position:center}.\!p-2{padding:.5rem!important}.p-0{padding:0}.p-0\.5{padding:.125rem}.p-1{padding:.25rem}.p-1\.5{padding:.375rem}.p-16{padding:4rem}.p-2{padding:.5rem}.p-3{padding:.75rem}.p-4{padding:1rem}.p-5{padding:1.25rem}.p-6{padding:1.5rem}.p-px{padding:1px}.px-0\.5{padding-left:.125rem;padding-right:.125rem}.px-1{padding-left:.25rem;padding-right:.25rem}.px-1\.5{padding-left:.375rem;padding-right:.375rem}.px-2{padding-left:.5rem;padding-right:.5rem}.px-2\.5{padding-left:.625rem;padding-right:.625rem}.px-3{padding-left:.75rem;padding-right:.75rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.px-8{padding-left:2rem;padding-right:2rem}.px-\[0\.3rem\]{padding-left:.3rem;padding-right:.3rem}.py-0\.5{padding-bottom:.125rem;padding-top:.125rem}.py-1{padding-bottom:.25rem;padding-top:.25rem}.py-1\.5{padding-bottom:.375rem;padding-top:.375rem}.py-2{padding-bottom:.5rem;padding-top:.5rem}.py-3{padding-bottom:.75rem;padding-top:.75rem}.py-4{padding-bottom:1rem;padding-top:1rem}.py-6{padding-bottom:1.5rem;padding-top:1.5rem}.py-8{padding-bottom:2rem;padding-top:2rem}.py-\[0\.2rem\]{padding-bottom:.2rem;padding-top:.2rem}.pb-2{padding-bottom:.5rem}.pb-3{padding-bottom:.75rem}.pb-4{padding-bottom:1rem}.pb-5{padding-bottom:1.25rem}.pl-2{padding-left:.5rem}.pl-3{padding-left:.75rem}.pl-4{padding-left:1rem}.pl-5{padding-left:1.25rem}.pl-6{padding-left:1.5rem}.pl-8{padding-left:2rem}.pr-0{padding-right:0}.pr-1\.5{padding-right:.375rem}.pr-3{padding-right:.75rem}.pr-6{padding-right:1.5rem}.pt-0{padding-top:0}.pt-1{padding-top:.25rem}.pt-2{padding-top:.5rem}.pt-4{padding-top:1rem}.pt-5{padding-top:1.25rem}.pt-6{padding-top:1.5rem}.text-left{text-align:left}.text-center{text-align:center}.text-right{text-align:right}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-3xl{font-size:1.875rem;line-height:2.25rem}.text-4xl{font-size:2.25rem;line-height:2.5rem}.text-5xl{font-size:3rem;line-height:1}.text-8xl{font-size:6rem;line-height:1}.text-\[10px\]{font-size:10px}.text-\[11px\]{font-size:11px}.text-\[12px\]{font-size:12px}.text-base{font-size:1rem;line-height:1.5rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xl{font-size:1.25rem;line-height:1.75rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-extrabold{font-weight:800}.font-light{font-weight:300}.font-medium{font-weight:500}.font-normal{font-weight:400}.font-semibold{font-weight:600}.uppercase{text-transform:uppercase}.lowercase{text-transform:lowercase}.capitalize{text-transform:capitalize}.italic{font-style:italic}.leading-4{line-height:1rem}.leading-7{line-height:1.75rem}.leading-none{line-height:1}.leading-relaxed{line-height:1.625}.leading-tight{line-height:1.25}.tracking-tight{letter-spacing:-.025em}.tracking-tighter{letter-spacing:-.05em}.\!text-primary{color:hsl(var(--primary))!important}.text-accent-foreground{color:hsl(var(--accent-foreground))}.text-amber-500{--tw-text-opacity:1;color:rgb(245 158 11/var(--tw-text-opacity,1))}.text-amber-600{--tw-text-opacity:1;color:rgb(217 119 6/var(--tw-text-opacity,1))}.text-black{--tw-text-opacity:1;color:rgb(0 0 0/var(--tw-text-opacity,1))}.text-blue-700{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity,1))}.text-card-foreground{color:hsl(var(--card-foreground))}.text-current{color:currentColor}.text-destructive{color:hsl(var(--destructive))}.text-destructive-foreground{color:hsl(var(--destructive-foreground))}.text-foreground{color:hsl(var(--foreground))}.text-foreground\/50{color:hsl(var(--foreground)/.5)}.text-foreground\/70{color:hsl(var(--foreground)/.7)}.text-foreground\/80{color:hsl(var(--foreground)/.8)}.text-gray-500{--tw-text-opacity:1;color:rgb(107 114 128/var(--tw-text-opacity,1))}.text-green-500{--tw-text-opacity:1;color:rgb(34 197 94/var(--tw-text-opacity,1))}.text-green-600{--tw-text-opacity:1;color:rgb(22 163 74/var(--tw-text-opacity,1))}.text-muted-foreground{color:hsl(var(--muted-foreground))}.text-muted-foreground\/80{color:hsl(var(--muted-foreground)/.8)}.text-popover-foreground{color:hsl(var(--popover-foreground))}.text-primary{color:hsl(var(--primary))}.text-primary-foreground{color:hsl(var(--primary-foreground))}.text-primary\/40{color:hsl(var(--primary)/.4)}.text-primary\/70{color:hsl(var(--primary)/.7)}.text-primary\/80{color:hsl(var(--primary)/.8)}.text-red-500{--tw-text-opacity:1;color:rgb(239 68 68/var(--tw-text-opacity,1))}.text-red-600{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity,1))}.text-secondary-foreground{color:hsl(var(--secondary-foreground))}.text-sky-500{--tw-text-opacity:1;color:rgb(14 165 233/var(--tw-text-opacity,1))}.text-sky-600{--tw-text-opacity:1;color:rgb(2 132 199/var(--tw-text-opacity,1))}.text-violet-600{--tw-text-opacity:1;color:rgb(124 58 237/var(--tw-text-opacity,1))}.text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.text-zinc-100{--tw-text-opacity:1;color:rgb(244 244 245/var(--tw-text-opacity,1))}.text-zinc-400{--tw-text-opacity:1;color:rgb(161 161 170/var(--tw-text-opacity,1))}.text-zinc-500{--tw-text-opacity:1;color:rgb(113 113 122/var(--tw-text-opacity,1))}.text-zinc-900{--tw-text-opacity:1;color:rgb(24 24 27/var(--tw-text-opacity,1))}.underline{text-decoration-line:underline}.no-underline{text-decoration-line:none}.underline-offset-4{text-underline-offset:4px}.opacity-0{opacity:0}.opacity-100{opacity:1}.opacity-50{opacity:.5}.opacity-70{opacity:.7}.opacity-90{opacity:.9}.shadow-inner{--tw-shadow:inset 0 2px 4px 0 rgba(0,0,0,.05);--tw-shadow-colored:inset 0 2px 4px 0 var(--tw-shadow-color)}.shadow-inner,.shadow-lg{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-lg{--tw-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -4px rgba(0,0,0,.1);--tw-shadow-colored:0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color)}.shadow-md{--tw-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -2px rgba(0,0,0,.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color)}.shadow-md,.shadow-none{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-none{--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000}.shadow-sm{--tw-shadow:0 1px 2px 0 rgba(0,0,0,.05);--tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color)}.shadow-sm,.shadow-xl{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-xl{--tw-shadow:0 20px 25px -5px rgba(0,0,0,.1),0 8px 10px -6px rgba(0,0,0,.1);--tw-shadow-colored:0 20px 25px -5px var(--tw-shadow-color),0 8px 10px -6px var(--tw-shadow-color)}.outline-none{outline:2px solid transparent;outline-offset:2px}.outline{outline-style:solid}.ring{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(3px + var(--tw-ring-offset-width)) var(--tw-ring-color);box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.ring-offset-background{--tw-ring-offset-color:hsl(var(--background))}.grayscale{--tw-grayscale:grayscale(100%)}.filter,.grayscale{filter:var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow)}.backdrop-blur-lg{--tw-backdrop-blur:blur(16px);-webkit-backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia);backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia)}.transition{transition-duration:.15s;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-all{transition-duration:.15s;transition-property:all;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-colors{transition-duration:.15s;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-none{transition-property:none}.transition-opacity{transition-duration:.15s;transition-property:opacity;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-shadow{transition-duration:.15s;transition-property:box-shadow;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-transform{transition-duration:.15s;transition-property:transform;transition-timing-function:cubic-bezier(.4,0,.2,1)}.duration-200{transition-duration:.2s}.duration-300{transition-duration:.3s}.duration-500{transition-duration:.5s}.duration-700{transition-duration:.7s}.duration-75{transition-duration:75ms}.ease-in{transition-timing-function:cubic-bezier(.4,0,1,1)}.ease-in-out{transition-timing-function:cubic-bezier(.4,0,.2,1)}.ease-out{transition-timing-function:cubic-bezier(0,0,.2,1)}@keyframes enter{0%{opacity:var(--tw-enter-opacity,1);transform:translate3d(var(--tw-enter-translate-x,0),var(--tw-enter-translate-y,0),0) scale3d(var(--tw-enter-scale,1),var(--tw-enter-scale,1),var(--tw-enter-scale,1)) rotate(var(--tw-enter-rotate,0))}}@keyframes exit{to{opacity:var(--tw-exit-opacity,1);transform:translate3d(var(--tw-exit-translate-x,0),var(--tw-exit-translate-y,0),0) scale3d(var(--tw-exit-scale,1),var(--tw-exit-scale,1),var(--tw-exit-scale,1)) rotate(var(--tw-exit-rotate,0))}}.duration-200{animation-duration:.2s}.duration-300{animation-duration:.3s}.duration-500{animation-duration:.5s}.duration-700{animation-duration:.7s}.duration-75{animation-duration:75ms}.ease-in{animation-timing-function:cubic-bezier(.4,0,1,1)}.ease-in-out{animation-timing-function:cubic-bezier(.4,0,.2,1)}.ease-out{animation-timing-function:cubic-bezier(0,0,.2,1)}.running{animation-play-state:running}.paused{animation-play-state:paused}.step{counter-increment:step}.step:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[counter-reset\:step\]{counter-reset:step}.\[deg_idx\:recovery_idx\]{deg_idx:recovery idx}.\[failure_idx\:recovery_idx\]{failure_idx:recovery idx}.\[i-50\:i\]{i-50:i}.\[i\:i\+2\]{i:i+2}.\[i\:i\+8\]{i:i+8}.\[i\:i\+batch_size\]{i:i+batch size}.\[i\:i\+window_samples\]{i:i+window samples}.\[start_idx\:end_idx\]{start_idx:end idx}.\[window_start\:window_end\]{window_start:window end}.file\:border-0::file-selector-button{border-width:0}.file\:bg-transparent::file-selector-button{background-color:transparent}.file\:text-sm::file-selector-button{font-size:.875rem;line-height:1.25rem}.file\:font-medium::file-selector-button{font-weight:500}.placeholder\:text-muted-foreground::-moz-placeholder{color:hsl(var(--muted-foreground))}.placeholder\:text-muted-foreground::placeholder{color:hsl(var(--muted-foreground))}.even\:bg-muted\/50:nth-child(2n){background-color:hsl(var(--muted)/.5)}.focus-within\:shadow-md:focus-within{--tw-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -2px rgba(0,0,0,.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color);box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.hover\:scale-\[1\.01\]:hover{--tw-scale-x:1.01;--tw-scale-y:1.01}.hover\:scale-\[1\.01\]:hover,.hover\:scale-\[1\.02\]:hover{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.hover\:scale-\[1\.02\]:hover{--tw-scale-x:1.02;--tw-scale-y:1.02}.hover\:cursor-pointer:hover{cursor:pointer}.hover\:bg-\[\#adfa1d\]:hover{--tw-bg-opacity:1;background-color:rgb(173 250 29/var(--tw-bg-opacity,1))}.hover\:bg-accent:hover{background-color:hsl(var(--accent))}.hover\:bg-amber-400:hover{--tw-bg-opacity:1;background-color:rgb(251 191 36/var(--tw-bg-opacity,1))}.hover\:bg-destructive\/80:hover{background-color:hsl(var(--destructive)/.8)}.hover\:bg-destructive\/90:hover{background-color:hsl(var(--destructive)/.9)}.hover\:bg-green-400:hover{--tw-bg-opacity:1;background-color:rgb(74 222 128/var(--tw-bg-opacity,1))}.hover\:bg-muted:hover{background-color:hsl(var(--muted))}.hover\:bg-muted\/40:hover{background-color:hsl(var(--muted)/.4)}.hover\:bg-muted\/50:hover{background-color:hsl(var(--muted)/.5)}.hover\:bg-primary\/10:hover{background-color:hsl(var(--primary)/.1)}.hover\:bg-primary\/80:hover{background-color:hsl(var(--primary)/.8)}.hover\:bg-primary\/90:hover{background-color:hsl(var(--primary)/.9)}.hover\:bg-red-400:hover{--tw-bg-opacity:1;background-color:rgb(248 113 113/var(--tw-bg-opacity,1))}.hover\:bg-secondary:hover{background-color:hsl(var(--secondary))}.hover\:bg-secondary\/80:hover{background-color:hsl(var(--secondary)/.8)}.hover\:bg-sky-400:hover{--tw-bg-opacity:1;background-color:rgb(56 189 248/var(--tw-bg-opacity,1))}.hover\:from-primary\/90:hover{--tw-gradient-from:hsl(var(--primary)/.9) var(--tw-gradient-from-position);--tw-gradient-to:hsl(var(--primary)/0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.hover\:to-primary:hover{--tw-gradient-to:hsl(var(--primary)) var(--tw-gradient-to-position)}.hover\:text-accent-foreground:hover{color:hsl(var(--accent-foreground))}.hover\:text-foreground:hover{color:hsl(var(--foreground))}.hover\:text-primary:hover{color:hsl(var(--primary))}.hover\:underline:hover{text-decoration-line:underline}.hover\:opacity-100:hover{opacity:1}.hover\:shadow:hover{--tw-shadow:0 1px 3px 0 rgba(0,0,0,.1),0 1px 2px -1px rgba(0,0,0,.1);--tw-shadow-colored:0 1px 3px 0 var(--tw-shadow-color),0 1px 2px -1px var(--tw-shadow-color)}.hover\:shadow-2xl:hover,.hover\:shadow:hover{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.hover\:shadow-2xl:hover{--tw-shadow:0 25px 50px -12px rgba(0,0,0,.25);--tw-shadow-colored:0 25px 50px -12px var(--tw-shadow-color)}.hover\:shadow-lg:hover{--tw-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -4px rgba(0,0,0,.1);--tw-shadow-colored:0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color)}.hover\:shadow-lg:hover,.hover\:shadow-md:hover{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.hover\:shadow-md:hover{--tw-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -2px rgba(0,0,0,.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color)}.focus\:bg-accent:focus{background-color:hsl(var(--accent))}.focus\:text-accent-foreground:focus{color:hsl(var(--accent-foreground))}.focus\:opacity-100:focus{opacity:1}.focus\:outline-none:focus{outline:2px solid transparent;outline-offset:2px}.focus\:ring-1:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color)}.focus\:ring-1:focus,.focus\:ring-2:focus{box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.focus\:ring-2:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color)}.focus\:ring-ring:focus{--tw-ring-color:hsl(var(--ring))}.focus\:ring-offset-2:focus{--tw-ring-offset-width:2px}.focus-visible\:outline-none:focus-visible{outline:2px solid transparent;outline-offset:2px}.focus-visible\:ring-2:focus-visible{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color);box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.focus-visible\:ring-ring:focus-visible{--tw-ring-color:hsl(var(--ring))}.focus-visible\:ring-offset-2:focus-visible{--tw-ring-offset-width:2px}.active\:scale-\[0\.98\]:active{--tw-scale-x:.98;--tw-scale-y:.98;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.disabled\:pointer-events-none:disabled{pointer-events:none}.disabled\:cursor-not-allowed:disabled{cursor:not-allowed}.disabled\:opacity-50:disabled{opacity:.5}.group:hover .group-hover\:opacity-100{opacity:1}.group.destructive .group-\[\.destructive\]\:border-muted\/40{border-color:hsl(var(--muted)/.4)}.group.destructive .group-\[\.destructive\]\:text-red-300{--tw-text-opacity:1;color:rgb(252 165 165/var(--tw-text-opacity,1))}.group.destructive .group-\[\.destructive\]\:hover\:border-destructive\/30:hover{border-color:hsl(var(--destructive)/.3)}.group.destructive .group-\[\.destructive\]\:hover\:bg-destructive:hover{background-color:hsl(var(--destructive))}.group.destructive .group-\[\.destructive\]\:hover\:text-destructive-foreground:hover{color:hsl(var(--destructive-foreground))}.group.destructive .group-\[\.destructive\]\:hover\:text-red-50:hover{--tw-text-opacity:1;color:rgb(254 242 242/var(--tw-text-opacity,1))}.group.destructive .group-\[\.destructive\]\:focus\:ring-destructive:focus{--tw-ring-color:hsl(var(--destructive))}.group.destructive .group-\[\.destructive\]\:focus\:ring-red-400:focus{--tw-ring-opacity:1;--tw-ring-color:rgb(248 113 113/var(--tw-ring-opacity,1))}.group.destructive .group-\[\.destructive\]\:focus\:ring-offset-red-600:focus{--tw-ring-offset-color:#dc2626}.peer:disabled~.peer-disabled\:cursor-not-allowed{cursor:not-allowed}.peer:disabled~.peer-disabled\:opacity-70{opacity:.7}.group:has(div) .group-has-\[div\]\:mt-0{margin-top:0}.data-\[disabled\]\:pointer-events-none[data-disabled]{pointer-events:none}.data-\[swipe\=cancel\]\:translate-x-0[data-swipe=cancel]{--tw-translate-x:0px}.data-\[swipe\=cancel\]\:translate-x-0[data-swipe=cancel],.data-\[swipe\=end\]\:translate-x-\[var\(--radix-toast-swipe-end-x\)\][data-swipe=end]{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.data-\[swipe\=end\]\:translate-x-\[var\(--radix-toast-swipe-end-x\)\][data-swipe=end]{--tw-translate-x:var(--radix-toast-swipe-end-x)}.data-\[swipe\=move\]\:translate-x-\[var\(--radix-toast-swipe-move-x\)\][data-swipe=move]{--tw-translate-x:var(--radix-toast-swipe-move-x);transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@keyframes accordion-up{0%{height:var(--radix-accordion-content-height)}to{height:0}}.data-\[state\=closed\]\:animate-accordion-up[data-state=closed]{animation:accordion-up .2s ease-out}@keyframes collapsible-up{0%{height:var(--radix-collapsible-content-height)}to{height:0}}.data-\[state\=closed\]\:animate-collapsible-up[data-state=closed]{animation:collapsible-up .2s ease-in-out}@keyframes accordion-down{0%{height:0}to{height:var(--radix-accordion-content-height)}}.data-\[state\=open\]\:animate-accordion-down[data-state=open]{animation:accordion-down .2s ease-out}@keyframes collapsible-down{0%{height:0}to{height:var(--radix-collapsible-content-height)}}.data-\[state\=open\]\:animate-collapsible-down[data-state=open]{animation:collapsible-down .2s ease-in-out}.data-\[state\=active\]\:border-b-primary[data-state=active]{border-bottom-color:hsl(var(--primary))}.data-\[active\]\:bg-accent\/50[data-active]{background-color:hsl(var(--accent)/.5)}.data-\[highlighted\]\:bg-accent[data-highlighted]{background-color:hsl(var(--accent))}.data-\[state\=active\]\:bg-background[data-state=active]{background-color:hsl(var(--background))}.data-\[state\=checked\]\:bg-primary[data-state=checked]{background-color:hsl(var(--primary))}.data-\[state\=open\]\:bg-accent[data-state=open]{background-color:hsl(var(--accent))}.data-\[state\=open\]\:bg-accent\/50[data-state=open]{background-color:hsl(var(--accent)/.5)}.data-\[state\=open\]\:bg-secondary[data-state=open]{background-color:hsl(var(--secondary))}.data-\[state\=active\]\:text-foreground[data-state=active]{color:hsl(var(--foreground))}.data-\[state\=checked\]\:text-primary-foreground[data-state=checked]{color:hsl(var(--primary-foreground))}.data-\[state\=open\]\:text-muted-foreground[data-state=open]{color:hsl(var(--muted-foreground))}.data-\[disabled\]\:opacity-50[data-disabled]{opacity:.5}.data-\[state\=active\]\:shadow-none[data-state=active]{--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.data-\[state\=active\]\:shadow-sm[data-state=active]{--tw-shadow:0 1px 2px 0 rgba(0,0,0,.05);--tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color);box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.data-\[swipe\=move\]\:transition-none[data-swipe=move]{transition-property:none}.data-\[state\=closed\]\:duration-300[data-state=closed]{transition-duration:.3s}.data-\[state\=open\]\:duration-500[data-state=open]{transition-duration:.5s}.data-\[motion\^\=from-\]\:animate-in[data-motion^=from-],.data-\[state\=open\]\:animate-in[data-state=open],.data-\[state\=visible\]\:animate-in[data-state=visible]{animation-duration:.15s;animation-name:enter;--tw-enter-opacity:initial;--tw-enter-scale:initial;--tw-enter-rotate:initial;--tw-enter-translate-x:initial;--tw-enter-translate-y:initial}.data-\[motion\^\=to-\]\:animate-out[data-motion^=to-],.data-\[state\=closed\]\:animate-out[data-state=closed],.data-\[state\=hidden\]\:animate-out[data-state=hidden],.data-\[swipe\=end\]\:animate-out[data-swipe=end]{animation-duration:.15s;animation-name:exit;--tw-exit-opacity:initial;--tw-exit-scale:initial;--tw-exit-rotate:initial;--tw-exit-translate-x:initial;--tw-exit-translate-y:initial}.data-\[motion\^\=from-\]\:fade-in[data-motion^=from-]{--tw-enter-opacity:0}.data-\[motion\^\=to-\]\:fade-out[data-motion^=to-],.data-\[state\=closed\]\:fade-out-0[data-state=closed]{--tw-exit-opacity:0}.data-\[state\=closed\]\:fade-out-80[data-state=closed]{--tw-exit-opacity:.8}.data-\[state\=hidden\]\:fade-out[data-state=hidden]{--tw-exit-opacity:0}.data-\[state\=open\]\:fade-in-0[data-state=open],.data-\[state\=visible\]\:fade-in[data-state=visible]{--tw-enter-opacity:0}.data-\[state\=closed\]\:zoom-out-95[data-state=closed]{--tw-exit-scale:.95}.data-\[state\=open\]\:zoom-in-90[data-state=open]{--tw-enter-scale:.9}.data-\[state\=open\]\:zoom-in-95[data-state=open]{--tw-enter-scale:.95}.data-\[motion\=from-end\]\:slide-in-from-right-52[data-motion=from-end]{--tw-enter-translate-x:13rem}.data-\[motion\=from-start\]\:slide-in-from-left-52[data-motion=from-start]{--tw-enter-translate-x:-13rem}.data-\[motion\=to-end\]\:slide-out-to-right-52[data-motion=to-end]{--tw-exit-translate-x:13rem}.data-\[motion\=to-start\]\:slide-out-to-left-52[data-motion=to-start]{--tw-exit-translate-x:-13rem}.data-\[side\=bottom\]\:slide-in-from-top-2[data-side=bottom]{--tw-enter-translate-y:-.5rem}.data-\[side\=left\]\:slide-in-from-right-2[data-side=left]{--tw-enter-translate-x:.5rem}.data-\[side\=right\]\:slide-in-from-left-2[data-side=right]{--tw-enter-translate-x:-.5rem}.data-\[side\=top\]\:slide-in-from-bottom-2[data-side=top]{--tw-enter-translate-y:.5rem}.data-\[state\=closed\]\:slide-out-to-bottom[data-state=closed]{--tw-exit-translate-y:100%}.data-\[state\=closed\]\:slide-out-to-left[data-state=closed]{--tw-exit-translate-x:-100%}.data-\[state\=closed\]\:slide-out-to-left-1\/2[data-state=closed]{--tw-exit-translate-x:-50%}.data-\[state\=closed\]\:slide-out-to-right-full[data-state=closed],.data-\[state\=closed\]\:slide-out-to-right[data-state=closed]{--tw-exit-translate-x:100%}.data-\[state\=closed\]\:slide-out-to-top[data-state=closed]{--tw-exit-translate-y:-100%}.data-\[state\=closed\]\:slide-out-to-top-\[48\%\][data-state=closed]{--tw-exit-translate-y:-48%}.data-\[state\=open\]\:slide-in-from-bottom[data-state=open]{--tw-enter-translate-y:100%}.data-\[state\=open\]\:slide-in-from-left[data-state=open]{--tw-enter-translate-x:-100%}.data-\[state\=open\]\:slide-in-from-left-1\/2[data-state=open]{--tw-enter-translate-x:-50%}.data-\[state\=open\]\:slide-in-from-right[data-state=open]{--tw-enter-translate-x:100%}.data-\[state\=open\]\:slide-in-from-top[data-state=open]{--tw-enter-translate-y:-100%}.data-\[state\=open\]\:slide-in-from-top-\[48\%\][data-state=open]{--tw-enter-translate-y:-48%}.data-\[state\=open\]\:slide-in-from-top-full[data-state=open]{--tw-enter-translate-y:-100%}.data-\[state\=closed\]\:duration-300[data-state=closed]{animation-duration:.3s}.data-\[state\=open\]\:duration-500[data-state=open]{animation-duration:.5s}.group[data-state=open] .group-data-\[state\=open\]\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:block:is(.dark *){display:block}.dark\:hidden:is(.dark *){display:none}.dark\:-rotate-90:is(.dark *){--tw-rotate:-90deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:rotate-0:is(.dark *){--tw-rotate:0deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:scale-0:is(.dark *){--tw-scale-x:0;--tw-scale-y:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:scale-100:is(.dark *){--tw-scale-x:1;--tw-scale-y:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:border-amber-800\/30:is(.dark *){border-color:#92400e4d}.dark\:border-destructive:is(.dark *){border-color:hsl(var(--destructive))}.dark\:from-amber-950\/20:is(.dark *){--tw-gradient-from:rgba(69,26,3,.2) var(--tw-gradient-from-position);--tw-gradient-to:rgba(69,26,3,0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.dark\:to-amber-900\/10:is(.dark *){--tw-gradient-to:rgba(120,53,15,.1) var(--tw-gradient-to-position)}@media (min-width:640px){.sm\:bottom-0{bottom:0}.sm\:right-0{right:0}.sm\:top-auto{top:auto}.sm\:inline{display:inline}.sm\:hidden{display:none}.sm\:h-\[350px\]{height:350px}.sm\:max-w-sm{max-width:24rem}.sm\:flex-row{flex-direction:row}.sm\:flex-col{flex-direction:column}.sm\:justify-end{justify-content:flex-end}.sm\:gap-2\.5{gap:.625rem}.sm\:gap-x-2{-moz-column-gap:.5rem;column-gap:.5rem}.sm\:rounded-lg{border-radius:var(--radius)}.sm\:text-left{text-align:left}.sm\:text-xl{font-size:1.25rem;line-height:1.75rem}.data-\[state\=open\]\:sm\:slide-in-from-bottom-full[data-state=open]{--tw-enter-translate-y:100%}}@media (min-width:768px){.md\:absolute{position:absolute}.md\:sticky{position:sticky}.md\:top-\[60px\]{top:60px}.md\:order-last{order:9999}.md\:col-span-2{grid-column:span 2/span 2}.md\:block{display:block}.md\:flex{display:flex}.md\:grid{display:grid}.md\:hidden{display:none}.md\:h-24{height:6rem}.md\:w-40{width:10rem}.md\:w-\[--radix-navigation-menu-viewport-width\]{width:var(--radix-navigation-menu-viewport-width)}.md\:w-auto{width:auto}.md\:w-full{width:100%}.md\:max-w-\[420px\]{max-width:420px}.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.md\:grid-cols-\[220px_minmax\(0\,1fr\)\]{grid-template-columns:220px minmax(0,1fr)}.md\:flex-row{flex-direction:row}.md\:gap-6{gap:1.5rem}.md\:px-8{padding-left:2rem;padding-right:2rem}.md\:py-0{padding-bottom:0;padding-top:0}.md\:py-12{padding-bottom:3rem;padding-top:3rem}.md\:pb-10{padding-bottom:2.5rem}.md\:pb-8{padding-bottom:2rem}.md\:pr-4{padding-right:1rem}.md\:text-4xl{font-size:2.25rem;line-height:2.5rem}.md\:text-6xl{font-size:3.75rem;line-height:1}}@media (min-width:1024px){.lg\:mt-6{margin-top:1.5rem}.lg\:block{display:block}.lg\:flex{display:flex}.lg\:grid{display:grid}.lg\:hidden{display:none}.lg\:w-64{width:16rem}.lg\:flex-none{flex:none}.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:grid-cols-\[1fr_220px\]{grid-template-columns:1fr 220px}.lg\:grid-cols-\[240px_minmax\(0\,1fr\)\]{grid-template-columns:240px minmax(0,1fr)}.lg\:flex-row{flex-direction:row}.lg\:flex-col{flex-direction:column}.lg\:gap-10{gap:2.5rem}.lg\:gap-14{gap:3.5rem}.lg\:border-b{border-bottom-width:1px}.lg\:border-b-0{border-bottom-width:0}.lg\:border-r{border-right-width:1px}.lg\:border-none{border-style:none}.lg\:py-12{padding-bottom:3rem;padding-top:3rem}.lg\:py-24{padding-bottom:6rem;padding-top:6rem}.lg\:py-6{padding-bottom:1.5rem;padding-top:1.5rem}.lg\:py-8{padding-bottom:2rem;padding-top:2rem}.lg\:pb-10{padding-bottom:2.5rem}.lg\:pb-20{padding-bottom:5rem}.lg\:text-center{text-align:center}.lg\:text-5xl{font-size:3rem;line-height:1}.lg\:leading-\[1\.1\]{line-height:1.1}}.\[\&\+div\]\:text-xs+div{font-size:.75rem;line-height:1rem}.\[\&\:not\(\:first-child\)\]\:mt-10:not(:first-child){margin-top:2.5rem}.\[\&\:not\(\:first-child\)\]\:mt-4:not(:first-child){margin-top:1rem}.\[\&\:not\(\:first-child\)\]\:mt-5:not(:first-child){margin-top:1.25rem}.\[\&\:not\(\:first-child\)\]\:mt-6:not(:first-child){margin-top:1.5rem}.\[\&\:not\(\:first-child\)\]\:mt-8:not(:first-child){margin-top:2rem}.\[\&\:not\(\:first-child\)\]\:pt-3:not(:first-child){padding-top:.75rem}.\[\&\:not\(\:first-child\)\]\:pt-4:not(:first-child){padding-top:1rem}.\[\&\:not\(\:last-child\)\]\:mb-12:not(:last-child){margin-bottom:3rem}.\[\&\:not\(\:last-child\)\]\:mb-5:not(:last-child){margin-bottom:1.25rem}.\[\&\:not\(\:last-child\)\]\:mb-6:not(:last-child){margin-bottom:1.5rem}.\[\&\:not\(\:last-child\)\]\:mb-8:not(:last-child){margin-bottom:2rem}.\[\&\>h1\]\:step>h1{counter-increment:step}.\[\&\>h1\]\:step>h1:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h2\]\:step>h2{counter-increment:step}.\[\&\>h2\]\:step>h2:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h3\]\:step>h3{counter-increment:step}.\[\&\>h3\]\:step>h3:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h4\]\:step>h4{counter-increment:step}.\[\&\>h4\]\:step>h4:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h5\]\:step>h5{counter-increment:step}.\[\&\>h5\]\:step>h5:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h6\]\:step>h6{counter-increment:step}.\[\&\>h6\]\:step>h6:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>li\:not\(\:first-child\)\]\:mt-2>li:not(:first-child){margin-top:.5rem}.\[\&\>li\]\:step>li{counter-increment:step}.\[\&\>li\]\:step>li:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>ol\]\:\!mt-2>ol{margin-top:.5rem!important}.\[\&\>svg\+div\]\:translate-y-\[-3px\]>svg+div{--tw-translate-y:-3px;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.\[\&\>svg\]\:absolute>svg{position:absolute}.\[\&\>svg\]\:left-4>svg{left:1rem}.\[\&\>svg\]\:top-4>svg{top:1rem}.\[\&\>svg\]\:size-3\.5>svg{height:.875rem;width:.875rem}.\[\&\>svg\]\:text-amber-600>svg{--tw-text-opacity:1;color:rgb(217 119 6/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-blue-700>svg{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-destructive>svg{color:hsl(var(--destructive))}.\[\&\>svg\]\:text-foreground>svg{color:hsl(var(--foreground))}.\[\&\>svg\]\:text-green-600>svg{--tw-text-opacity:1;color:rgb(22 163 74/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-red-600>svg{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-sky-600>svg{--tw-text-opacity:1;color:rgb(2 132 199/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-violet-600>svg{--tw-text-opacity:1;color:rgb(124 58 237/var(--tw-text-opacity,1))}.\[\&\>svg\~\*\]\:pl-7>svg~*{padding-left:1.75rem}.\[\&\>ul\]\:\!mt-2>ul{margin-top:.5rem!important}.\[\&\[align\=center\]\]\:text-center[align=center]{text-align:center}.\[\&\[align\=right\]\]\:text-right[align=right]{text-align:right}.\[\&\[data-state\=open\]\>svg\]\:rotate-180[data-state=open]>svg{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.\[\&_\[cmdk-group-heading\]\]\:px-2 [cmdk-group-heading]{padding-left:.5rem;padding-right:.5rem}.\[\&_\[cmdk-group-heading\]\]\:py-1\.5 [cmdk-group-heading]{padding-bottom:.375rem;padding-top:.375rem}.\[\&_\[cmdk-group-heading\]\]\:text-xs [cmdk-group-heading]{font-size:.75rem;line-height:1rem}.\[\&_\[cmdk-group-heading\]\]\:font-medium [cmdk-group-heading]{font-weight:500}.\[\&_\[cmdk-group-heading\]\]\:text-muted-foreground [cmdk-group-heading]{color:hsl(var(--muted-foreground))}.\[\&_p\]\:leading-relaxed p{line-height:1.625}</style>
<style>html{color-scheme:light}html.dark{color-scheme:dark}.theme-zinc{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:240 5.9% 10%;--primary-foreground:0 0% 98%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:240 5.9% 10%;--radius:.5rem}.dark .theme-zinc{--background:240 10% 3.9%;--foreground:0 0% 98%;--muted:240 3.7% 15.9%;--muted-foreground:240 5% 64.9%;--popover:240 10% 3.9%;--popover-foreground:0 0% 98%;--card:240 10% 3.9%;--card-foreground:0 0% 98%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:0 0% 98%;--primary-foreground:240 5.9% 10%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:240 3.7% 15.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:240 4.9% 83.9%}.theme-slate{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--primary:222.2 47.4% 11.2%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--ring:222.2 84% 4.9%;--radius:.5rem}.dark .theme-slate{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--primary:210 40% 98%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--ring:212.7 26.8% 83.9}.theme-stone{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:24 9.8% 10%;--primary-foreground:60 9.1% 97.8%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:20 14.3% 4.1%;--radius:.5rem}.dark .theme-stone{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:60 9.1% 97.8%;--primary-foreground:24 9.8% 10%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 62.8% 30.6%;--destructive-foreground:60 9.1% 97.8%;--ring:24 5.7% 82.9%}.theme-gray{--background:0 0% 100%;--foreground:224 71.4% 4.1%;--muted:220 14.3% 95.9%;--muted-foreground:220 8.9% 46.1%;--popover:0 0% 100%;--popover-foreground:224 71.4% 4.1%;--card:0 0% 100%;--card-foreground:224 71.4% 4.1%;--border:220 13% 91%;--input:220 13% 91%;--primary:220.9 39.3% 11%;--primary-foreground:210 20% 98%;--secondary:220 14.3% 95.9%;--secondary-foreground:220.9 39.3% 11%;--accent:220 14.3% 95.9%;--accent-foreground:220.9 39.3% 11%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 20% 98%;--ring:224 71.4% 4.1%;--radius:.5rem}.dark .theme-gray{--background:224 71.4% 4.1%;--foreground:210 20% 98%;--muted:215 27.9% 16.9%;--muted-foreground:217.9 10.6% 64.9%;--popover:224 71.4% 4.1%;--popover-foreground:210 20% 98%;--card:224 71.4% 4.1%;--card-foreground:210 20% 98%;--border:215 27.9% 16.9%;--input:215 27.9% 16.9%;--primary:210 20% 98%;--primary-foreground:220.9 39.3% 11%;--secondary:215 27.9% 16.9%;--secondary-foreground:210 20% 98%;--accent:215 27.9% 16.9%;--accent-foreground:210 20% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 20% 98%;--ring:216 12.2% 83.9%}.theme-neutral{--background:0 0% 100%;--foreground:0 0% 3.9%;--muted:0 0% 96.1%;--muted-foreground:0 0% 45.1%;--popover:0 0% 100%;--popover-foreground:0 0% 3.9%;--card:0 0% 100%;--card-foreground:0 0% 3.9%;--border:0 0% 89.8%;--input:0 0% 89.8%;--primary:0 0% 9%;--primary-foreground:0 0% 98%;--secondary:0 0% 96.1%;--secondary-foreground:0 0% 9%;--accent:0 0% 96.1%;--accent-foreground:0 0% 9%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:0 0% 3.9%;--radius:.5rem}.dark .theme-neutral{--background:0 0% 3.9%;--foreground:0 0% 98%;--muted:0 0% 14.9%;--muted-foreground:0 0% 63.9%;--popover:0 0% 3.9%;--popover-foreground:0 0% 98%;--card:0 0% 3.9%;--card-foreground:0 0% 98%;--border:0 0% 14.9%;--input:0 0% 14.9%;--primary:0 0% 98%;--primary-foreground:0 0% 9%;--secondary:0 0% 14.9%;--secondary-foreground:0 0% 98%;--accent:0 0% 14.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:0 0% 83.1%}.theme-red{--background:0 0% 100%;--foreground:0 0% 3.9%;--muted:0 0% 96.1%;--muted-foreground:0 0% 45.1%;--popover:0 0% 100%;--popover-foreground:0 0% 3.9%;--card:0 0% 100%;--card-foreground:0 0% 3.9%;--border:0 0% 89.8%;--input:0 0% 89.8%;--primary:0 72.2% 50.6%;--primary-foreground:0 85.7% 97.3%;--secondary:0 0% 96.1%;--secondary-foreground:0 0% 9%;--accent:0 0% 96.1%;--accent-foreground:0 0% 9%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:0 72.2% 50.6%;--radius:.5rem}.dark .theme-red{--background:0 0% 3.9%;--foreground:0 0% 98%;--muted:0 0% 14.9%;--muted-foreground:0 0% 63.9%;--popover:0 0% 3.9%;--popover-foreground:0 0% 98%;--card:0 0% 3.9%;--card-foreground:0 0% 98%;--border:0 0% 14.9%;--input:0 0% 14.9%;--primary:0 72.2% 50.6%;--primary-foreground:0 85.7% 97.3%;--secondary:0 0% 14.9%;--secondary-foreground:0 0% 98%;--accent:0 0% 14.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:0 72.2% 50.6%}.theme-rose{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:346.8 77.2% 49.8%;--primary-foreground:355.7 100% 97.3%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:346.8 77.2% 49.8%;--radius:.5rem}.dark .theme-rose{--background:20 14.3% 4.1%;--foreground:0 0% 95%;--muted:0 0% 15%;--muted-foreground:240 5% 64.9%;--popover:0 0% 9%;--popover-foreground:0 0% 95%;--card:24 9.8% 10%;--card-foreground:0 0% 95%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:346.8 77.2% 49.8%;--primary-foreground:355.7 100% 97.3%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:12 6.5% 15.1%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 85.7% 97.3%;--ring:346.8 77.2% 49.8%}.theme-orange{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:24.6 95% 53.1%;--primary-foreground:60 9.1% 97.8%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:24.6 95% 53.1%;--radius:.5rem}.dark .theme-orange{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:20.5 90.2% 48.2%;--primary-foreground:60 9.1% 97.8%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 72.2% 50.6%;--destructive-foreground:60 9.1% 97.8%;--ring:20.5 90.2% 48.2%}.theme-green{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:142.1 76.2% 36.3%;--primary-foreground:355.7 100% 97.3%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:142.1 76.2% 36.3%;--radius:.5rem}.dark .theme-green{--background:20 14.3% 4.1%;--foreground:0 0% 95%;--muted:0 0% 15%;--muted-foreground:240 5% 64.9%;--popover:0 0% 9%;--popover-foreground:0 0% 95%;--card:24 9.8% 10%;--card-foreground:0 0% 95%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:142.1 70.6% 45.3%;--primary-foreground:144.9 80.4% 10%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:12 6.5% 15.1%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 85.7% 97.3%;--ring:142.4 71.8% 29.2%}.theme-blue{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--primary:221.2 83.2% 53.3%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--ring:221.2 83.2% 53.3%;--radius:.5rem}.dark .theme-blue{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--primary:217.2 91.2% 59.8%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--ring:224.3 76.3% 48%}.theme-yellow{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:47.9 95.8% 53.1%;--primary-foreground:26 83.3% 14.1%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:20 14.3% 4.1%;--radius:.5rem}.dark .theme-yellow{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:47.9 95.8% 53.1%;--primary-foreground:26 83.3% 14.1%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 62.8% 30.6%;--destructive-foreground:60 9.1% 97.8%;--ring:35.5 91.7% 32.9%}.theme-violet{--background:0 0% 100%;--foreground:224 71.4% 4.1%;--muted:220 14.3% 95.9%;--muted-foreground:220 8.9% 46.1%;--popover:0 0% 100%;--popover-foreground:224 71.4% 4.1%;--card:0 0% 100%;--card-foreground:224 71.4% 4.1%;--border:220 13% 91%;--input:220 13% 91%;--primary:262.1 83.3% 57.8%;--primary-foreground:210 20% 98%;--secondary:220 14.3% 95.9%;--secondary-foreground:220.9 39.3% 11%;--accent:220 14.3% 95.9%;--accent-foreground:220.9 39.3% 11%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 20% 98%;--ring:262.1 83.3% 57.8%;--radius:.5rem}.dark .theme-violet{--background:224 71.4% 4.1%;--foreground:210 20% 98%;--muted:215 27.9% 16.9%;--muted-foreground:217.9 10.6% 64.9%;--popover:224 71.4% 4.1%;--popover-foreground:210 20% 98%;--card:224 71.4% 4.1%;--card-foreground:210 20% 98%;--border:215 27.9% 16.9%;--input:215 27.9% 16.9%;--primary:263.4 70% 50.4%;--primary-foreground:210 20% 98%;--secondary:215 27.9% 16.9%;--secondary-foreground:210 20% 98%;--accent:215 27.9% 16.9%;--accent-foreground:210 20% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 20% 98%;--ring:263.4 70% 50.4%}</style>
<style>.carbon-responsive-wrap{align-items:center!important;background-color:hsl(var(--background))!important;border-color:hsl(var(--border))!important;border-radius:var(--radius)!important;display:flex!important;padding:1rem!important}@media (min-width:1024px){.carbon-responsive-wrap{flex-direction:column!important;padding-bottom:1.5rem!important;padding-top:1.5rem!important}}.carbon-responsive-wrap .carbon-img{border-radius:.25rem!important;overflow:hidden!important}@media (min-width:1024px){.carbon-responsive-wrap .carbon-img{flex:none!important}}.carbon-responsive-wrap .carbon-text{color:hsl(var(--muted-foreground))!important;font-size:.875rem!important;line-height:1.25rem!important}@media (min-width:1024px){.carbon-responsive-wrap .carbon-text{flex:none!important;text-align:center!important}}#carbonads .carbon-poweredby{background-color:hsl(var(--background))!important;color:hsl(var(--muted-foreground))!important;display:block!important;font-size:10px!important;text-align:right!important;text-decoration-line:none!important;text-transform:uppercase!important}</style>
<link rel="stylesheet" href="/_nuxt/entry.Y5sdK25x.css" crossorigin>
<style>:where(.i-carbon\:course){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32' width='32' height='32'%3E%3Cpath fill='black' d='M24 30H8a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h16a2 2 0 0 1 2 2v16.618l-5-2.5l-5 2.5V4H8v24h16v-4h2v4a2.003 2.003 0 0 1-2 2m-3-14.118l3 1.5V4h-6v13.382Z'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-left){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m12 19l-7-7l7-7m7 7H5'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-right){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M5 12h14m-7-7l7 7l-7 7'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-up){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m5 12l7-7l7 7m-7 7V5'/%3E%3C/svg%3E")}:where(.i-lucide\:bot){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M12 8V4H8'/%3E%3Crect width='16' height='12' x='4' y='8' rx='2'/%3E%3Cpath d='M2 14h2m16 0h2m-7-1v2m-6-2v2'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:brain){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M12 5a3 3 0 1 0-5.997.125a4 4 0 0 0-2.526 5.77a4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z'/%3E%3Cpath d='M12 5a3 3 0 1 1 5.997.125a4 4 0 0 1 2.526 5.77a4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z'/%3E%3Cpath d='M15 13a4.5 4.5 0 0 1-3-4a4.5 4.5 0 0 1-3 4m8.599-6.5a3 3 0 0 0 .399-1.375m-11.995 0A3 3 0 0 0 6.401 6.5m-2.924 4.396a4 4 0 0 1 .585-.396m15.876 0a4 4 0 0 1 .585.396M6 18a4 4 0 0 1-1.967-.516m15.934 0A4 4 0 0 1 18 18'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:database){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cellipse cx='12' cy='5' rx='9' ry='3'/%3E%3Cpath d='M3 5v14a9 3 0 0 0 18 0V5'/%3E%3Cpath d='M3 12a9 3 0 0 0 18 0'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:download){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4m4-5l5 5l5-5m-5 5V3'/%3E%3C/svg%3E")}:where(.i-lucide\:flag){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 15s1-1 4-1s5 2 8 2s4-1 4-1V3s-1 1-4 1s-5-2-8-2s-4 1-4 1zm0 7v-7'/%3E%3C/svg%3E")}:where(.i-lucide\:github){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5c.08-1.25-.27-2.48-1-3.5c.28-1.15.28-2.35 0-3.5c0 0-1 0-3 1.5c-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.4 5.4 0 0 0 4 9c0 3.5 3 5.5 6 5.5c-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4'/%3E%3Cpath d='M9 18c-4.51 2-5-2-7-2'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:hand){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M18 11V6a2 2 0 0 0-2-2a2 2 0 0 0-2 2m0 4V4a2 2 0 0 0-2-2a2 2 0 0 0-2 2v2m0 4.5V6a2 2 0 0 0-2-2a2 2 0 0 0-2 2v8'/%3E%3Cpath d='M18 8a2 2 0 1 1 4 0v6a8 8 0 0 1-8 8h-2c-2.8 0-4.5-.86-5.99-2.34l-3.6-3.6a2 2 0 0 1 2.83-2.82L7 15'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:menu){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 12h16M4 6h16M4 18h16'/%3E%3C/svg%3E")}:where(.i-lucide\:moon){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M12 3a6 6 0 0 0 9 9a9 9 0 1 1-9-9'/%3E%3C/svg%3E")}:where(.i-lucide\:sun){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Ccircle cx='12' cy='12' r='4'/%3E%3Cpath d='M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-material-symbols\:water-drop-outline){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='black' d='M12.275 19q.3-.025.513-.238T13 18.25q0-.35-.225-.562T12.2 17.5q-1.025.075-2.175-.562t-1.45-2.313q-.05-.275-.262-.45T7.825 14q-.35 0-.575.263t-.15.612q.425 2.275 2 3.25t3.175.875M12 22q-3.425 0-5.712-2.35T4 13.8q0-2.5 1.988-5.437T12 2q4.025 3.425 6.013 6.363T20 13.8q0 3.5-2.287 5.85T12 22m0-2q2.6 0 4.3-1.763T18 13.8q0-1.825-1.513-4.125T12 4.65Q9.025 7.375 7.513 9.675T6 13.8q0 2.675 1.7 4.438T12 20m0-8'/%3E%3C/svg%3E")}</style>
<link rel="preload" as="fetch" crossorigin="anonymous" href="/robotics-overview/robotics-datasets/_payload.json?1defcafe-82b4-4ebd-9604-7ed2beb079ac">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/B663YWtb.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BW0ds9ZD.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CRHqcAuT.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CIIuXiNY.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C_JUrZ42.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DFABIXck.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DpgiKU3o.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C89eQFnR.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/Bumpvn_e.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C7ANcheu.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/pxM5uM41.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DAVNjjuB.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BKBxzSEV.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDRn9gVb.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DGe4lc1z.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BVrm2KGx.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/62T6SIU5.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DkYHptcI.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BpKG8Ymq.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/daEll9fd.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/GbZ6cGiI.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DzsAbCrU.js">
<link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/1defcafe-82b4-4ebd-9604-7ed2beb079ac.json">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/pXWERE62.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/BNg4H9A5.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/bNaE6FFb.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/I49mxDSz.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Bj4cSqXp.js">
<meta property="og:image" content="https://cybernachos.github.io/__og-image__/static/robotics-overview/robotics-datasets/og.png">
<meta property="og:image:type" content="image/png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cybernachos.github.io/__og-image__/static/robotics-overview/robotics-datasets/og.png">
<meta name="twitter:image:src" content="https://cybernachos.github.io/__og-image__/static/robotics-overview/robotics-datasets/og.png">
<meta property="og:image:width" content="1200">
<meta name="twitter:image:width" content="1200">
<meta property="og:image:height" content="600">
<meta name="twitter:image:height" content="600">
<meta name="description" content="A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, DobbE, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.">
<meta property="og:description" content="A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, DobbE, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.">
<meta property="og:title" content="Robotics Dataset Comparison">
<script type="module" src="/_nuxt/B663YWtb.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body  class="theme-Zinc" style="--radius:0.5rem"><div id="__nuxt"><!--[--><div class="nuxt-loading-indicator z-100 bg-primary/80" style="position:fixed;top:0;right:0;left:0;pointer-events:none;width:auto;height:3px;opacity:0;background-size:Infinity% auto;transform:scaleX(0%);transform-origin:left;transition:transform 0.1s, height 0.4s, opacity 0.4s;z-index:999999;"></div><!----><header class="sticky top-0 z-40 bg-background/80 backdrop-blur-lg"><div class="container max-w-screen-2xl flex h-14 items-center justify-between gap-2 px-4 md:px-8"><div class="hidden flex-1 md:flex"><a href="/" class="flex"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" data-nuxt-img srcset="/_ipx/_/transparent-logo.svg 1x, /_ipx/_/transparent-logo.svg 2x" class="h-7 dark:hidden" src="/_ipx/_/transparent-logo.svg"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" data-nuxt-img srcset="/_ipx/_/transparent-logo.svg 1x, /_ipx/_/transparent-logo.svg 2x" class="hidden h-7 dark:block" src="/_ipx/_/transparent-logo.svg"><span class="ml-3 self-center font-bold">Cyber Nachos</span></a></div><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 md:hidden" type="button" aria-haspopup="dialog" aria-expanded="false" data-state="closed"><!--[--><span class="iconify i-lucide:menu" aria-hidden="true" style="font-size:18px;"></span><!--]--></button><!----><!--]--><!--]--><!----><nav aria-label="Main" data-orientation="horizontal" dir="ltr" data-radix-navigation-menu class="relative z-10 max-w-max items-center justify-center hidden flex-1 lg:flex"><!--[--><!--[--><div style="position:relative;"><ul class="group flex flex-1 list-none items-center justify-center gap-x-1" data-orientation="horizontal"><!--[--><!--]--></ul></div><!--]--><div class="absolute left-0 top-full flex justify-center"><!----></div><!--]--></nav><div class="flex flex-1 justify-end gap-2"><a href="/promptrobot" class=""><button class="items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground rounded-md px-3 flex gap-2 h-8 mt-1"><!--[--> Prompt Robotics <!--]--></button></a><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap text-sm ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent px-4 py-2 h-8 w-full self-center rounded-md pr-1.5 font-normal text-muted-foreground hover:text-accent-foreground md:w-40 lg:w-64"><!--[--><span class="mr-auto overflow-hidden">Search...</span><kbd class="pointer-events-none inline-flex h-5 select-none items-center gap-1 rounded border border-border bg-muted font-sans font-medium min-h-5 text-[11px] h-5 px-1 ml-auto hidden md:block"><!--[--><span class="text-xs"></span>K <!--]--></kbd><!--]--></button><!--]--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><div class="flex"><!----><!----><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10"><!--[--><span class="iconify i-lucide:sun rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0" aria-hidden="true" style="font-size:18px;"></span><span class="iconify i-lucide:moon absolute rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100" aria-hidden="true" style="font-size:18px;"></span><span class="sr-only">Toggle theme</span><!--]--></button><!--[--><a href="https://github.com/CyberNachos" rel="noopener noreferrer" target="_blank"><button class="items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 flex gap-2"><!--[--><span class="iconify i-lucide:github" aria-hidden="true" style="font-size:18px;"></span><!--]--></button></a><!--]--></div></div></div><!----></header><div class="min-h-screen border-b"><div class="container md:grid-cols-[220px_minmax(0,1fr)] lg:grid-cols-[240px_minmax(0,1fr)] flex-1 items-start px-4 md:grid md:gap-6 md:px-8 lg:gap-10"><aside class="fixed top-[102px] z-30 -ml-2 hidden h-[calc(100vh-3.5rem)] w-full shrink-0 overflow-y-auto md:sticky md:top-[60px] md:block"><div dir="ltr" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px;" class="relative h-full overflow-hidden py-6 pr-6 text-sm md:pr-4" orientation="vertical"><!--[--><!--[--><div data-radix-scroll-area-viewport style="overflow-x:hidden;overflow-y:hidden;" class="size-full rounded-[inherit]" tabindex="0"><div style=""><!--[--><!--[--><!----><!----><!----><ul class="flex flex-col gap-1 py-1 pt-1"><!--[--><li><a href="/cybernachos" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:hand min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Cyber Nachos</span><!--[--><!--]--><!--]--></a></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Published Products</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/published/cybernachos-gpt" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Cyber Nachos GPT</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Tutorial Isaac Lab</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/tutorial-isaaclab/introduction" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Introduction</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/installation" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:download min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Installation Guide</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/getting-started" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:flag min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Getting Started</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/enable-fluid" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-material-symbols:water-drop-outline min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Enabling fluid simulation</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Tutorial LLMs</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/tutorial-llms/distilling-gpt-with-pytorch-and-transformers" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Distilling GPT with PyTorch and Transformers</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Published Research Papers</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/published-research/you-only-render-once" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">You Only Render Once</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/real-time-dexterous" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Real-time Dexterous Telemanipulation</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/hallucination" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Hallucination Prevention in LLMs</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/ai-anchor" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">AI Anchor Framework</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/ai-productivity" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">AI as the Cognitive Engine of Productivity</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/ai-next" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">AI&#39;s Euclid&#39;s Elements Moment</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Robotics Overview</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a aria-current="page" href="/robotics-overview/robotics-datasets" class="router-link-active router-link-exact-active flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary bg-muted !text-primary h-8"><!--[--><span class="iconify i-lucide:database min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Robotics Dataset Comparison</span><!--[--><!--]--><!--]--></a></li><li><a href="/robotics-overview/robotics-models" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">General-Purpose Robot Models Analysis</span><!--[--><!--]--><!--]--></a></li><li><a href="/robotics-overview/wheel-based-robots" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Wheel-Based Humanoid Robots</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Posts</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/posts/robotic-sociology" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Robotic Sociology: Human-Robot Interaction and the Emergence of Machine Social Structures</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Courses</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/courses/mobile-lab" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-carbon:course min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Mobile Computing Lab Course</span><!--[--><!--]--><!--]--></a></li><li><a href="/courses/security-lab" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-carbon:course min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Mobile and IoT Security Lab Course</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">AI Robotics 100 Questions</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/ai-robotics-100q/outline" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">AI Robotics 100 Questions: Complete Learning Path</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part1" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 1: Fundamentals of AI Robotics (Questions 1-10)</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part2" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 2: Perception and Sensing (Questions 11-28)</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part3" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 3: Control and Manipulation (Questions 29-45)</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part4" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 4: Localization and Navigation (Questions 46-63)</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part5" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 5: Human-Robot Interaction (HRI) (Questions 64-78)</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part6" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 6: AI Decision Making and Autonomy (Questions 79-90)</span><!--[--><!--]--><!--]--></a></li><li><a href="/ai-robotics-100q/part7" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Part 7: Simulation and Sim2Real Transfer (Questions 91-101)</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><a href="/terms" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><!----><span class="truncate text-nowrap">Terms of Service &amp; User Agreement</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--><!--]--></div></div><style> /* Hide scrollbars cross-browser and enable momentum scroll for touch devices */ [data-radix-scroll-area-viewport] { scrollbar-width:none; -ms-overflow-style:none; -webkit-overflow-scrolling:touch; } [data-radix-scroll-area-viewport]::-webkit-scrollbar { display:none; } </style><!--]--><!----><!----><!--]--></div></aside><!--[--><main class="lg:grid lg:grid-cols-[1fr_220px] lg:gap-14 lg:py-8 relative py-6"><div class="mx-auto w-full min-w-0"><!----><div class="mb-6"><h1 class="scroll-m-20 text-4xl font-extrabold tracking-tight lg:text-5xl"><!--[-->Robotics Dataset Comparison<!--]--></h1><p class="pt-1 text-lg text-muted-foreground"></p><!----><!----></div><div class="docs-content"><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[-->Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table that highlights key characteristics, and second, detailed descriptions of each dataset&#39;s scope, technical features, advantages, and disadvantages.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Note:<!--]--></strong> This analysis is accurate as of the last modified date, &quot;Mar 28, 2025.&quot;<!--]--></p><h2 id="summary-table" class="scroll-m-20 border-b pb-2 text-3xl font-semibold tracking-tight transition-colors [&amp;:not(:first-child)]:mt-10"><a href="#summary-table"><!--[-->Summary Table<!--]--></a></h2><div class="w-full overflow-y-auto [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6"><table class="w-full"><!--[--><thead><!--[--><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Dataset / Framework<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Scale &amp; Modalities<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Key Advantages<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Key Disadvantages<!--]--></strong><!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->1. <strong class="font-semibold"><!--[-->LeRobot<!--]--></strong> <br> (<a href="https://github.com/huggingface/lerobot" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->GitHub<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->End-to-end learning with community support; integrated simulation environments.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Complex setup; may require substantial computing and sensor calibration.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->2. <strong class="font-semibold"><!--[-->Open X-Embodiment<!--]--></strong> <br> (<a href="https://robotics-transformer-x.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Massive diversity enabling cross-robot transfer and positive knowledge sharing.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Heterogeneous quality and potential standardization issues across varied sources.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->3. <strong class="font-semibold"><!--[-->DROID<!--]--></strong> <br> (<a href="https://droid-dataset.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->In-the-wild robot manipulation for robust imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Diverse, large-scale manipulation data that improves policy robustness.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Mostly limited to manipulation with a specific hardware setup; less diversity in task types.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->4. <strong class="font-semibold"><!--[-->RoboTurk<!--]--></strong> <br> (<a href="https://roboturk.stanford.edu/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Leverages non-expert, scalable human demonstrations; supports collaborative tasks.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Variation in demonstration quality and potential limits in scale compared to fully automated data collection.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->5. <strong class="font-semibold"><!--[-->MIME<!--]--></strong> <br> (<a href="https://sites.google.com/view/mimedataset/dataset?authuser=0" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Google Sites<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Imitation learning for robot manipulation using human demonstrations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Focus on high-quality manipulation trajectories; well-suited for imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->May be smaller in scale and less diverse than some large-scale multi-robot datasets.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->6. <strong class="font-semibold"><!--[-->Meta-World<!--]--></strong> <br> (<a href="https://meta-world.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Benchmark for multi-task and meta-reinforcement learning in simulation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->50 distinct simulated manipulation environments; task variations with visual observations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Standardized benchmark for meta-RL; structured for evaluating generalization.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Limited to simulation and may not capture the full variability of real-world settings.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->7. <strong class="font-semibold"><!--[-->RoboNet<!--]--></strong> <br> (<a href="https://www.robonet.wiki/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Open database of real robotic experience for manipulation tasks across multiple platforms.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->~15M video frames, collected from 7 robot platforms with diverse camera viewpoints.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale, multi-platform real-world data that facilitates cross-robot generalization.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very high storage and processing requirements; complex data integration.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->8. <strong class="font-semibold"><!--[-->RoboSet<!--]--></strong> <br> (<a href="https://robopen.github.io/roboset/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-task dataset for household (kitchen) manipulation tasks, including language instructions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Rich, multi-modal data in realistic home environments; supports language-guided sequencing.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Domain-specific (largely kitchens); may not generalize to non-domestic scenarios.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->9. <strong class="font-semibold"><!--[-->BridgeData V2<!--]--></strong> <br> (<a href="https://rail-berkeley.github.io/bridgedata/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale robotic manipulation across diverse environments and skills with language annotations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->10. <strong class="font-semibold"><!--[-->RT-1<!--]--></strong> <br> (<a href="https://robotics-transformer1.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Real-world imitation learning for multi-task manipulation using transformer architectures.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Outstanding generalization and performance on diverse tasks; scalable transformer model.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->High training and computational requirements; system complexity may be a barrier.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->11. <strong class="font-semibold"><!--[-->DobbE<!--]--></strong> <br> (<a href="https://dobb-e.com/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Framework for home robotics: learning household manipulation tasks quickly in real homes.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->HoNY dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30fps; also includes hardware (the Stick) for data collection.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Cost-effective, rapid task learning with real household data; designed for generalist home robots.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->12. <strong class="font-semibold"><!--[-->RH20T<!--]--></strong> <br> (<a href="https://rh20t.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very large and complex; requires significant computational and storage resources; complex data processing pipeline.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->13. <strong class="font-semibold"><!--[-->BC-Z<!--]--></strong> <br> (<a href="https://sites.google.com/view/bc-z/home?pli=1" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale behavior cloning for robotic manipulation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->14. <strong class="font-semibold"><!--[-->MT-Opt<!--]--></strong> <br> (<a href="https://karolhausman.github.io/mt-opt/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-task reinforcement learning at scale across many manipulation skills.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->15. <strong class="font-semibold"><!--[-->VIMA<!--]--></strong> <br> (<a href="https://vimalabs.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->General robot manipulation via multimodal prompts (combining language and vision) for unified task specification.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Unified formulation that prompts the robot to perform diverse tasks; highly scalable and sample-efficient.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->16. <strong class="font-semibold"><!--[-->SPOC<!--]--></strong> <br> (<a href="https://spoc-robot.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world).<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios.<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table></div><hr class="[&amp;:not(:first-child)]:mt-6"><h2 id="detailed-comparison" class="scroll-m-20 border-b pb-2 text-3xl font-semibold tracking-tight transition-colors [&amp;:not(:first-child)]:mt-10"><a href="#detailed-comparison"><!--[-->Detailed Comparison<!--]--></a></h2><h3 id="_1-lerobot" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_1-lerobot"><!--[-->1. LeRobot<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
LeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Built in PyTorch with modular dataset classes that support multi-frame temporal sampling.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Community-driven with active contributions and hosted on Hugging Face.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Facilitates rapid prototyping in robotics with an accessible codebase.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_2-open-x-embodiment" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_2-open-x-embodiment"><!--[-->2. Open X-Embodiment<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A collaborative effort pooling robot data from 21 institutions, it is aimed at training generalist policies across 22 different robot embodiments.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Aggregates 1M+ trajectories from diverse robots and tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Supports learning via transformer-based architectures that can generalize across different embodiments.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Unmatched diversity, which is ideal for studying cross-robot transfer.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Large scale increases the potential for generalization.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_3-droid" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_3-droid"><!--[-->3. DROID<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Focused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Multi-camera views (including wrist and exterior images) enable rich visual inputs.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Large, diverse dataset that significantly boosts policy performance and robustness.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Extensive coverage of real-world scenarios.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Being collected with a specific hardware platform, its applicability to other robots may be limited.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_4-roboturk" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_4-roboturk"><!--[-->4. RoboTurk<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset).<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes system features for low-latency teleoperation and human-in-the-loop interventions.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Proven effectiveness in enabling imitation learning on challenging tasks.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->The quality of demonstrations may vary due to differences in human teleoperation skills.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_5-mime" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_5-mime"><!--[-->5. MIME<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
MIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Focused on detailed manipulation tasks, making it ideal for imitation learning studies.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Generally smaller in scale compared to some of the largest datasets; might offer limited diversity.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_6-meta-world" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_6-meta-world"><!--[-->6. Meta-World<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Structured environments with varying goal positions and task variations to test generalization.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_7-robonet" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_7-robonet"><!--[-->7. RoboNet<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains over 15M video frames and data from multiple camera viewpoints.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Offers vast amounts of real-world data to study generalization across different robot hardware.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Requires heavy storage and processing; integrating multi-platform data can be challenging.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_8-roboset" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_8-roboset"><!--[-->8. RoboSet<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Rich multi-modal information (visual + language) supports language-guided robotic learning.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_9-bridgedata-v2" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_9-bridgedata-v2"><!--[-->9. BridgeData V2<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Designed to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Approximately 60K trajectories with multi-view RGB (and some depth) data.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes both teleoperated and scripted demonstrations.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->High diversity in environments and tasks; strong support for language-conditioned policy learning.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_10-rt-1" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_10-rt-1"><!--[-->10. RT-1<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Over 130K episodes covering more than 700 tasks collected from 13 robots.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Utilizes vision and natural language inputs to produce discretized action tokens.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates superior performance and generalization, including sim-to-real transfer.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Scalability through high-capacity transformer models.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demands extensive data, compute, and engineering expertise; system complexity is high.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_11-dobbe" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_11-dobbe"><!--[-->11. DobbE<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
DobbE focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->HoNY dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30fps).<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes a low-cost hardware Stick for demonstration collection.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Cost-effective and designed for rapid task learning in domestic environments.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates strong real-world applicability in home settings.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_12-rh20t" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_12-rh20t"><!--[-->12. RH20T<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Detailed synchronization and calibration across multiple sensors.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Supports research on contact-rich and dexterous manipulation.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_13-bc-z" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_13-bc-z"><!--[-->13. BC-Z<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
BC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Serves as a standardized resource for evaluating behavior cloning algorithms.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_14-mt-opt" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_14-mt-opt"><!--[-->14. MT-Opt<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
MT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Effective at sharing experience across tasks, significantly boosting performance on rare tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates both zero-shot and rapid fine-tuning capabilities.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_15-vima" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_15-vima"><!--[-->15. VIMA<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
VIMA presents a novel formulation in which diverse robot manipulation tasks are prompted via interleaved language and visual tokens, unifying task specification.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Allows integration of various forms of task instructions (text + image).<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_16-spoc" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_16-spoc"><!--[-->16. SPOC<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
SPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Trains entirely in simulation and transfers effectively.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios.<!--]--></li><!--]--></ul></div><div class="mt-16"><div class="mb-6 flex w-full items-center justify-between"><!----><div class="w-fit"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 underline-offset-4 hover:underline h-10 px-4 py-2 text-sm font-semibold text-primary"><!--[--><div class="flex items-center gap-2"><span class="iconify i-lucide:arrow-up" aria-hidden="true" style="font-size:16px;"></span><span>Back to Top</span></div><!--]--></button></div><div class="text-sm text-gray-500"> Last modified on: Mar 28, 2025</div></div><div class="border-t pt-6 lg:flex lg:flex-row"><a href="/published-research/ai-next" class="basis-1/3"><div class="mb-4 space-y-2 rounded-lg border p-4 transition-all hover:bg-muted/50"><div class="flex flex-row gap-3"><div class="flex size-6 min-w-6"><span class="iconify i-lucide:arrow-left mx-auto self-center" aria-hidden="true" style="font-size:20px;"></span></div><span class="space-y-2 self-center"><div class="text-lg font-semibold">AI&#39;s Euclid&#39;s Elements Moment</div><div class="text-sm text-muted-foreground">AI&#39;s Euclid&#39;s Elements Moment: From Language Models to Computable Thought</div></span><!----></div></div></a><span class="flex-1"></span><a href="/robotics-overview/robotics-models" class="basis-1/3"><div class="mb-4 space-y-2 rounded-lg border p-4 transition-all hover:bg-muted/50"><div class="flex flex-row gap-3"><!----><span class="space-y-2 self-center"><div class="text-lg font-semibold">General-Purpose Robot Models Analysis</div><div class="text-sm text-muted-foreground">Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.</div></span><div class="ml-auto flex size-6 min-w-6"><span class="iconify i-lucide:arrow-right mx-auto self-center" aria-hidden="true" style="font-size:20px;"></span></div></div></div></a></div><div class="flex"><!----></div></div></div><div class="hidden text-sm lg:block"><div class="sticky top-[90px] h-[calc(100vh-3.5rem)] overflow-hidden"><div dir="ltr" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px;" class="relative overflow-hidden z-30 hidden h-[calc(100vh-6.5rem)] overflow-y-auto md:block lg:block" orientation="vertical"><!--[--><!--[--><div data-radix-scroll-area-viewport style="overflow-x:hidden;overflow-y:hidden;" class="size-full rounded-[inherit]" tabindex="0"><div style=""><!--[--><!--[--><div class="flex h-[calc(100vh-6.5rem)] flex-col"><div><p class="mb-2 text-base font-semibold">On This Page</p><ul class=""><!--[--><li class="pt-2"><a href="#summary-table" class="text-muted-foreground transition-all hover:text-primary">Summary Table</a><!----></li><li class="pt-2"><a href="#detailed-comparison" class="text-muted-foreground transition-all hover:text-primary">Detailed Comparison</a><ul class="pl-4"><!--[--><li class="pt-2"><a href="#_1-lerobot" class="text-muted-foreground transition-all hover:text-primary">1. LeRobot</a><!----></li><li class="pt-2"><a href="#_2-open-x-embodiment" class="text-muted-foreground transition-all hover:text-primary">2. Open X-Embodiment</a><!----></li><li class="pt-2"><a href="#_3-droid" class="text-muted-foreground transition-all hover:text-primary">3. DROID</a><!----></li><li class="pt-2"><a href="#_4-roboturk" class="text-muted-foreground transition-all hover:text-primary">4. RoboTurk</a><!----></li><li class="pt-2"><a href="#_5-mime" class="text-muted-foreground transition-all hover:text-primary">5. MIME</a><!----></li><li class="pt-2"><a href="#_6-meta-world" class="text-muted-foreground transition-all hover:text-primary">6. Meta-World</a><!----></li><li class="pt-2"><a href="#_7-robonet" class="text-muted-foreground transition-all hover:text-primary">7. RoboNet</a><!----></li><li class="pt-2"><a href="#_8-roboset" class="text-muted-foreground transition-all hover:text-primary">8. RoboSet</a><!----></li><li class="pt-2"><a href="#_9-bridgedata-v2" class="text-muted-foreground transition-all hover:text-primary">9. BridgeData V2</a><!----></li><li class="pt-2"><a href="#_10-rt-1" class="text-muted-foreground transition-all hover:text-primary">10. RT-1</a><!----></li><li class="pt-2"><a href="#_11-dobbe" class="text-muted-foreground transition-all hover:text-primary">11. DobbE</a><!----></li><li class="pt-2"><a href="#_12-rh20t" class="text-muted-foreground transition-all hover:text-primary">12. RH20T</a><!----></li><li class="pt-2"><a href="#_13-bc-z" class="text-muted-foreground transition-all hover:text-primary">13. BC-Z</a><!----></li><li class="pt-2"><a href="#_14-mt-opt" class="text-muted-foreground transition-all hover:text-primary">14. MT-Opt</a><!----></li><li class="pt-2"><a href="#_15-vima" class="text-muted-foreground transition-all hover:text-primary">15. VIMA</a><!----></li><li class="pt-2"><a href="#_16-spoc" class="text-muted-foreground transition-all hover:text-primary">16. SPOC</a><!----></li><!--]--></ul></li><!--]--></ul><div class="pt-5 text-muted-foreground"><!--[--><!--]--></div></div><div class="flex-grow"></div><!----></div><!--]--><!--]--></div></div><style> /* Hide scrollbars cross-browser and enable momentum scroll for touch devices */ [data-radix-scroll-area-viewport] { scrollbar-width:none; -ms-overflow-style:none; -webkit-overflow-scrolling:touch; } [data-radix-scroll-area-viewport]::-webkit-scrollbar { display:none; } </style><!--]--><!----><!----><!--]--></div></div></div></main><!--]--></div></div><!--[--><!--[--><!--[--><!--]--><div role="region" aria-label="Notifications (F8)" tabindex="-1" style="pointer-events:none;"><!--[--><!----><ol tabindex="-1" class="fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]"><!--[--><!--]--></ol><!----><!--]--></div><!--]--><!--]--><footer class="py-6 text-muted-foreground md:px-8 md:py-0"><div class="container flex flex-col items-center justify-between gap-2 md:h-24 md:flex-row"><!--[--><div class="text-sm"><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[-->Copyright  2025<!--]--></p></div><!--]--><span class="flex-1"></span><!--[--><a href="https://github.com/CyberNachos" rel="noopener noreferrer" target="_blank"><button class="items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 flex gap-2"><!--[--><span class="iconify i-lucide:github" aria-hidden="true" style="font-size:20px;"></span><!----><!--]--></button></a><!--]--></div></footer><!--]--></div><div id="teleports"></div>
<script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/robotics-overview/robotics-datasets/_payload.json?1defcafe-82b4-4ebd-9604-7ed2beb079ac">[{"state":1,"once":2178,"_errors":2179,"serverRendered":5,"path":11,"prerenderedAt":2182},["Reactive",2],{"$scolor-mode":3,"$sdd-pages":7,"$sdd-surrounds":2029,"$sdd-globals":2051,"$sdd-navigation":2053,"$sdocs-collapsed-map":2169,"$ssite-config":2170},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,["ShallowRef",8],["ShallowReactive",9],{"/robotics-overview/robotics-datasets":10},{"_path":11,"_dir":12,"_draft":6,"_partial":6,"_locale":13,"title":14,"description":15,"icon":16,"lastModified":17,"body":18,"_type":2022,"_id":2023,"_source":2024,"_file":2025,"_stem":2026,"_extension":2027,"layout":2028},"/robotics-overview/robotics-datasets","robotics-overview","","Robotics Dataset Comparison","A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, DobbE, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.","lucide:database","Mar 28, 2025",{"type":19,"children":20,"toc":1999},"root",[21,29,40,47,834,838,844,851,864,872,887,895,908,916,924,927,933,945,952,965,972,985,992,1000,1003,1009,1021,1028,1041,1048,1061,1068,1076,1079,1085,1097,1104,1117,1124,1137,1144,1152,1155,1161,1173,1180,1188,1195,1203,1210,1218,1221,1227,1239,1246,1254,1261,1269,1276,1284,1287,1293,1305,1312,1320,1327,1335,1342,1350,1353,1359,1371,1378,1386,1393,1401,1408,1416,1419,1425,1437,1444,1457,1464,1472,1479,1487,1490,1496,1508,1515,1528,1535,1548,1555,1563,1566,1572,1584,1591,1604,1611,1624,1631,1639,1642,1648,1660,1667,1680,1687,1700,1707,1715,1718,1724,1736,1743,1751,1758,1766,1773,1781,1784,1790,1802,1809,1817,1824,1837,1844,1852,1855,1861,1873,1880,1888,1895,1908,1915,1923,1926,1932,1944,1951,1964,1971,1984,1991],{"type":22,"tag":23,"props":24,"children":25},"element","p",{},[26],{"type":27,"value":28},"text","Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table that highlights key characteristics, and second, detailed descriptions of each dataset's scope, technical features, advantages, and disadvantages.",{"type":22,"tag":23,"props":30,"children":31},{},[32,38],{"type":22,"tag":33,"props":34,"children":35},"strong",{},[36],{"type":27,"value":37},"Note:",{"type":27,"value":39}," This analysis is accurate as of the last modified date, \"Mar 28, 2025.\"",{"type":22,"tag":41,"props":42,"children":44},"h2",{"id":43},"summary-table",[45],{"type":27,"value":46},"Summary Table",{"type":22,"tag":48,"props":49,"children":50},"table",{},[51,100],{"type":22,"tag":52,"props":53,"children":54},"thead",{},[55],{"type":22,"tag":56,"props":57,"children":58},"tr",{},[59,68,76,84,92],{"type":22,"tag":60,"props":61,"children":62},"th",{},[63],{"type":22,"tag":33,"props":64,"children":65},{},[66],{"type":27,"value":67},"Dataset / Framework",{"type":22,"tag":60,"props":69,"children":70},{},[71],{"type":22,"tag":33,"props":72,"children":73},{},[74],{"type":27,"value":75},"Scope & Application",{"type":22,"tag":60,"props":77,"children":78},{},[79],{"type":22,"tag":33,"props":80,"children":81},{},[82],{"type":27,"value":83},"Scale & Modalities",{"type":22,"tag":60,"props":85,"children":86},{},[87],{"type":22,"tag":33,"props":88,"children":89},{},[90],{"type":27,"value":91},"Key Advantages",{"type":22,"tag":60,"props":93,"children":94},{},[95],{"type":22,"tag":33,"props":96,"children":97},{},[98],{"type":27,"value":99},"Key Disadvantages",{"type":22,"tag":101,"props":102,"children":103},"tbody",{},[104,157,203,248,293,339,384,429,474,519,564,609,654,699,744,789],{"type":22,"tag":56,"props":105,"children":106},{},[107,137,142,147,152],{"type":22,"tag":108,"props":109,"children":110},"td",{},[111,113,118,120,124,126,135],{"type":27,"value":112},"1. ",{"type":22,"tag":33,"props":114,"children":115},{},[116],{"type":27,"value":117},"LeRobot",{"type":27,"value":119}," ",{"type":22,"tag":121,"props":122,"children":123},"br",{},[],{"type":27,"value":125}," (",{"type":22,"tag":127,"props":128,"children":132},"a",{"href":129,"rel":130},"https://github.com/huggingface/lerobot",[131],"nofollow",[133],{"type":27,"value":134},"GitHub",{"type":27,"value":136},")",{"type":22,"tag":108,"props":138,"children":139},{},[140],{"type":27,"value":141},"Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots.",{"type":22,"tag":108,"props":143,"children":144},{},[145],{"type":27,"value":146},"Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context.",{"type":22,"tag":108,"props":148,"children":149},{},[150],{"type":27,"value":151},"End-to-end learning with community support; integrated simulation environments.",{"type":22,"tag":108,"props":153,"children":154},{},[155],{"type":27,"value":156},"Complex setup; may require substantial computing and sensor calibration.",{"type":22,"tag":56,"props":158,"children":159},{},[160,183,188,193,198],{"type":22,"tag":108,"props":161,"children":162},{},[163,165,170,171,174,175,182],{"type":27,"value":164},"2. ",{"type":22,"tag":33,"props":166,"children":167},{},[168],{"type":27,"value":169},"Open X-Embodiment",{"type":27,"value":119},{"type":22,"tag":121,"props":172,"children":173},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":176,"children":179},{"href":177,"rel":178},"https://robotics-transformer-x.github.io/",[131],[180],{"type":27,"value":181},"Website",{"type":27,"value":136},{"type":22,"tag":108,"props":184,"children":185},{},[186],{"type":27,"value":187},"Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions.",{"type":22,"tag":108,"props":189,"children":190},{},[191],{"type":27,"value":192},"1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data.",{"type":22,"tag":108,"props":194,"children":195},{},[196],{"type":27,"value":197},"Massive diversity enabling cross-robot transfer and positive knowledge sharing.",{"type":22,"tag":108,"props":199,"children":200},{},[201],{"type":27,"value":202},"Heterogeneous quality and potential standardization issues across varied sources.",{"type":22,"tag":56,"props":204,"children":205},{},[206,228,233,238,243],{"type":22,"tag":108,"props":207,"children":208},{},[209,211,216,217,220,221,227],{"type":27,"value":210},"3. ",{"type":22,"tag":33,"props":212,"children":213},{},[214],{"type":27,"value":215},"DROID",{"type":27,"value":119},{"type":22,"tag":121,"props":218,"children":219},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":222,"children":225},{"href":223,"rel":224},"https://droid-dataset.github.io/",[131],[226],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":229,"children":230},{},[231],{"type":27,"value":232},"In-the-wild robot manipulation for robust imitation learning.",{"type":22,"tag":108,"props":234,"children":235},{},[236],{"type":27,"value":237},"76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints.",{"type":22,"tag":108,"props":239,"children":240},{},[241],{"type":27,"value":242},"Diverse, large-scale manipulation data that improves policy robustness.",{"type":22,"tag":108,"props":244,"children":245},{},[246],{"type":27,"value":247},"Mostly limited to manipulation with a specific hardware setup; less diversity in task types.",{"type":22,"tag":56,"props":249,"children":250},{},[251,273,278,283,288],{"type":22,"tag":108,"props":252,"children":253},{},[254,256,261,262,265,266,272],{"type":27,"value":255},"4. ",{"type":22,"tag":33,"props":257,"children":258},{},[259],{"type":27,"value":260},"RoboTurk",{"type":27,"value":119},{"type":22,"tag":121,"props":263,"children":264},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":267,"children":270},{"href":268,"rel":269},"https://roboturk.stanford.edu/",[131],[271],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":274,"children":275},{},[276],{"type":27,"value":277},"Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection.",{"type":22,"tag":108,"props":279,"children":280},{},[281],{"type":27,"value":282},"Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions.",{"type":22,"tag":108,"props":284,"children":285},{},[286],{"type":27,"value":287},"Leverages non-expert, scalable human demonstrations; supports collaborative tasks.",{"type":22,"tag":108,"props":289,"children":290},{},[291],{"type":27,"value":292},"Variation in demonstration quality and potential limits in scale compared to fully automated data collection.",{"type":22,"tag":56,"props":294,"children":295},{},[296,319,324,329,334],{"type":22,"tag":108,"props":297,"children":298},{},[299,301,306,307,310,311,318],{"type":27,"value":300},"5. ",{"type":22,"tag":33,"props":302,"children":303},{},[304],{"type":27,"value":305},"MIME",{"type":27,"value":119},{"type":22,"tag":121,"props":308,"children":309},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":312,"children":315},{"href":313,"rel":314},"https://sites.google.com/view/mimedataset/dataset?authuser=0",[131],[316],{"type":27,"value":317},"Google Sites",{"type":27,"value":136},{"type":22,"tag":108,"props":320,"children":321},{},[322],{"type":27,"value":323},"Imitation learning for robot manipulation using human demonstrations.",{"type":22,"tag":108,"props":325,"children":326},{},[327],{"type":27,"value":328},"Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories.",{"type":22,"tag":108,"props":330,"children":331},{},[332],{"type":27,"value":333},"Focus on high-quality manipulation trajectories; well-suited for imitation learning.",{"type":22,"tag":108,"props":335,"children":336},{},[337],{"type":27,"value":338},"May be smaller in scale and less diverse than some large-scale multi-robot datasets.",{"type":22,"tag":56,"props":340,"children":341},{},[342,364,369,374,379],{"type":22,"tag":108,"props":343,"children":344},{},[345,347,352,353,356,357,363],{"type":27,"value":346},"6. ",{"type":22,"tag":33,"props":348,"children":349},{},[350],{"type":27,"value":351},"Meta-World",{"type":27,"value":119},{"type":22,"tag":121,"props":354,"children":355},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":358,"children":361},{"href":359,"rel":360},"https://meta-world.github.io/",[131],[362],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":365,"children":366},{},[367],{"type":27,"value":368},"Benchmark for multi-task and meta-reinforcement learning in simulation.",{"type":22,"tag":108,"props":370,"children":371},{},[372],{"type":27,"value":373},"50 distinct simulated manipulation environments; task variations with visual observations.",{"type":22,"tag":108,"props":375,"children":376},{},[377],{"type":27,"value":378},"Standardized benchmark for meta-RL; structured for evaluating generalization.",{"type":22,"tag":108,"props":380,"children":381},{},[382],{"type":27,"value":383},"Limited to simulation and may not capture the full variability of real-world settings.",{"type":22,"tag":56,"props":385,"children":386},{},[387,409,414,419,424],{"type":22,"tag":108,"props":388,"children":389},{},[390,392,397,398,401,402,408],{"type":27,"value":391},"7. ",{"type":22,"tag":33,"props":393,"children":394},{},[395],{"type":27,"value":396},"RoboNet",{"type":27,"value":119},{"type":22,"tag":121,"props":399,"children":400},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":403,"children":406},{"href":404,"rel":405},"https://www.robonet.wiki/",[131],[407],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":410,"children":411},{},[412],{"type":27,"value":413},"Open database of real robotic experience for manipulation tasks across multiple platforms.",{"type":22,"tag":108,"props":415,"children":416},{},[417],{"type":27,"value":418},"~15M video frames, collected from 7 robot platforms with diverse camera viewpoints.",{"type":22,"tag":108,"props":420,"children":421},{},[422],{"type":27,"value":423},"Large-scale, multi-platform real-world data that facilitates cross-robot generalization.",{"type":22,"tag":108,"props":425,"children":426},{},[427],{"type":27,"value":428},"Very high storage and processing requirements; complex data integration.",{"type":22,"tag":56,"props":430,"children":431},{},[432,454,459,464,469],{"type":22,"tag":108,"props":433,"children":434},{},[435,437,442,443,446,447,453],{"type":27,"value":436},"8. ",{"type":22,"tag":33,"props":438,"children":439},{},[440],{"type":27,"value":441},"RoboSet",{"type":27,"value":119},{"type":22,"tag":121,"props":444,"children":445},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":448,"children":451},{"href":449,"rel":450},"https://robopen.github.io/roboset/",[131],[452],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":455,"children":456},{},[457],{"type":27,"value":458},"Multi-task dataset for household (kitchen) manipulation tasks, including language instructions.",{"type":22,"tag":108,"props":460,"children":461},{},[462],{"type":27,"value":463},"28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame.",{"type":22,"tag":108,"props":465,"children":466},{},[467],{"type":27,"value":468},"Rich, multi-modal data in realistic home environments; supports language-guided sequencing.",{"type":22,"tag":108,"props":470,"children":471},{},[472],{"type":27,"value":473},"Domain-specific (largely kitchens); may not generalize to non-domestic scenarios.",{"type":22,"tag":56,"props":475,"children":476},{},[477,499,504,509,514],{"type":22,"tag":108,"props":478,"children":479},{},[480,482,487,488,491,492,498],{"type":27,"value":481},"9. ",{"type":22,"tag":33,"props":483,"children":484},{},[485],{"type":27,"value":486},"BridgeData V2",{"type":27,"value":119},{"type":22,"tag":121,"props":489,"children":490},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":493,"children":496},{"href":494,"rel":495},"https://rail-berkeley.github.io/bridgedata/",[131],[497],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":500,"children":501},{},[502],{"type":27,"value":503},"Large-scale robotic manipulation across diverse environments and skills with language annotations.",{"type":22,"tag":108,"props":505,"children":506},{},[507],{"type":27,"value":508},"~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language.",{"type":22,"tag":108,"props":510,"children":511},{},[512],{"type":27,"value":513},"Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning.",{"type":22,"tag":108,"props":515,"children":516},{},[517],{"type":27,"value":518},"Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges.",{"type":22,"tag":56,"props":520,"children":521},{},[522,544,549,554,559],{"type":22,"tag":108,"props":523,"children":524},{},[525,527,532,533,536,537,543],{"type":27,"value":526},"10. ",{"type":22,"tag":33,"props":528,"children":529},{},[530],{"type":27,"value":531},"RT-1",{"type":27,"value":119},{"type":22,"tag":121,"props":534,"children":535},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":538,"children":541},{"href":539,"rel":540},"https://robotics-transformer1.github.io/",[131],[542],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":545,"children":546},{},[547],{"type":27,"value":548},"Real-world imitation learning for multi-task manipulation using transformer architectures.",{"type":22,"tag":108,"props":550,"children":551},{},[552],{"type":27,"value":553},"Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control.",{"type":22,"tag":108,"props":555,"children":556},{},[557],{"type":27,"value":558},"Outstanding generalization and performance on diverse tasks; scalable transformer model.",{"type":22,"tag":108,"props":560,"children":561},{},[562],{"type":27,"value":563},"High training and computational requirements; system complexity may be a barrier.",{"type":22,"tag":56,"props":565,"children":566},{},[567,589,594,599,604],{"type":22,"tag":108,"props":568,"children":569},{},[570,572,577,578,581,582,588],{"type":27,"value":571},"11. ",{"type":22,"tag":33,"props":573,"children":574},{},[575],{"type":27,"value":576},"DobbE",{"type":27,"value":119},{"type":22,"tag":121,"props":579,"children":580},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":583,"children":586},{"href":584,"rel":585},"https://dobb-e.com/",[131],[587],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":590,"children":591},{},[592],{"type":27,"value":593},"Framework for home robotics: learning household manipulation tasks quickly in real homes.",{"type":22,"tag":108,"props":595,"children":596},{},[597],{"type":27,"value":598},"HoNY dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30fps; also includes hardware (the Stick) for data collection.",{"type":22,"tag":108,"props":600,"children":601},{},[602],{"type":27,"value":603},"Cost-effective, rapid task learning with real household data; designed for generalist home robots.",{"type":22,"tag":108,"props":605,"children":606},{},[607],{"type":27,"value":608},"Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations.",{"type":22,"tag":56,"props":610,"children":611},{},[612,634,639,644,649],{"type":22,"tag":108,"props":613,"children":614},{},[615,617,622,623,626,627,633],{"type":27,"value":616},"12. ",{"type":22,"tag":33,"props":618,"children":619},{},[620],{"type":27,"value":621},"RH20T",{"type":27,"value":119},{"type":22,"tag":121,"props":624,"children":625},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":628,"children":631},{"href":629,"rel":630},"https://rh20t.github.io/",[131],[632],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":635,"children":636},{},[637],{"type":27,"value":638},"Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world.",{"type":22,"tag":108,"props":640,"children":641},{},[642],{"type":27,"value":643},"Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data.",{"type":22,"tag":108,"props":645,"children":646},{},[647],{"type":27,"value":648},"Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning.",{"type":22,"tag":108,"props":650,"children":651},{},[652],{"type":27,"value":653},"Very large and complex; requires significant computational and storage resources; complex data processing pipeline.",{"type":22,"tag":56,"props":655,"children":656},{},[657,679,684,689,694],{"type":22,"tag":108,"props":658,"children":659},{},[660,662,667,668,671,672,678],{"type":27,"value":661},"13. ",{"type":22,"tag":33,"props":663,"children":664},{},[665],{"type":27,"value":666},"BC-Z",{"type":27,"value":119},{"type":22,"tag":121,"props":669,"children":670},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":673,"children":676},{"href":674,"rel":675},"https://sites.google.com/view/bc-z/home?pli=1",[131],[677],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":680,"children":681},{},[682],{"type":27,"value":683},"Large-scale behavior cloning for robotic manipulation.",{"type":22,"tag":108,"props":685,"children":686},{},[687],{"type":27,"value":688},"(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)",{"type":22,"tag":108,"props":690,"children":691},{},[692],{"type":27,"value":693},"Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms.",{"type":22,"tag":108,"props":695,"children":696},{},[697],{"type":27,"value":698},"May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets.",{"type":22,"tag":56,"props":700,"children":701},{},[702,724,729,734,739],{"type":22,"tag":108,"props":703,"children":704},{},[705,707,712,713,716,717,723],{"type":27,"value":706},"14. ",{"type":22,"tag":33,"props":708,"children":709},{},[710],{"type":27,"value":711},"MT-Opt",{"type":27,"value":119},{"type":22,"tag":121,"props":714,"children":715},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":718,"children":721},{"href":719,"rel":720},"https://karolhausman.github.io/mt-opt/",[131],[722],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":725,"children":726},{},[727],{"type":27,"value":728},"Multi-task reinforcement learning at scale across many manipulation skills.",{"type":22,"tag":108,"props":730,"children":731},{},[732],{"type":27,"value":733},"Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework.",{"type":22,"tag":108,"props":735,"children":736},{},[737],{"type":27,"value":738},"Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience.",{"type":22,"tag":108,"props":740,"children":741},{},[742],{"type":27,"value":743},"Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination.",{"type":22,"tag":56,"props":745,"children":746},{},[747,769,774,779,784],{"type":22,"tag":108,"props":748,"children":749},{},[750,752,757,758,761,762,768],{"type":27,"value":751},"15. ",{"type":22,"tag":33,"props":753,"children":754},{},[755],{"type":27,"value":756},"VIMA",{"type":27,"value":119},{"type":22,"tag":121,"props":759,"children":760},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":763,"children":766},{"href":764,"rel":765},"https://vimalabs.github.io/",[131],[767],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":770,"children":771},{},[772],{"type":27,"value":773},"General robot manipulation via multimodal prompts (combining language and vision) for unified task specification.",{"type":22,"tag":108,"props":775,"children":776},{},[777],{"type":27,"value":778},"Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models.",{"type":22,"tag":108,"props":780,"children":781},{},[782],{"type":27,"value":783},"Unified formulation that prompts the robot to perform diverse tasks; highly scalable and sample-efficient.",{"type":22,"tag":108,"props":785,"children":786},{},[787],{"type":27,"value":788},"Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation.",{"type":22,"tag":56,"props":790,"children":791},{},[792,814,819,824,829],{"type":22,"tag":108,"props":793,"children":794},{},[795,797,802,803,806,807,813],{"type":27,"value":796},"16. ",{"type":22,"tag":33,"props":798,"children":799},{},[800],{"type":27,"value":801},"SPOC",{"type":27,"value":119},{"type":22,"tag":121,"props":804,"children":805},{},[],{"type":27,"value":125},{"type":22,"tag":127,"props":808,"children":811},{"href":809,"rel":810},"https://spoc-robot.github.io/",[131],[812],{"type":27,"value":181},{"type":27,"value":136},{"type":22,"tag":108,"props":815,"children":816},{},[817],{"type":27,"value":818},"Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world).",{"type":22,"tag":108,"props":820,"children":821},{},[822],{"type":27,"value":823},"Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation.",{"type":22,"tag":108,"props":825,"children":826},{},[827],{"type":27,"value":828},"Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info.",{"type":22,"tag":108,"props":830,"children":831},{},[832],{"type":27,"value":833},"RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios.",{"type":22,"tag":835,"props":836,"children":837},"hr",{},[],{"type":22,"tag":41,"props":839,"children":841},{"id":840},"detailed-comparison",[842],{"type":27,"value":843},"Detailed Comparison",{"type":22,"tag":845,"props":846,"children":848},"h3",{"id":847},"_1-lerobot",[849],{"type":27,"value":850},"1. LeRobot",{"type":22,"tag":23,"props":852,"children":853},{},[854,859,862],{"type":22,"tag":33,"props":855,"children":856},{},[857],{"type":27,"value":858},"Scope & Application:",{"type":22,"tag":121,"props":860,"children":861},{},[],{"type":27,"value":863},"\nLeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots.",{"type":22,"tag":23,"props":865,"children":866},{},[867],{"type":22,"tag":33,"props":868,"children":869},{},[870],{"type":27,"value":871},"Technical Features:",{"type":22,"tag":873,"props":874,"children":875},"ul",{},[876,882],{"type":22,"tag":877,"props":878,"children":879},"li",{},[880],{"type":27,"value":881},"Built in PyTorch with modular dataset classes that support multi-frame temporal sampling.",{"type":22,"tag":877,"props":883,"children":884},{},[885],{"type":27,"value":886},"Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments.",{"type":22,"tag":23,"props":888,"children":889},{},[890],{"type":22,"tag":33,"props":891,"children":892},{},[893],{"type":27,"value":894},"Advantages:",{"type":22,"tag":873,"props":896,"children":897},{},[898,903],{"type":22,"tag":877,"props":899,"children":900},{},[901],{"type":27,"value":902},"Community-driven with active contributions and hosted on Hugging Face.",{"type":22,"tag":877,"props":904,"children":905},{},[906],{"type":27,"value":907},"Facilitates rapid prototyping in robotics with an accessible codebase.",{"type":22,"tag":23,"props":909,"children":910},{},[911],{"type":22,"tag":33,"props":912,"children":913},{},[914],{"type":27,"value":915},"Disadvantages:",{"type":22,"tag":873,"props":917,"children":918},{},[919],{"type":22,"tag":877,"props":920,"children":921},{},[922],{"type":27,"value":923},"Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise.",{"type":22,"tag":835,"props":925,"children":926},{},[],{"type":22,"tag":845,"props":928,"children":930},{"id":929},"_2-open-x-embodiment",[931],{"type":27,"value":932},"2. Open X-Embodiment",{"type":22,"tag":23,"props":934,"children":935},{},[936,940,943],{"type":22,"tag":33,"props":937,"children":938},{},[939],{"type":27,"value":858},{"type":22,"tag":121,"props":941,"children":942},{},[],{"type":27,"value":944},"\nA collaborative effort pooling robot data from 21 institutions, it is aimed at training generalist policies across 22 different robot embodiments.",{"type":22,"tag":23,"props":946,"children":947},{},[948],{"type":22,"tag":33,"props":949,"children":950},{},[951],{"type":27,"value":871},{"type":22,"tag":873,"props":953,"children":954},{},[955,960],{"type":22,"tag":877,"props":956,"children":957},{},[958],{"type":27,"value":959},"Aggregates 1M+ trajectories from diverse robots and tasks.",{"type":22,"tag":877,"props":961,"children":962},{},[963],{"type":27,"value":964},"Supports learning via transformer-based architectures that can generalize across different embodiments.",{"type":22,"tag":23,"props":966,"children":967},{},[968],{"type":22,"tag":33,"props":969,"children":970},{},[971],{"type":27,"value":894},{"type":22,"tag":873,"props":973,"children":974},{},[975,980],{"type":22,"tag":877,"props":976,"children":977},{},[978],{"type":27,"value":979},"Unmatched diversity, which is ideal for studying cross-robot transfer.",{"type":22,"tag":877,"props":981,"children":982},{},[983],{"type":27,"value":984},"Large scale increases the potential for generalization.",{"type":22,"tag":23,"props":986,"children":987},{},[988],{"type":22,"tag":33,"props":989,"children":990},{},[991],{"type":27,"value":915},{"type":22,"tag":873,"props":993,"children":994},{},[995],{"type":22,"tag":877,"props":996,"children":997},{},[998],{"type":27,"value":999},"The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging.",{"type":22,"tag":835,"props":1001,"children":1002},{},[],{"type":22,"tag":845,"props":1004,"children":1006},{"id":1005},"_3-droid",[1007],{"type":27,"value":1008},"3. DROID",{"type":22,"tag":23,"props":1010,"children":1011},{},[1012,1016,1019],{"type":22,"tag":33,"props":1013,"children":1014},{},[1015],{"type":27,"value":858},{"type":22,"tag":121,"props":1017,"children":1018},{},[],{"type":27,"value":1020},"\nFocused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots.",{"type":22,"tag":23,"props":1022,"children":1023},{},[1024],{"type":22,"tag":33,"props":1025,"children":1026},{},[1027],{"type":27,"value":871},{"type":22,"tag":873,"props":1029,"children":1030},{},[1031,1036],{"type":22,"tag":877,"props":1032,"children":1033},{},[1034],{"type":27,"value":1035},"Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks.",{"type":22,"tag":877,"props":1037,"children":1038},{},[1039],{"type":27,"value":1040},"Multi-camera views (including wrist and exterior images) enable rich visual inputs.",{"type":22,"tag":23,"props":1042,"children":1043},{},[1044],{"type":22,"tag":33,"props":1045,"children":1046},{},[1047],{"type":27,"value":894},{"type":22,"tag":873,"props":1049,"children":1050},{},[1051,1056],{"type":22,"tag":877,"props":1052,"children":1053},{},[1054],{"type":27,"value":1055},"Large, diverse dataset that significantly boosts policy performance and robustness.",{"type":22,"tag":877,"props":1057,"children":1058},{},[1059],{"type":27,"value":1060},"Extensive coverage of real-world scenarios.",{"type":22,"tag":23,"props":1062,"children":1063},{},[1064],{"type":22,"tag":33,"props":1065,"children":1066},{},[1067],{"type":27,"value":915},{"type":22,"tag":873,"props":1069,"children":1070},{},[1071],{"type":22,"tag":877,"props":1072,"children":1073},{},[1074],{"type":27,"value":1075},"Being collected with a specific hardware platform, its applicability to other robots may be limited.",{"type":22,"tag":835,"props":1077,"children":1078},{},[],{"type":22,"tag":845,"props":1080,"children":1082},{"id":1081},"_4-roboturk",[1083],{"type":27,"value":1084},"4. RoboTurk",{"type":22,"tag":23,"props":1086,"children":1087},{},[1088,1092,1095],{"type":22,"tag":33,"props":1089,"children":1090},{},[1091],{"type":27,"value":858},{"type":22,"tag":121,"props":1093,"children":1094},{},[],{"type":27,"value":1096},"\nRoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks.",{"type":22,"tag":23,"props":1098,"children":1099},{},[1100],{"type":22,"tag":33,"props":1101,"children":1102},{},[1103],{"type":27,"value":871},{"type":22,"tag":873,"props":1105,"children":1106},{},[1107,1112],{"type":22,"tag":877,"props":1108,"children":1109},{},[1110],{"type":27,"value":1111},"Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset).",{"type":22,"tag":877,"props":1113,"children":1114},{},[1115],{"type":27,"value":1116},"Includes system features for low-latency teleoperation and human-in-the-loop interventions.",{"type":22,"tag":23,"props":1118,"children":1119},{},[1120],{"type":22,"tag":33,"props":1121,"children":1122},{},[1123],{"type":27,"value":894},{"type":22,"tag":873,"props":1125,"children":1126},{},[1127,1132],{"type":22,"tag":877,"props":1128,"children":1129},{},[1130],{"type":27,"value":1131},"Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations.",{"type":22,"tag":877,"props":1133,"children":1134},{},[1135],{"type":27,"value":1136},"Proven effectiveness in enabling imitation learning on challenging tasks.",{"type":22,"tag":23,"props":1138,"children":1139},{},[1140],{"type":22,"tag":33,"props":1141,"children":1142},{},[1143],{"type":27,"value":915},{"type":22,"tag":873,"props":1145,"children":1146},{},[1147],{"type":22,"tag":877,"props":1148,"children":1149},{},[1150],{"type":27,"value":1151},"The quality of demonstrations may vary due to differences in human teleoperation skills.",{"type":22,"tag":835,"props":1153,"children":1154},{},[],{"type":22,"tag":845,"props":1156,"children":1158},{"id":1157},"_5-mime",[1159],{"type":27,"value":1160},"5. MIME",{"type":22,"tag":23,"props":1162,"children":1163},{},[1164,1168,1171],{"type":22,"tag":33,"props":1165,"children":1166},{},[1167],{"type":27,"value":858},{"type":22,"tag":121,"props":1169,"children":1170},{},[],{"type":27,"value":1172},"\nMIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors.",{"type":22,"tag":23,"props":1174,"children":1175},{},[1176],{"type":22,"tag":33,"props":1177,"children":1178},{},[1179],{"type":27,"value":871},{"type":22,"tag":873,"props":1181,"children":1182},{},[1183],{"type":22,"tag":877,"props":1184,"children":1185},{},[1186],{"type":27,"value":1187},"Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation.",{"type":22,"tag":23,"props":1189,"children":1190},{},[1191],{"type":22,"tag":33,"props":1192,"children":1193},{},[1194],{"type":27,"value":894},{"type":22,"tag":873,"props":1196,"children":1197},{},[1198],{"type":22,"tag":877,"props":1199,"children":1200},{},[1201],{"type":27,"value":1202},"Focused on detailed manipulation tasks, making it ideal for imitation learning studies.",{"type":22,"tag":23,"props":1204,"children":1205},{},[1206],{"type":22,"tag":33,"props":1207,"children":1208},{},[1209],{"type":27,"value":915},{"type":22,"tag":873,"props":1211,"children":1212},{},[1213],{"type":22,"tag":877,"props":1214,"children":1215},{},[1216],{"type":27,"value":1217},"Generally smaller in scale compared to some of the largest datasets; might offer limited diversity.",{"type":22,"tag":835,"props":1219,"children":1220},{},[],{"type":22,"tag":845,"props":1222,"children":1224},{"id":1223},"_6-meta-world",[1225],{"type":27,"value":1226},"6. Meta-World",{"type":22,"tag":23,"props":1228,"children":1229},{},[1230,1234,1237],{"type":22,"tag":33,"props":1231,"children":1232},{},[1233],{"type":27,"value":858},{"type":22,"tag":121,"props":1235,"children":1236},{},[],{"type":27,"value":1238},"\nA simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments.",{"type":22,"tag":23,"props":1240,"children":1241},{},[1242],{"type":22,"tag":33,"props":1243,"children":1244},{},[1245],{"type":27,"value":871},{"type":22,"tag":873,"props":1247,"children":1248},{},[1249],{"type":22,"tag":877,"props":1250,"children":1251},{},[1252],{"type":27,"value":1253},"Structured environments with varying goal positions and task variations to test generalization.",{"type":22,"tag":23,"props":1255,"children":1256},{},[1257],{"type":22,"tag":33,"props":1258,"children":1259},{},[1260],{"type":27,"value":894},{"type":22,"tag":873,"props":1262,"children":1263},{},[1264],{"type":22,"tag":877,"props":1265,"children":1266},{},[1267],{"type":27,"value":1268},"Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms.",{"type":22,"tag":23,"props":1270,"children":1271},{},[1272],{"type":22,"tag":33,"props":1273,"children":1274},{},[1275],{"type":27,"value":915},{"type":22,"tag":873,"props":1277,"children":1278},{},[1279],{"type":22,"tag":877,"props":1280,"children":1281},{},[1282],{"type":27,"value":1283},"Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured.",{"type":22,"tag":835,"props":1285,"children":1286},{},[],{"type":22,"tag":845,"props":1288,"children":1290},{"id":1289},"_7-robonet",[1291],{"type":27,"value":1292},"7. RoboNet",{"type":22,"tag":23,"props":1294,"children":1295},{},[1296,1300,1303],{"type":22,"tag":33,"props":1297,"children":1298},{},[1299],{"type":27,"value":858},{"type":22,"tag":121,"props":1301,"children":1302},{},[],{"type":27,"value":1304},"\nRoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation.",{"type":22,"tag":23,"props":1306,"children":1307},{},[1308],{"type":22,"tag":33,"props":1309,"children":1310},{},[1311],{"type":27,"value":871},{"type":22,"tag":873,"props":1313,"children":1314},{},[1315],{"type":22,"tag":877,"props":1316,"children":1317},{},[1318],{"type":27,"value":1319},"Contains over 15M video frames and data from multiple camera viewpoints.",{"type":22,"tag":23,"props":1321,"children":1322},{},[1323],{"type":22,"tag":33,"props":1324,"children":1325},{},[1326],{"type":27,"value":894},{"type":22,"tag":873,"props":1328,"children":1329},{},[1330],{"type":22,"tag":877,"props":1331,"children":1332},{},[1333],{"type":27,"value":1334},"Offers vast amounts of real-world data to study generalization across different robot hardware.",{"type":22,"tag":23,"props":1336,"children":1337},{},[1338],{"type":22,"tag":33,"props":1339,"children":1340},{},[1341],{"type":27,"value":915},{"type":22,"tag":873,"props":1343,"children":1344},{},[1345],{"type":22,"tag":877,"props":1346,"children":1347},{},[1348],{"type":27,"value":1349},"Requires heavy storage and processing; integrating multi-platform data can be challenging.",{"type":22,"tag":835,"props":1351,"children":1352},{},[],{"type":22,"tag":845,"props":1354,"children":1356},{"id":1355},"_8-roboset",[1357],{"type":27,"value":1358},"8. RoboSet",{"type":22,"tag":23,"props":1360,"children":1361},{},[1362,1366,1369],{"type":22,"tag":33,"props":1363,"children":1364},{},[1365],{"type":27,"value":858},{"type":22,"tag":121,"props":1367,"children":1368},{},[],{"type":27,"value":1370},"\nA dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions.",{"type":22,"tag":23,"props":1372,"children":1373},{},[1374],{"type":22,"tag":33,"props":1375,"children":1376},{},[1377],{"type":27,"value":871},{"type":22,"tag":873,"props":1379,"children":1380},{},[1381],{"type":22,"tag":877,"props":1382,"children":1383},{},[1384],{"type":27,"value":1385},"28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped.",{"type":22,"tag":23,"props":1387,"children":1388},{},[1389],{"type":22,"tag":33,"props":1390,"children":1391},{},[1392],{"type":27,"value":894},{"type":22,"tag":873,"props":1394,"children":1395},{},[1396],{"type":22,"tag":877,"props":1397,"children":1398},{},[1399],{"type":27,"value":1400},"Rich multi-modal information (visual + language) supports language-guided robotic learning.",{"type":22,"tag":23,"props":1402,"children":1403},{},[1404],{"type":22,"tag":33,"props":1405,"children":1406},{},[1407],{"type":27,"value":915},{"type":22,"tag":873,"props":1409,"children":1410},{},[1411],{"type":22,"tag":877,"props":1412,"children":1413},{},[1414],{"type":27,"value":1415},"Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios.",{"type":22,"tag":835,"props":1417,"children":1418},{},[],{"type":22,"tag":845,"props":1420,"children":1422},{"id":1421},"_9-bridgedata-v2",[1423],{"type":27,"value":1424},"9. BridgeData V2",{"type":22,"tag":23,"props":1426,"children":1427},{},[1428,1432,1435],{"type":22,"tag":33,"props":1429,"children":1430},{},[1431],{"type":27,"value":858},{"type":22,"tag":121,"props":1433,"children":1434},{},[],{"type":27,"value":1436},"\nDesigned to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning.",{"type":22,"tag":23,"props":1438,"children":1439},{},[1440],{"type":22,"tag":33,"props":1441,"children":1442},{},[1443],{"type":27,"value":871},{"type":22,"tag":873,"props":1445,"children":1446},{},[1447,1452],{"type":22,"tag":877,"props":1448,"children":1449},{},[1450],{"type":27,"value":1451},"Approximately 60K trajectories with multi-view RGB (and some depth) data.",{"type":22,"tag":877,"props":1453,"children":1454},{},[1455],{"type":27,"value":1456},"Includes both teleoperated and scripted demonstrations.",{"type":22,"tag":23,"props":1458,"children":1459},{},[1460],{"type":22,"tag":33,"props":1461,"children":1462},{},[1463],{"type":27,"value":894},{"type":22,"tag":873,"props":1465,"children":1466},{},[1467],{"type":22,"tag":877,"props":1468,"children":1469},{},[1470],{"type":27,"value":1471},"High diversity in environments and tasks; strong support for language-conditioned policy learning.",{"type":22,"tag":23,"props":1473,"children":1474},{},[1475],{"type":22,"tag":33,"props":1476,"children":1477},{},[1478],{"type":27,"value":915},{"type":22,"tag":873,"props":1480,"children":1481},{},[1482],{"type":22,"tag":877,"props":1483,"children":1484},{},[1485],{"type":27,"value":1486},"Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing.",{"type":22,"tag":835,"props":1488,"children":1489},{},[],{"type":22,"tag":845,"props":1491,"children":1493},{"id":1492},"_10-rt-1",[1494],{"type":27,"value":1495},"10. RT-1",{"type":22,"tag":23,"props":1497,"children":1498},{},[1499,1503,1506],{"type":22,"tag":33,"props":1500,"children":1501},{},[1502],{"type":27,"value":858},{"type":22,"tag":121,"props":1504,"children":1505},{},[],{"type":27,"value":1507},"\nRT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks.",{"type":22,"tag":23,"props":1509,"children":1510},{},[1511],{"type":22,"tag":33,"props":1512,"children":1513},{},[1514],{"type":27,"value":871},{"type":22,"tag":873,"props":1516,"children":1517},{},[1518,1523],{"type":22,"tag":877,"props":1519,"children":1520},{},[1521],{"type":27,"value":1522},"Over 130K episodes covering more than 700 tasks collected from 13 robots.",{"type":22,"tag":877,"props":1524,"children":1525},{},[1526],{"type":27,"value":1527},"Utilizes vision and natural language inputs to produce discretized action tokens.",{"type":22,"tag":23,"props":1529,"children":1530},{},[1531],{"type":22,"tag":33,"props":1532,"children":1533},{},[1534],{"type":27,"value":894},{"type":22,"tag":873,"props":1536,"children":1537},{},[1538,1543],{"type":22,"tag":877,"props":1539,"children":1540},{},[1541],{"type":27,"value":1542},"Demonstrates superior performance and generalization, including sim-to-real transfer.",{"type":22,"tag":877,"props":1544,"children":1545},{},[1546],{"type":27,"value":1547},"Scalability through high-capacity transformer models.",{"type":22,"tag":23,"props":1549,"children":1550},{},[1551],{"type":22,"tag":33,"props":1552,"children":1553},{},[1554],{"type":27,"value":915},{"type":22,"tag":873,"props":1556,"children":1557},{},[1558],{"type":22,"tag":877,"props":1559,"children":1560},{},[1561],{"type":27,"value":1562},"Demands extensive data, compute, and engineering expertise; system complexity is high.",{"type":22,"tag":835,"props":1564,"children":1565},{},[],{"type":22,"tag":845,"props":1567,"children":1569},{"id":1568},"_11-dobbe",[1570],{"type":27,"value":1571},"11. DobbE",{"type":22,"tag":23,"props":1573,"children":1574},{},[1575,1579,1582],{"type":22,"tag":33,"props":1576,"children":1577},{},[1578],{"type":27,"value":858},{"type":22,"tag":121,"props":1580,"children":1581},{},[],{"type":27,"value":1583},"\nDobbE focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time.",{"type":22,"tag":23,"props":1585,"children":1586},{},[1587],{"type":22,"tag":33,"props":1588,"children":1589},{},[1590],{"type":27,"value":871},{"type":22,"tag":873,"props":1592,"children":1593},{},[1594,1599],{"type":22,"tag":877,"props":1595,"children":1596},{},[1597],{"type":27,"value":1598},"HoNY dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30fps).",{"type":22,"tag":877,"props":1600,"children":1601},{},[1602],{"type":27,"value":1603},"Includes a low-cost hardware Stick for demonstration collection.",{"type":22,"tag":23,"props":1605,"children":1606},{},[1607],{"type":22,"tag":33,"props":1608,"children":1609},{},[1610],{"type":27,"value":894},{"type":22,"tag":873,"props":1612,"children":1613},{},[1614,1619],{"type":22,"tag":877,"props":1615,"children":1616},{},[1617],{"type":27,"value":1618},"Cost-effective and designed for rapid task learning in domestic environments.",{"type":22,"tag":877,"props":1620,"children":1621},{},[1622],{"type":27,"value":1623},"Demonstrates strong real-world applicability in home settings.",{"type":22,"tag":23,"props":1625,"children":1626},{},[1627],{"type":22,"tag":33,"props":1628,"children":1629},{},[1630],{"type":27,"value":915},{"type":22,"tag":873,"props":1632,"children":1633},{},[1634],{"type":22,"tag":877,"props":1635,"children":1636},{},[1637],{"type":27,"value":1638},"Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability.",{"type":22,"tag":835,"props":1640,"children":1641},{},[],{"type":22,"tag":845,"props":1643,"children":1645},{"id":1644},"_12-rh20t",[1646],{"type":27,"value":1647},"12. RH20T",{"type":22,"tag":23,"props":1649,"children":1650},{},[1651,1655,1658],{"type":22,"tag":33,"props":1652,"children":1653},{},[1654],{"type":27,"value":858},{"type":22,"tag":121,"props":1656,"children":1657},{},[],{"type":27,"value":1659},"\nRH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information.",{"type":22,"tag":23,"props":1661,"children":1662},{},[1663],{"type":22,"tag":33,"props":1664,"children":1665},{},[1666],{"type":27,"value":871},{"type":22,"tag":873,"props":1668,"children":1669},{},[1670,1675],{"type":22,"tag":877,"props":1671,"children":1672},{},[1673],{"type":27,"value":1674},"Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing.",{"type":22,"tag":877,"props":1676,"children":1677},{},[1678],{"type":27,"value":1679},"Detailed synchronization and calibration across multiple sensors.",{"type":22,"tag":23,"props":1681,"children":1682},{},[1683],{"type":22,"tag":33,"props":1684,"children":1685},{},[1686],{"type":27,"value":894},{"type":22,"tag":873,"props":1688,"children":1689},{},[1690,1695],{"type":22,"tag":877,"props":1691,"children":1692},{},[1693],{"type":27,"value":1694},"Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion.",{"type":22,"tag":877,"props":1696,"children":1697},{},[1698],{"type":27,"value":1699},"Supports research on contact-rich and dexterous manipulation.",{"type":22,"tag":23,"props":1701,"children":1702},{},[1703],{"type":22,"tag":33,"props":1704,"children":1705},{},[1706],{"type":27,"value":915},{"type":22,"tag":873,"props":1708,"children":1709},{},[1710],{"type":22,"tag":877,"props":1711,"children":1712},{},[1713],{"type":27,"value":1714},"Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing.",{"type":22,"tag":835,"props":1716,"children":1717},{},[],{"type":22,"tag":845,"props":1719,"children":1721},{"id":1720},"_13-bc-z",[1722],{"type":27,"value":1723},"13. BC-Z",{"type":22,"tag":23,"props":1725,"children":1726},{},[1727,1731,1734],{"type":22,"tag":33,"props":1728,"children":1729},{},[1730],{"type":27,"value":858},{"type":22,"tag":121,"props":1732,"children":1733},{},[],{"type":27,"value":1735},"\nBC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches.",{"type":22,"tag":23,"props":1737,"children":1738},{},[1739],{"type":22,"tag":33,"props":1740,"children":1741},{},[1742],{"type":27,"value":871},{"type":22,"tag":873,"props":1744,"children":1745},{},[1746],{"type":22,"tag":877,"props":1747,"children":1748},{},[1749],{"type":27,"value":1750},"Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets.",{"type":22,"tag":23,"props":1752,"children":1753},{},[1754],{"type":22,"tag":33,"props":1755,"children":1756},{},[1757],{"type":27,"value":894},{"type":22,"tag":873,"props":1759,"children":1760},{},[1761],{"type":22,"tag":877,"props":1762,"children":1763},{},[1764],{"type":27,"value":1765},"Serves as a standardized resource for evaluating behavior cloning algorithms.",{"type":22,"tag":23,"props":1767,"children":1768},{},[1769],{"type":22,"tag":33,"props":1770,"children":1771},{},[1772],{"type":27,"value":915},{"type":22,"tag":873,"props":1774,"children":1775},{},[1776],{"type":22,"tag":877,"props":1777,"children":1778},{},[1779],{"type":27,"value":1780},"May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets.",{"type":22,"tag":835,"props":1782,"children":1783},{},[],{"type":22,"tag":845,"props":1785,"children":1787},{"id":1786},"_14-mt-opt",[1788],{"type":27,"value":1789},"14. MT-Opt",{"type":22,"tag":23,"props":1791,"children":1792},{},[1793,1797,1800],{"type":22,"tag":33,"props":1794,"children":1795},{},[1796],{"type":27,"value":858},{"type":22,"tag":121,"props":1798,"children":1799},{},[],{"type":27,"value":1801},"\nMT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently.",{"type":22,"tag":23,"props":1803,"children":1804},{},[1805],{"type":22,"tag":33,"props":1806,"children":1807},{},[1808],{"type":27,"value":871},{"type":22,"tag":873,"props":1810,"children":1811},{},[1812],{"type":22,"tag":877,"props":1813,"children":1814},{},[1815],{"type":27,"value":1816},"Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method.",{"type":22,"tag":23,"props":1818,"children":1819},{},[1820],{"type":22,"tag":33,"props":1821,"children":1822},{},[1823],{"type":27,"value":894},{"type":22,"tag":873,"props":1825,"children":1826},{},[1827,1832],{"type":22,"tag":877,"props":1828,"children":1829},{},[1830],{"type":27,"value":1831},"Effective at sharing experience across tasks, significantly boosting performance on rare tasks.",{"type":22,"tag":877,"props":1833,"children":1834},{},[1835],{"type":27,"value":1836},"Demonstrates both zero-shot and rapid fine-tuning capabilities.",{"type":22,"tag":23,"props":1838,"children":1839},{},[1840],{"type":22,"tag":33,"props":1841,"children":1842},{},[1843],{"type":27,"value":915},{"type":22,"tag":873,"props":1845,"children":1846},{},[1847],{"type":22,"tag":877,"props":1848,"children":1849},{},[1850],{"type":27,"value":1851},"Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines.",{"type":22,"tag":835,"props":1853,"children":1854},{},[],{"type":22,"tag":845,"props":1856,"children":1858},{"id":1857},"_15-vima",[1859],{"type":27,"value":1860},"15. VIMA",{"type":22,"tag":23,"props":1862,"children":1863},{},[1864,1868,1871],{"type":22,"tag":33,"props":1865,"children":1866},{},[1867],{"type":27,"value":858},{"type":22,"tag":121,"props":1869,"children":1870},{},[],{"type":27,"value":1872},"\nVIMA presents a novel formulation in which diverse robot manipulation tasks are prompted via interleaved language and visual tokens, unifying task specification.",{"type":22,"tag":23,"props":1874,"children":1875},{},[1876],{"type":22,"tag":33,"props":1877,"children":1878},{},[1879],{"type":27,"value":871},{"type":22,"tag":873,"props":1881,"children":1882},{},[1883],{"type":22,"tag":877,"props":1884,"children":1885},{},[1886],{"type":27,"value":1887},"Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances.",{"type":22,"tag":23,"props":1889,"children":1890},{},[1891],{"type":22,"tag":33,"props":1892,"children":1893},{},[1894],{"type":27,"value":894},{"type":22,"tag":873,"props":1896,"children":1897},{},[1898,1903],{"type":22,"tag":877,"props":1899,"children":1900},{},[1901],{"type":27,"value":1902},"Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency.",{"type":22,"tag":877,"props":1904,"children":1905},{},[1906],{"type":27,"value":1907},"Allows integration of various forms of task instructions (text + image).",{"type":22,"tag":23,"props":1909,"children":1910},{},[1911],{"type":22,"tag":33,"props":1912,"children":1913},{},[1914],{"type":27,"value":915},{"type":22,"tag":873,"props":1916,"children":1917},{},[1918],{"type":22,"tag":877,"props":1919,"children":1920},{},[1921],{"type":27,"value":1922},"Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment.",{"type":22,"tag":835,"props":1924,"children":1925},{},[],{"type":22,"tag":845,"props":1927,"children":1929},{"id":1928},"_16-spoc",[1930],{"type":27,"value":1931},"16. SPOC",{"type":22,"tag":23,"props":1933,"children":1934},{},[1935,1939,1942],{"type":22,"tag":33,"props":1936,"children":1937},{},[1938],{"type":27,"value":858},{"type":22,"tag":121,"props":1940,"children":1941},{},[],{"type":27,"value":1943},"\nSPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation.",{"type":22,"tag":23,"props":1945,"children":1946},{},[1947],{"type":22,"tag":33,"props":1948,"children":1949},{},[1950],{"type":27,"value":871},{"type":22,"tag":873,"props":1952,"children":1953},{},[1954,1959],{"type":22,"tag":877,"props":1955,"children":1956},{},[1957],{"type":27,"value":1958},"Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames.",{"type":22,"tag":877,"props":1960,"children":1961},{},[1962],{"type":27,"value":1963},"Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion.",{"type":22,"tag":23,"props":1965,"children":1966},{},[1967],{"type":22,"tag":33,"props":1968,"children":1969},{},[1970],{"type":27,"value":894},{"type":22,"tag":873,"props":1972,"children":1973},{},[1974,1979],{"type":22,"tag":877,"props":1975,"children":1976},{},[1977],{"type":27,"value":1978},"Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities.",{"type":22,"tag":877,"props":1980,"children":1981},{},[1982],{"type":27,"value":1983},"Trains entirely in simulation and transfers effectively.",{"type":22,"tag":23,"props":1985,"children":1986},{},[1987],{"type":22,"tag":33,"props":1988,"children":1989},{},[1990],{"type":27,"value":915},{"type":22,"tag":873,"props":1992,"children":1993},{},[1994],{"type":22,"tag":877,"props":1995,"children":1996},{},[1997],{"type":27,"value":1998},"RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios.",{"title":13,"searchDepth":2000,"depth":2000,"links":2001},2,[2002,2003],{"id":43,"depth":2000,"text":46},{"id":840,"depth":2000,"text":843,"children":2004},[2005,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021],{"id":847,"depth":2006,"text":850},3,{"id":929,"depth":2006,"text":932},{"id":1005,"depth":2006,"text":1008},{"id":1081,"depth":2006,"text":1084},{"id":1157,"depth":2006,"text":1160},{"id":1223,"depth":2006,"text":1226},{"id":1289,"depth":2006,"text":1292},{"id":1355,"depth":2006,"text":1358},{"id":1421,"depth":2006,"text":1424},{"id":1492,"depth":2006,"text":1495},{"id":1568,"depth":2006,"text":1571},{"id":1644,"depth":2006,"text":1647},{"id":1720,"depth":2006,"text":1723},{"id":1786,"depth":2006,"text":1789},{"id":1857,"depth":2006,"text":1860},{"id":1928,"depth":2006,"text":1931},"markdown","content:6.robotics-overview:1.robotics-datasets.md","content","6.robotics-overview/1.robotics-datasets.md","6.robotics-overview/1.robotics-datasets","md","default",["ShallowRef",2030],["ShallowReactive",2031],{"/robotics-overview/robotics-datasets":2032},[2033,2043],{"_path":2034,"_dir":2035,"_draft":6,"_partial":6,"_locale":13,"title":2036,"description":2037,"icon":2038,"lastModified":2039,"_type":2022,"_id":2040,"_source":2024,"_file":2041,"_stem":2042,"_extension":2027},"/published-research/ai-next","published-research","AI's Euclid's Elements Moment","AI's Euclid's Elements Moment: From Language Models to Computable Thought","lucide:brain","Jul 7, 2025","content:5.published-research:6.ai-next.md","5.published-research/6.ai-next.md","5.published-research/6.ai-next",{"_path":2044,"_dir":12,"_draft":6,"_partial":6,"_locale":13,"title":2045,"description":2046,"icon":2047,"lastModified":17,"_type":2022,"_id":2048,"_source":2024,"_file":2049,"_stem":2050,"_extension":2027},"/robotics-overview/robotics-models","General-Purpose Robot Models Analysis","Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.","lucide:bot","content:6.robotics-overview:2.robotics-models.md","6.robotics-overview/2.robotics-models.md","6.robotics-overview/2.robotics-models",["ShallowRef",2052],{},[2054,2058,2065,2084,2091,2111,2120,2127,2138,2166],{"title":2055,"_path":2056,"icon":2057},"Cyber Nachos","/cybernachos","lucide:hand",{"title":2059,"_path":2060,"children":2061},"Published Products","/published",[2062],{"title":2063,"_path":2064,"icon":2038},"Cyber Nachos GPT","/published/cybernachos-gpt",{"title":2066,"_path":2067,"children":2068},"Tutorial Isaac Lab","/tutorial-isaaclab",[2069,2072,2076,2080],{"title":2070,"_path":2071,"icon":2047},"Introduction","/tutorial-isaaclab/introduction",{"title":2073,"_path":2074,"icon":2075},"Installation Guide","/tutorial-isaaclab/installation","lucide:download",{"title":2077,"_path":2078,"icon":2079},"Getting Started","/tutorial-isaaclab/getting-started","lucide:flag",{"title":2081,"_path":2082,"icon":2083},"Enabling fluid simulation","/tutorial-isaaclab/enable-fluid","material-symbols:water-drop-outline",{"title":2085,"_path":2086,"children":2087},"Tutorial LLMs","/tutorial-llms",[2088],{"title":2089,"_path":2090,"icon":2038},"Distilling GPT with PyTorch and Transformers","/tutorial-llms/distilling-gpt-with-pytorch-and-transformers",{"title":2092,"_path":2093,"children":2094},"Published Research Papers","/published-research",[2095,2098,2101,2104,2107,2110],{"title":2096,"_path":2097,"icon":2047},"You Only Render Once","/published-research/you-only-render-once",{"title":2099,"_path":2100,"icon":2047},"Real-time Dexterous Telemanipulation","/published-research/real-time-dexterous",{"title":2102,"_path":2103,"icon":2038},"Hallucination Prevention in LLMs","/published-research/hallucination",{"title":2105,"_path":2106,"icon":2038},"AI Anchor Framework","/published-research/ai-anchor",{"title":2108,"_path":2109,"icon":2038},"AI as the Cognitive Engine of Productivity","/published-research/ai-productivity",{"title":2036,"_path":2034,"icon":2038},{"title":2112,"_path":2113,"children":2114},"Robotics Overview","/robotics-overview",[2115,2116,2117],{"title":14,"_path":11,"icon":16},{"title":2045,"_path":2044,"icon":2047},{"title":2118,"_path":2119,"icon":2047},"Wheel-Based Humanoid Robots","/robotics-overview/wheel-based-robots",{"title":2121,"_path":2122,"children":2123},"Posts","/posts",[2124],{"title":2125,"_path":2126,"icon":2047},"Robotic Sociology: Human-Robot Interaction and the Emergence of Machine Social Structures","/posts/robotic-sociology",{"title":2128,"_path":2129,"children":2130},"Courses","/courses",[2131,2135],{"title":2132,"_path":2133,"icon":2134},"Mobile Computing Lab Course","/courses/mobile-lab","carbon:course",{"title":2136,"_path":2137,"icon":2134},"Mobile and IoT Security Lab Course","/courses/security-lab",{"title":2139,"_path":2140,"children":2141},"AI Robotics 100 Questions","/ai-robotics-100q",[2142,2145,2148,2151,2154,2157,2160,2163],{"title":2143,"_path":2144},"AI Robotics 100 Questions: Complete Learning Path","/ai-robotics-100q/outline",{"title":2146,"_path":2147},"Part 1: Fundamentals of AI Robotics (Questions 1-10)","/ai-robotics-100q/part1",{"title":2149,"_path":2150},"Part 2: Perception and Sensing (Questions 11-28)","/ai-robotics-100q/part2",{"title":2152,"_path":2153},"Part 3: Control and Manipulation (Questions 29-45)","/ai-robotics-100q/part3",{"title":2155,"_path":2156},"Part 4: Localization and Navigation (Questions 46-63)","/ai-robotics-100q/part4",{"title":2158,"_path":2159},"Part 5: Human-Robot Interaction (HRI) (Questions 64-78)","/ai-robotics-100q/part5",{"title":2161,"_path":2162},"Part 6: AI Decision Making and Autonomy (Questions 79-90)","/ai-robotics-100q/part6",{"title":2164,"_path":2165},"Part 7: Simulation and Sim2Real Transfer (Questions 91-101)","/ai-robotics-100q/part7",{"title":2167,"_path":2168},"Terms of Service & User Agreement","/terms",["Map"],{"_priority":2171,"env":2175,"name":2176,"url":2177},{"name":2172,"env":2173,"url":2174},-10,-15,-3,"production","shadcn-docs-nuxt-starter","https://cybernachos.github.io",["Set"],["ShallowReactive",2180],{"XFVaCZk9MC":2181},null,1752544592155]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{mdc:{components:{prose:true,map:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},content:{locales:[],defaultLocale:"",integrity:1752544546451,experimental:{stripQueryParameters:false,advanceQuery:false,clientDB:false},respectPathCase:false,api:{baseURL:"/api/_content"},navigation:{fields:["icon","navBadges","navTruncate","badges","toc","sidebar","collapse","editLink","prevNext","breadcrumb","fullpage","layout"]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:{theme:{default:"github-light",dark:"github-dark"},preload:["json","js","ts","html","css","vue","diff","shell","markdown","mdc","yaml","bash","ini","dotenv"]},wsUrl:"",documentDriven:{page:true,navigation:true,surround:true,globals:{},layoutFallbacks:["theme"],injectPage:true},host:"",trailingSlash:false,search:{indexed:true,ignoredTags:["script","style","pre"],filterQuery:{_draft:false,_partial:false},options:{fields:["title","content","titles"],storeFields:["title","content","titles"],searchOptions:{prefix:true,fuzzy:.2,boost:{title:4,content:2,titles:1}}}},contentHead:true,anchorLinks:{depth:4,exclude:[1]}},"nuxt-scripts":{version:"",defaultScriptOptions:{trigger:"onNuxtReady"}}},app:{baseURL:"/",buildId:"1defcafe-82b4-4ebd-9604-7ed2beb079ac",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>