{"_path":"/model-dataset-comp/robotics-models","_dir":"model-dataset-comp","_draft":false,"_partial":false,"_locale":"","title":"General-Purpose Robot Models Analysis","description":"Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.","icon":"lucide:bot","body":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Bellow provides an overview of several recent works on general-purpose robot models. It compares key technical aspects—such as model architecture, parameter scale, training data volume, and major innovations—and outlines the typical hardware and time requirements for training, fine-tuning, or distillation."}]},{"type":"element","tag":"h2","props":{"id":"comparison-of-robot-models"},"children":[{"type":"text","value":"Comparison of Robot Models"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Below is a table summarizing 10 projects, including their model architecture/method, parameter scale, training data volume, and key features/remarks. (Note: For some projects, specific numbers such as parameter counts or data volumes are not disclosed; descriptive indicators are provided instead.)"}]},{"type":"element","tag":"table","props":{},"children":[{"type":"element","tag":"thead","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Project Name"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Model Architecture / Method"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Parameter Scale"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Training Data / Data Volume"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Key Features / Remarks"}]}]}]}]},{"type":"element","tag":"tbody","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"1. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Octo"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://octo-models.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Transformer-based diffusion policy supporting language, target images, and sensor history conditioning"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Octo-Small: 27M"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"Octo-Base: 93M"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Pre-trained on 800k robot demonstrations, integrating 25 datasets (Open X-Embodiment)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Flexibly adaptable to different robots and sensors; efficient fine-tuning; excellent performance in zero-shot and few-shot scenarios"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"2. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"OpenVLA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://openvla.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Fuses a visual encoder (SigLIP + DinoV2) with a Llama 2 7B language model to generate action tokens"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"7B"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Pre-trained on 970k robot demonstrations (Open X-Embodiment)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Leverages internet pre-trained vision-language knowledge; supports multi-robot control; resource-intensive training (64 A100 GPUs, 15 days)"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"3. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"UMI"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://umi-gripper.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Data collection and policy learning framework based on a handheld gripper and wrist-mounted camera for in-the-wild demonstrations"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Not disclosed"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Rapid in-the-wild demonstration capture (approx. 30 seconds per demo), with high data diversity"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Low-cost, portable hardware design; enables zero-calibration and bimanual dynamic manipulation; focuses on demonstration data collection"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"4. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RDT-1B"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://rdt-robotics.github.io/rdt-robotics/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Transformer policy based on diffusion models, specifically designed for bimanual manipulation"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"1.2B"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Pre-trained on 46 datasets with over 1M demonstrations; additional 6K+ bimanual demonstrations"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Large-scale pre-training, multi-task cross-robot capability; excellent zero-shot generalization and few-shot learning ability"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"5. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"openpi"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://github.com/Physical-Intelligence/openpi","rel":["nofollow"]},"children":[{"type":"text","value":"GitHub"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Consists of π₀ (streaming diffusion VLA) and π₀-FAST (autoregressive VLA) for vision-language-action tasks"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Not explicitly disclosed"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Pre-trained on 10k+ hours of robot data"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Provides multiple base model checkpoints; easy fine-tuning for downstream tasks; adaptable to various robot platforms"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"6. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Mobile ALOHA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://mobile-aloha.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Imitation learning (behavior cloning) based mobile manipulation system combining whole-body control and low-cost remote operation"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Not disclosed"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Approximately 50 demonstrations per task, jointly trained with a static ALOHA dataset"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Extends traditional ALOHA to mobile platforms; enables complex mobile manipulation tasks (e.g., opening doors, using elevators)"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"7. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RT-2"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://robotics-transformer2.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Vision-language-action model that encodes robot actions as text tokens, combining internet pre-training with robot data"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Based on PaLM-E: 12B"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"or PaLI-X: 55B"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Mixed large-scale internet vision-language data and robot trajectory data (exact numbers undisclosed)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Utilizes a pre-trained large model’s semantic understanding and reasoning; enables multi-step task planning and coherent execution; strong generalization"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"8. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"VIMA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://vimalabs.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Transformer-based robotic agent that generates actions through multimodal prompts (language, image/video)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"2M - 200M (depending on variant)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Over 600K expert demonstrations; supplemented with large amounts of programmatically generated task data"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Data-efficient; unified representation for various tasks; exhibits good zero-shot generalization and cross-task adaptability"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"9. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Perceiver-Actor"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://peract.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Behavior cloning strategy based on a Perceiver Transformer, using RGB-D voxelized input and discretized action prediction"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Not disclosed (relatively lightweight)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Demonstration counts are relatively low (e.g., for RLBench with 249 variants and 7 real-world tasks, approx. 53 demos)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Data-efficient learning for 6-DoF manipulation; suitable for few-shot multi-task scenarios; high-performance action detection"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"10. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"SayCan"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://say-can.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Integrates a large language model with pre-trained skill/value functions; uses language scoring combined with execution probabilities for task planning"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Based on LLM (e.g., PaLM) with parameters up to tens of billions; skill modules are smaller"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Utilizes large-scale internet text and robot skill demonstration data (exact figures undisclosed)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Achieves long-horizon task planning and semantic reasoning; composes multi-step skills; supports multilingual capability; improves execution success rate"}]}]}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h2","props":{"id":"hardware-and-time-requirements-for-training-fine-tuning-or-distillation"},"children":[{"type":"text","value":"Hardware and Time Requirements for Training, Fine-Tuning, or Distillation"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"The following table outlines typical hardware devices and approximate training times for various stages, such as pre-training (full training), fine-tuning (or parameter-efficient fine-tuning), and distillation. Actual requirements vary depending on model scale, data volume, training strategy (e.g., full vs. parameter-efficient fine-tuning), and task specifics."}]},{"type":"element","tag":"table","props":{},"children":[{"type":"element","tag":"thead","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Project Name"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Pre-training / Full Training (Hardware & Time)"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Fine-Tuning / Parameter-Efficient Fine-Tuning / Distillation (Hardware & Time)"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Remarks"}]}]}]}]},{"type":"element","tag":"tbody","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"1. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Octo"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://octo-models.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Pre-training on 800k demos typically requires multiple high-performance GPUs (e.g., A100/RTX4090)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Training time: several days to weeks"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning using efficient strategies can often be completed on a single GPU in a few hours to one day"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Adaptable to different robots and sensors; fine-tuning time is relatively short"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"2. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"OpenVLA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://openvla.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Pre-training used 64 A100 GPUs, with a training duration of about 15 days"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Task-specific fine-tuning using parameter-efficient methods usually takes a few hours to one day on a single GPU"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Leverages large-scale internet pre-training; resource-intensive"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"3. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"UMI"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://umi-gripper.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Focused on data collection and policy learning; training can be done on low-cost GPUs (or even a single card)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Training time: on the order of a few hours"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning for specific tasks (using fast demonstration capture) typically completes within hours"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Uses portable hardware design; suitable for in-the-wild demonstration data"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"4. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RDT-1B"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://rdt-robotics.github.io/rdt-robotics/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Pre-training a 1.2B parameter model generally requires a multi-GPU cluster (e.g., 8-16 A100 GPUs)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Training time: possibly over a week"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning on specific bimanual tasks (using additional 6K+ demos) may take from a few hours to one day"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Large parameter scale and rich data; high resource and time demand for pre-training"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"5. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"openpi"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://github.com/Physical-Intelligence/openpi","rel":["nofollow"]},"children":[{"type":"text","value":"GitHub"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Full training on 10k+ hours of robot data may require high-memory GPUs (e.g., A100/H100)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Parameter-efficient fine-tuning (e.g., using LoRA) typically requires at least 22.5GB of GPU memory (e.g., RTX4090) with training times from a few hours to a few days"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Offers both full training and efficient fine-tuning options; hardware requirements are clearly defined"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"6. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Mobile ALOHA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://mobile-aloha.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Imitation learning methods typically train on a single high-end GPU (e.g., RTX 3090/4090)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Training time: several hours to one day"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning using combined static ALOHA data generally completes on a single GPU in a short period"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Focuses on mobile and whole-body control; relatively small data volume"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"7. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RT-2"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://robotics-transformer2.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Large models (12B-55B parameters) require extensive GPU clusters (e.g., 64 A100 GPUs)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Pre-training time: typically several weeks"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning for specific tasks using joint training strategies may take from a few hours to one day, depending on data volume"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Combines internet-scale pre-training with robot data; high hardware and time requirements"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"8. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"VIMA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://vimalabs.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Model sizes range from a few million to several hundred million parameters"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Smaller variants can be trained on a single GPU in hours; larger variants may need multiple GPUs for days to a week"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning is typically done on a single GPU or a small multi-GPU setup; high data efficiency can greatly reduce training time"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Model and data scale are adjustable, making fine-tuning flexible"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"9. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Perceiver-Actor"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://peract.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Due to voxelized inputs and discrete action prediction, training can often be done on a single GPU (8-16GB memory)"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Training time: typically several hours to one day"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning for few-shot scenarios is highly efficient, often completing within a few hours"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Emphasizes data-efficient learning for 6-DoF manipulation; suitable for low-resource environments"}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"10. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"SayCan"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://say-can.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Integrates a large language model (e.g., PaLM series) with robot skills; pre-training typically uses TPUs or large-scale GPU clusters"},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"- Pre-training time: may span several weeks"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"- Fine-tuning or distillation for specific scenarios is typically carried out on multi-GPU or TPU setups, taking from a few hours to one day"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Combines semantic reasoning with low-level skills; high resource requirements for pre-training, but fine-tuning can leverage LLM improvements"}]}]}]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"comparison-of-robot-models","depth":2,"text":"Comparison of Robot Models"},{"id":"hardware-and-time-requirements-for-training-fine-tuning-or-distillation","depth":2,"text":"Hardware and Time Requirements for Training, Fine-Tuning, or Distillation"}]}},"_type":"markdown","_id":"content:5.model-dataset-comp:2.robotics-models.md","_source":"content","_file":"5.model-dataset-comp/2.robotics-models.md","_stem":"5.model-dataset-comp/2.robotics-models","_extension":"md"}