{"_path":"/robotics-overview/robotics-datasets","_dir":"robotics-overview","_draft":false,"_partial":false,"_locale":"","title":"Robotics Dataset Comparison","description":"A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, DobbÂ·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.","icon":"lucide:database","lastModified":"Mar 28, 2025","body":{"type":"root","children":[{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table that highlights key characteristics, and second, detailed descriptions of each dataset's scope, technical features, advantages, and disadvantages."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Note:"}]},{"type":"text","value":" This analysis is accurate as of the last modified date, \"Mar 28, 2025.\""}]},{"type":"element","tag":"h2","props":{"id":"summary-table"},"children":[{"type":"text","value":"Summary Table"}]},{"type":"element","tag":"table","props":{},"children":[{"type":"element","tag":"thead","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Dataset / Framework"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scale & Modalities"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Key Advantages"}]}]},{"type":"element","tag":"th","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Key Disadvantages"}]}]}]}]},{"type":"element","tag":"tbody","props":{},"children":[{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"1. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"LeRobot"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://github.com/huggingface/lerobot","rel":["nofollow"]},"children":[{"type":"text","value":"GitHub"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"End-to-end learning with community support; integrated simulation environments."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Complex setup; may require substantial computing and sensor calibration."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"2. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Open X-Embodiment"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://robotics-transformer-x.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Massive diversity enabling cross-robot transfer and positive knowledge sharing."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Heterogeneous quality and potential standardization issues across varied sources."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"3. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"DROID"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://droid-dataset.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"In-the-wild robot manipulation for robust imitation learning."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Diverse, large-scale manipulation data that improves policy robustness."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Mostly limited to manipulation with a specific hardware setup; less diversity in task types."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"4. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RoboTurk"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://roboturk.stanford.edu/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Leverages non-expert, scalable human demonstrations; supports collaborative tasks."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Variation in demonstration quality and potential limits in scale compared to fully automated data collection."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"5. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"MIME"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://sites.google.com/view/mimedataset/dataset?authuser=0","rel":["nofollow"]},"children":[{"type":"text","value":"Google Sites"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Imitation learning for robot manipulation using human demonstrations."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Focus on high-quality manipulation trajectories; well-suited for imitation learning."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"May be smaller in scale and less diverse than some large-scale multi-robot datasets."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"6. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Meta-World"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://meta-world.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Benchmark for multi-task and meta-reinforcement learning in simulation."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"50 distinct simulated manipulation environments; task variations with visual observations."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Standardized benchmark for meta-RL; structured for evaluating generalization."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Limited to simulation and may not capture the full variability of real-world settings."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"7. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RoboNet"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://www.robonet.wiki/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Open database of real robotic experience for manipulation tasks across multiple platforms."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"~15M video frames, collected from 7 robot platforms with diverse camera viewpoints."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Large-scale, multi-platform real-world data that facilitates cross-robot generalization."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Very high storage and processing requirements; complex data integration."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"8. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RoboSet"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://robopen.github.io/roboset/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Multi-task dataset for household (kitchen) manipulation tasks, including language instructions."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Rich, multi-modal data in realistic home environments; supports language-guided sequencing."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Domain-specific (largely kitchens); may not generalize to non-domestic scenarios."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"9. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"BridgeData V2"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://rail-berkeley.github.io/bridgedata/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Large-scale robotic manipulation across diverse environments and skills with language annotations."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"10. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RT-1"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://robotics-transformer1.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Real-world imitation learning for multi-task manipulation using transformer architectures."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Outstanding generalization and performance on diverse tasks; scalable transformer model."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"High training and computational requirements; system complexity may be a barrier."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"11. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"DobbÂ·E"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://dobb-e.com/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Framework for home robotics: learning household manipulation tasks quickly in real homes."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"âHoNYâ dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30Â fps; also includes hardware (the âStickâ) for data collection."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Cost-effective, rapid task learning with real household data; designed for generalist home robots."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"12. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"RH20T"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://rh20t.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Very large and complex; requires significant computational and storage resources; complex data processing pipeline."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"13. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"BC-Z"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://sites.google.com/view/bc-z/home?pli=1","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Large-scale behavior cloning for robotic manipulation."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"14. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"MT-Opt"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://karolhausman.github.io/mt-opt/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Multi-task reinforcement learning at scale across many manipulation skills."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"15. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"VIMA"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://vimalabs.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"General robot manipulation via multimodal prompts (combining language and vision) for unified task specification."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Unified formulation that âpromptsâ the robot to perform diverse tasks; highly scalable and sample-efficient."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation."}]}]},{"type":"element","tag":"tr","props":{},"children":[{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"16. "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"SPOC"}]},{"type":"text","value":" "},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":" ("},{"type":"element","tag":"a","props":{"href":"https://spoc-robot.github.io/","rel":["nofollow"]},"children":[{"type":"text","value":"Website"}]},{"type":"text","value":")"}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world)."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info."}]},{"type":"element","tag":"td","props":{},"children":[{"type":"text","value":"RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios."}]}]}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h2","props":{"id":"detailed-comparison"},"children":[{"type":"text","value":"Detailed Comparison"}]},{"type":"element","tag":"h3","props":{"id":"_1-lerobot"},"children":[{"type":"text","value":"1. LeRobot"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nLeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Built in PyTorch with modular dataset classes that support multi-frame temporal sampling."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Community-driven with active contributions and hosted on Hugging Face."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Facilitates rapid prototyping in robotics with an accessible codebase."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_2-open-x-embodiment"},"children":[{"type":"text","value":"2. Open X-Embodiment"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nA collaborative effort pooling robot data from 21 institutions, it is aimed at training âgeneralistâ policies across 22 different robot embodiments."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Aggregates 1M+ trajectories from diverse robots and tasks."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Supports learning via transformer-based architectures that can generalize across different embodiments."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Unmatched diversity, which is ideal for studying cross-robot transfer."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Large scale increases the potential for generalization."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_3-droid"},"children":[{"type":"text","value":"3. DROID"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nFocused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Multi-camera views (including wrist and exterior images) enable rich visual inputs."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Large, diverse dataset that significantly boosts policy performance and robustness."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Extensive coverage of real-world scenarios."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Being collected with a specific hardware platform, its applicability to other robots may be limited."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_4-roboturk"},"children":[{"type":"text","value":"4. RoboTurk"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nRoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset)."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Includes system features for low-latency teleoperation and human-in-the-loop interventions."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Proven effectiveness in enabling imitation learning on challenging tasks."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"The quality of demonstrations may vary due to differences in human teleoperation skills."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_5-mime"},"children":[{"type":"text","value":"5. MIME"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nMIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Focused on detailed manipulation tasks, making it ideal for imitation learning studies."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Generally smaller in scale compared to some of the largest datasets; might offer limited diversity."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_6-meta-world"},"children":[{"type":"text","value":"6. Meta-World"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nA simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Structured environments with varying goal positions and task variations to test generalization."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_7-robonet"},"children":[{"type":"text","value":"7. RoboNet"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nRoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Contains over 15M video frames and data from multiple camera viewpoints."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Offers vast amounts of real-world data to study generalization across different robot hardware."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Requires heavy storage and processing; integrating multi-platform data can be challenging."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_8-roboset"},"children":[{"type":"text","value":"8. RoboSet"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nA dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Rich multi-modal information (visual + language) supports language-guided robotic learning."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_9-bridgedata-v2"},"children":[{"type":"text","value":"9. BridgeData V2"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nDesigned to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Approximately 60K trajectories with multi-view RGB (and some depth) data."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Includes both teleoperated and scripted demonstrations."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"High diversity in environments and tasks; strong support for language-conditioned policy learning."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_10-rt-1"},"children":[{"type":"text","value":"10. RT-1"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nRT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Over 130K episodes covering more than 700 tasks collected from 13 robots."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Utilizes vision and natural language inputs to produce discretized action tokens."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Demonstrates superior performance and generalization, including sim-to-real transfer."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Scalability through high-capacity transformer models."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Demands extensive data, compute, and engineering expertise; system complexity is high."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_11-dobbe"},"children":[{"type":"text","value":"11. DobbÂ·E"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nDobbÂ·E focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"âHoNYâ dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30Â fps)."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Includes a low-cost hardware âStickâ for demonstration collection."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Cost-effective and designed for rapid task learning in domestic environments."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Demonstrates strong real-world applicability in home settings."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_12-rh20t"},"children":[{"type":"text","value":"12. RH20T"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nRH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Detailed synchronization and calibration across multiple sensors."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Supports research on contact-rich and dexterous manipulation."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_13-bc-z"},"children":[{"type":"text","value":"13. BC-Z"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nBC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Serves as a standardized resource for evaluating behavior cloning algorithms."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_14-mt-opt"},"children":[{"type":"text","value":"14. MT-Opt"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nMT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Effective at sharing experience across tasks, significantly boosting performance on rare tasks."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Demonstrates both zero-shot and rapid fine-tuning capabilities."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_15-vima"},"children":[{"type":"text","value":"15. VIMA"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nVIMA presents a novel formulation in which diverse robot manipulation tasks are âpromptedâ via interleaved language and visual tokens, unifying task specification."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Allows integration of various forms of task instructions (text + image)."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment."}]}]},{"type":"element","tag":"hr","props":{},"children":[]},{"type":"element","tag":"h3","props":{"id":"_16-spoc"},"children":[{"type":"text","value":"16. SPOC"}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Scope & Application:"}]},{"type":"element","tag":"br","props":{},"children":[]},{"type":"text","value":"\nSPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Technical Features:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Advantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"Trains entirely in simulation and transfers effectively."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Disadvantages:"}]}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"text","value":"RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios."}]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"summary-table","depth":2,"text":"Summary Table"},{"id":"detailed-comparison","depth":2,"text":"Detailed Comparison","children":[{"id":"_1-lerobot","depth":3,"text":"1. LeRobot"},{"id":"_2-open-x-embodiment","depth":3,"text":"2. Open X-Embodiment"},{"id":"_3-droid","depth":3,"text":"3. DROID"},{"id":"_4-roboturk","depth":3,"text":"4. RoboTurk"},{"id":"_5-mime","depth":3,"text":"5. MIME"},{"id":"_6-meta-world","depth":3,"text":"6. Meta-World"},{"id":"_7-robonet","depth":3,"text":"7. RoboNet"},{"id":"_8-roboset","depth":3,"text":"8. RoboSet"},{"id":"_9-bridgedata-v2","depth":3,"text":"9. BridgeData V2"},{"id":"_10-rt-1","depth":3,"text":"10. RT-1"},{"id":"_11-dobbe","depth":3,"text":"11. DobbÂ·E"},{"id":"_12-rh20t","depth":3,"text":"12. RH20T"},{"id":"_13-bc-z","depth":3,"text":"13. BC-Z"},{"id":"_14-mt-opt","depth":3,"text":"14. MT-Opt"},{"id":"_15-vima","depth":3,"text":"15. VIMA"},{"id":"_16-spoc","depth":3,"text":"16. SPOC"}]}]}},"_type":"markdown","_id":"content:6.robotics-overview:1.robotics-datasets.md","_source":"content","_file":"6.robotics-overview/1.robotics-datasets.md","_stem":"6.robotics-overview/1.robotics-datasets","_extension":"md","sitemap":{"loc":"/robotics-overview/robotics-datasets"}}