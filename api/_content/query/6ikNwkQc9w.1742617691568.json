{"_path":"/published-research/hallucination","_dir":"published-research","_draft":false,"_partial":false,"_locale":"","title":"Hallucination Prevention in LLMs","description":"Zero-Resource Hallucination Prevention for Large Language Models","icon":"lucide:brain","body":{"type":"root","children":[{"type":"element","tag":"h2","props":{"id":"zero-resource-hallucination-prevention-for-large-language-models"},"children":[{"type":"text","value":"Zero-Resource Hallucination Prevention for Large Language Models"}]},{"type":"element","tag":"img","props":{"alt":"CyberNachos","className":["h-full","w-full","object-cover","object-[50%_53%]"],"src":"/hallucination.png"},"children":[]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"The paper "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"\"Zero-Resource Hallucination Prevention for Large Language Models\""}]},{"type":"text","value":", published in the "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Findings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024 Findings)"}]},{"type":"text","value":",\nheld from November 12–16 in Miami, Florida, introduces a novel approach to mitigating hallucinations—instances where large language models (LLMs)\nproduce inaccurate or ungrounded information. EMNLP, organized by the Association for Computational Linguistics (ACL), is one of the premier conferences in\nthe field of natural language processing (NLP). It provides a leading platform for presenting groundbreaking research in NLP and computational linguistics,\nattracting researchers, practitioners, and industry leaders worldwide. The "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Findings of EMNLP"}]},{"type":"text","value":" serves as an associated venue for high-quality papers, ensuring a broader platform for innovative contributions."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"This paper proposes "},{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"SELF-FAMILIARITY"}]},{"type":"text","value":", a zero-resource pre-detection mechanism that evaluates the model's\nfamiliarity with the concepts in a given instruction and refrains from generating responses if the concepts are unfamiliar."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Key Contributions:"}]}]},{"type":"element","tag":"ol","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Self-Familiarity Mechanism:"}]},{"type":"text","value":" This approach mimics human self-assessment, analyzing concept familiarity to prevent hallucinations proactively rather than correcting them post hoc."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Three-Step Framework:"}]},{"type":"element","tag":"ul","props":{},"children":[{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Concept Extraction:"}]},{"type":"text","value":" Identifies key entities within an instruction."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Concept Guessing:"}]},{"type":"text","value":" Assesses the familiarity of extracted concepts using prompt engineering."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Aggregation:"}]},{"type":"text","value":" Combines familiarity scores of all concepts to determine the overall instruction familiarity."}]}]}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Robustness and Versatility:"}]},{"type":"text","value":" Unlike previous methods, SELF-FAMILIARITY achieves consistent performance across different LLMs and instruction styles without requiring external knowledge or resources."}]},{"type":"element","tag":"li","props":{},"children":[{"type":"element","tag":"strong","props":{},"children":[{"type":"text","value":"Empirical Validation:"}]},{"type":"text","value":" Evaluated across four LLMs using the proposed Concept-7 dataset, SELF-FAMILIARITY outperforms existing methods in detecting hallucinatory instructions, showcasing higher accuracy, consistency, and interpretability."}]}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"This work marks a shift toward proactive and preventative strategies for hallucination mitigation in LLMs, enhancing their reliability and usability in real-world applications."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"The paper and related materials, including the implementation code, are available on "},{"type":"element","tag":"a","props":{"href":"https://github.com/soap117/Self-evaluation/tree/main","rel":["nofollow"]},"children":[{"type":"text","value":"GitHub"}]},{"type":"text","value":" and "},{"type":"element","tag":"a","props":{"href":"https://www.accessdata.fda.gov/drugsatfda_docs/label/2021/761177lbl.pdf","rel":["nofollow"]},"children":[{"type":"text","value":"AccessData"}]},{"type":"text","value":"."}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[{"id":"zero-resource-hallucination-prevention-for-large-language-models","depth":2,"text":"Zero-Resource Hallucination Prevention for Large Language Models"}]}},"_type":"markdown","_id":"content:4.published-research:2.hallucination.md","_source":"content","_file":"4.published-research/2.hallucination.md","_stem":"4.published-research/2.hallucination","_extension":"md"}