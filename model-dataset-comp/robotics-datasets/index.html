<!DOCTYPE html><html><head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Robotics Dataset Comparison - Cyber Nachos</title>
<style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }/*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{border:0 solid #e5e7eb;box-sizing:border-box}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-tap-highlight-color:transparent}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-size:1em;font-variation-settings:normal}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{color:inherit;font-family:inherit;font-feature-settings:inherit;font-size:100%;font-variation-settings:inherit;font-weight:inherit;letter-spacing:inherit;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]:where(:not([hidden=until-found])){display:none}:root{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--primary:221.2 83.2% 53.3%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--ring:221.2 83.2% 53.3%;--radius:.5rem}.dark{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--primary:217.2 91.2% 59.8%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--ring:224.3 76.3% 48%}*{border-color:hsl(var(--border))}body{background-color:hsl(var(--background));color:hsl(var(--foreground))}.container{margin-left:auto;margin-right:auto;padding-left:2rem;padding-right:2rem;width:100%}@media (min-width:1400px){.container{max-width:1400px}}.sr-only{height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border-width:0;white-space:nowrap}.pointer-events-none{pointer-events:none}.pointer-events-auto{pointer-events:auto}.visible{visibility:visible}.\!collapse{visibility:collapse!important}.collapse{visibility:collapse}.static{position:static}.fixed{position:fixed}.absolute{position:absolute}.relative{position:relative}.sticky{position:sticky}.inset-0{top:0;right:0;bottom:0;left:0}.inset-x-0{left:0;right:0}.inset-y-0{bottom:0;top:0}.bottom-0{bottom:0}.left-0{left:0}.left-1\/2{left:50%}.right-0{right:0}.right-1{right:.25rem}.right-2{right:.5rem}.right-3{right:.75rem}.right-4{right:1rem}.top-0{top:0}.top-1{top:.25rem}.top-1\/2{top:50%}.top-2{top:.5rem}.top-3{top:.75rem}.top-4{top:1rem}.top-\[102px\]{top:102px}.top-\[60\%\]{top:60%}.top-\[90px\]{top:90px}.top-full{top:100%}.top-px{top:1px}.z-10{z-index:10}.z-30{z-index:30}.z-40{z-index:40}.z-50{z-index:50}.z-\[100\]{z-index:100}.z-\[1\]{z-index:1}.order-first{order:-9999}.order-last{order:9999}.col-span-2{grid-column:span 2/span 2}.m-0{margin:0}.-mx-1{margin-left:-.25rem;margin-right:-.25rem}.-mx-4{margin-left:-1rem;margin-right:-1rem}.mx-0\.5{margin-left:.125rem;margin-right:.125rem}.mx-2{margin-left:.5rem;margin-right:.5rem}.mx-3\.5{margin-left:.875rem;margin-right:.875rem}.mx-4{margin-left:1rem;margin-right:1rem}.mx-auto{margin-left:auto;margin-right:auto}.my-8{margin-bottom:2rem;margin-top:2rem}.-ml-2{margin-left:-.5rem}.mb-0{margin-bottom:0}.mb-1{margin-bottom:.25rem}.mb-2{margin-bottom:.5rem}.mb-3{margin-bottom:.75rem}.mb-4{margin-bottom:1rem}.mb-5{margin-bottom:1.25rem}.mb-6{margin-bottom:1.5rem}.mb-\[2px\]{margin-bottom:2px}.ml-1{margin-left:.25rem}.ml-2{margin-left:.5rem}.ml-3{margin-left:.75rem}.ml-4{margin-left:1rem}.ml-6{margin-left:1.5rem}.ml-auto{margin-left:auto}.mr-1{margin-right:.25rem}.mr-1\.5{margin-right:.375rem}.mr-2{margin-right:.5rem}.mr-3{margin-right:.75rem}.mr-4{margin-right:1rem}.mr-auto{margin-right:auto}.mt-0{margin-top:0}.mt-1{margin-top:.25rem}.mt-1\.5{margin-top:.375rem}.mt-16{margin-top:4rem}.mt-2{margin-top:.5rem}.mt-4{margin-top:1rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.inline-flex{display:inline-flex}.table{display:table}.grid{display:grid}.contents{display:contents}.hidden{display:none}.size-10{height:2.5rem;width:2.5rem}.size-16{height:4rem;width:4rem}.size-2{height:.5rem;width:.5rem}.size-3{height:.75rem;width:.75rem}.size-3\.5{height:.875rem;width:.875rem}.size-32{height:8rem;width:8rem}.size-4{height:1rem;width:1rem}.size-5{height:1.25rem;width:1.25rem}.size-6{height:1.5rem;width:1.5rem}.size-8{height:2rem;width:2rem}.size-full{height:100%;width:100%}.h-1\.5{height:.375rem}.h-10{height:2.5rem}.h-11{height:2.75rem}.h-12{height:3rem}.h-14{height:3.5rem}.h-2\.5{height:.625rem}.h-4{height:1rem}.h-48{height:12rem}.h-5{height:1.25rem}.h-6{height:1.5rem}.h-7{height:1.75rem}.h-8{height:2rem}.h-9{height:2.25rem}.h-\[--radix-navigation-menu-viewport-height\]{height:var(--radix-navigation-menu-viewport-height)}.h-\[calc\(100vh-3\.5rem\)\]{height:calc(100vh - 3.5rem)}.h-\[calc\(100vh-6\.5rem\)\]{height:calc(100vh - 6.5rem)}.h-full{height:100%}.h-px{height:1px}.h-svh{height:100svh}.max-h-screen{max-height:100vh}.min-h-4{min-height:1rem}.min-h-5{min-height:1.25rem}.min-h-6{min-height:1.5rem}.min-h-screen{min-height:100vh}.w-2\.5{width:.625rem}.w-3\/4{width:75%}.w-4{width:1rem}.w-5{width:1.25rem}.w-72{width:18rem}.w-9{width:2.25rem}.w-\[200px\]{width:200px}.w-\[23rem\]{width:23rem}.w-\[250px\]{width:250px}.w-auto{width:auto}.w-fit{width:-moz-fit-content;width:fit-content}.w-full{width:100%}.w-max{width:-moz-max-content;width:max-content}.w-px{width:1px}.min-w-0{min-width:0}.min-w-4{min-width:1rem}.min-w-5{min-width:1.25rem}.min-w-6{min-width:1.5rem}.max-w-2xl{max-width:42rem}.max-w-\[750px\]{max-width:750px}.max-w-\[980px\]{max-width:980px}.max-w-lg{max-width:32rem}.max-w-max{max-width:-moz-max-content;max-width:max-content}.max-w-screen-2xl{max-width:1536px}.flex-1{flex:1 1 0%}.shrink-0{flex-shrink:0}.flex-grow{flex-grow:1}.basis-1\/3{flex-basis:33.333333%}.-translate-x-1\/2{--tw-translate-x:-50%}.-translate-x-1\/2,.-translate-y-1\/2{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.-translate-y-1\/2{--tw-translate-y:-50%}.-rotate-90{--tw-rotate:-90deg}.-rotate-90,.rotate-0{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.rotate-0{--tw-rotate:0deg}.rotate-45{--tw-rotate:45deg}.rotate-45,.rotate-90{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.rotate-90{--tw-rotate:90deg}.scale-0{--tw-scale-x:0;--tw-scale-y:0}.scale-0,.scale-100{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.scale-100{--tw-scale-x:1;--tw-scale-y:1}.transform{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@keyframes spin{to{transform:rotate(1turn)}}.animate-spin{animation:spin 1s linear infinite}.cursor-default{cursor:default}.cursor-pointer{cursor:pointer}.touch-none{touch-action:none}.select-none{-webkit-user-select:none;-moz-user-select:none;user-select:none}.scroll-m-20{scroll-margin:5rem}.list-decimal{list-style-type:decimal}.list-disc{list-style-type:disc}.list-none{list-style-type:none}.grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.grid-cols-5{grid-template-columns:repeat(5,minmax(0,1fr))}.grid-cols-\[repeat\(auto-fit\,_minmax\(270px\,_1fr\)\)\]{grid-template-columns:repeat(auto-fit,minmax(270px,1fr))}.flex-row{flex-direction:row}.flex-col{flex-direction:column}.flex-col-reverse{flex-direction:column-reverse}.flex-wrap{flex-wrap:wrap}.place-items-center{place-items:center}.items-start{align-items:flex-start}.items-end{align-items:flex-end}.items-center{align-items:center}.justify-start{justify-content:flex-start}.justify-end{justify-content:flex-end}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.gap-1{gap:.25rem}.gap-1\.5{gap:.375rem}.gap-2{gap:.5rem}.gap-3{gap:.75rem}.gap-4{gap:1rem}.gap-5{gap:1.25rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.gap-x-1{-moz-column-gap:.25rem;column-gap:.25rem}.gap-y-1\.5{row-gap:.375rem}.gap-y-2{row-gap:.5rem}.space-x-2>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(.5rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(.5rem*var(--tw-space-x-reverse))}.space-x-4>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(1rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(1rem*var(--tw-space-x-reverse))}.space-y-1>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.25rem*var(--tw-space-y-reverse));margin-top:calc(.25rem*(1 - var(--tw-space-y-reverse)))}.space-y-1\.5>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.375rem*var(--tw-space-y-reverse));margin-top:calc(.375rem*(1 - var(--tw-space-y-reverse)))}.space-y-10>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(2.5rem*var(--tw-space-y-reverse));margin-top:calc(2.5rem*(1 - var(--tw-space-y-reverse)))}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.5rem*var(--tw-space-y-reverse));margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)))}.divide-x>:not([hidden])~:not([hidden]){--tw-divide-x-reverse:0;border-left-width:calc(1px*(1 - var(--tw-divide-x-reverse)));border-right-width:calc(1px*var(--tw-divide-x-reverse))}.divide-y>:not([hidden])~:not([hidden]){--tw-divide-y-reverse:0;border-bottom-width:calc(1px*var(--tw-divide-y-reverse));border-top-width:calc(1px*(1 - var(--tw-divide-y-reverse)))}.self-center{align-self:center}.overflow-hidden{overflow:hidden}.overflow-clip{overflow:clip}.overflow-x-auto{overflow-x:auto}.overflow-y-auto{overflow-y:auto}.overflow-x-hidden{overflow-x:hidden}.truncate{overflow:hidden;text-overflow:ellipsis}.truncate,.whitespace-nowrap{white-space:nowrap}.text-nowrap{text-wrap:nowrap}.break-words{overflow-wrap:break-word}.rounded{border-radius:.25rem}.rounded-\[inherit\]{border-radius:inherit}.rounded-full{border-radius:9999px}.rounded-lg{border-radius:var(--radius)}.rounded-md{border-radius:calc(var(--radius) - 2px)}.rounded-none{border-radius:0}.rounded-sm{border-radius:calc(var(--radius) - 4px)}.rounded-t-none{border-top-left-radius:0;border-top-right-radius:0}.rounded-tl-sm{border-top-left-radius:calc(var(--radius) - 4px)}.border{border-width:1px}.border-2{border-width:2px}.border-b{border-bottom-width:1px}.border-b-2{border-bottom-width:2px}.border-l{border-left-width:1px}.border-l-2{border-left-width:2px}.border-r{border-right-width:1px}.border-t{border-top-width:1px}.border-none{border-style:none}.border-\[\#adfa1d\]{--tw-border-opacity:1;border-color:rgb(173 250 29/var(--tw-border-opacity,1))}.border-amber-500{--tw-border-opacity:1;border-color:rgb(245 158 11/var(--tw-border-opacity,1))}.border-amber-600{--tw-border-opacity:1;border-color:rgb(217 119 6/var(--tw-border-opacity,1))}.border-blue-700{--tw-border-opacity:1;border-color:rgb(29 78 216/var(--tw-border-opacity,1))}.border-border{border-color:hsl(var(--border))}.border-destructive{border-color:hsl(var(--destructive))}.border-destructive\/50{border-color:hsl(var(--destructive)/.5)}.border-green-500{--tw-border-opacity:1;border-color:rgb(34 197 94/var(--tw-border-opacity,1))}.border-green-600{--tw-border-opacity:1;border-color:rgb(22 163 74/var(--tw-border-opacity,1))}.border-input{border-color:hsl(var(--input))}.border-primary{border-color:hsl(var(--primary))}.border-red-500{--tw-border-opacity:1;border-color:rgb(239 68 68/var(--tw-border-opacity,1))}.border-red-600{--tw-border-opacity:1;border-color:rgb(220 38 38/var(--tw-border-opacity,1))}.border-sky-500{--tw-border-opacity:1;border-color:rgb(14 165 233/var(--tw-border-opacity,1))}.border-sky-600{--tw-border-opacity:1;border-color:rgb(2 132 199/var(--tw-border-opacity,1))}.border-transparent{border-color:transparent}.border-violet-600{--tw-border-opacity:1;border-color:rgb(124 58 237/var(--tw-border-opacity,1))}.border-b-transparent{border-bottom-color:transparent}.border-l-transparent{border-left-color:transparent}.border-t-transparent{border-top-color:transparent}.bg-\[\#adfa1d\]{--tw-bg-opacity:1;background-color:rgb(173 250 29/var(--tw-bg-opacity,1))}.bg-amber-500{--tw-bg-opacity:1;background-color:rgb(245 158 11/var(--tw-bg-opacity,1))}.bg-background{background-color:hsl(var(--background))}.bg-background\/80{background-color:hsl(var(--background)/.8)}.bg-black\/80{background-color:#000c}.bg-border{background-color:hsl(var(--border))}.bg-card{background-color:hsl(var(--card))}.bg-destructive{background-color:hsl(var(--destructive))}.bg-green-100{--tw-bg-opacity:1;background-color:rgb(220 252 231/var(--tw-bg-opacity,1))}.bg-green-500{--tw-bg-opacity:1;background-color:rgb(34 197 94/var(--tw-bg-opacity,1))}.bg-muted{background-color:hsl(var(--muted))}.bg-muted\/30{background-color:hsl(var(--muted)/.3)}.bg-muted\/40{background-color:hsl(var(--muted)/.4)}.bg-muted\/50{background-color:hsl(var(--muted)/.5)}.bg-popover{background-color:hsl(var(--popover))}.bg-primary{background-color:hsl(var(--primary))}.bg-primary\/80{background-color:hsl(var(--primary)/.8)}.bg-red-100{--tw-bg-opacity:1;background-color:rgb(254 226 226/var(--tw-bg-opacity,1))}.bg-red-500{--tw-bg-opacity:1;background-color:rgb(239 68 68/var(--tw-bg-opacity,1))}.bg-secondary{background-color:hsl(var(--secondary))}.bg-sky-500{--tw-bg-opacity:1;background-color:rgb(14 165 233/var(--tw-bg-opacity,1))}.bg-transparent{background-color:transparent}.bg-white{--tw-bg-opacity:1;background-color:rgb(255 255 255/var(--tw-bg-opacity,1))}.bg-zinc-950{--tw-bg-opacity:1;background-color:rgb(9 9 11/var(--tw-bg-opacity,1))}.object-cover{-o-object-fit:cover;object-fit:cover}.object-\[50\%_53\%\]{-o-object-position:50% 53%;object-position:50% 53%}.object-center{-o-object-position:center;object-position:center}.\!p-2{padding:.5rem!important}.p-0{padding:0}.p-0\.5{padding:.125rem}.p-1{padding:.25rem}.p-1\.5{padding:.375rem}.p-16{padding:4rem}.p-2{padding:.5rem}.p-3{padding:.75rem}.p-4{padding:1rem}.p-6{padding:1.5rem}.p-px{padding:1px}.px-0\.5{padding-left:.125rem;padding-right:.125rem}.px-1{padding-left:.25rem;padding-right:.25rem}.px-1\.5{padding-left:.375rem;padding-right:.375rem}.px-2{padding-left:.5rem;padding-right:.5rem}.px-2\.5{padding-left:.625rem;padding-right:.625rem}.px-3{padding-left:.75rem;padding-right:.75rem}.px-4{padding-left:1rem;padding-right:1rem}.px-8{padding-left:2rem;padding-right:2rem}.px-\[0\.3rem\]{padding-left:.3rem;padding-right:.3rem}.py-0\.5{padding-bottom:.125rem;padding-top:.125rem}.py-1{padding-bottom:.25rem;padding-top:.25rem}.py-1\.5{padding-bottom:.375rem;padding-top:.375rem}.py-2{padding-bottom:.5rem;padding-top:.5rem}.py-3{padding-bottom:.75rem;padding-top:.75rem}.py-4{padding-bottom:1rem;padding-top:1rem}.py-6{padding-bottom:1.5rem;padding-top:1.5rem}.py-8{padding-bottom:2rem;padding-top:2rem}.py-\[0\.2rem\]{padding-bottom:.2rem;padding-top:.2rem}.pb-2{padding-bottom:.5rem}.pb-3{padding-bottom:.75rem}.pb-4{padding-bottom:1rem}.pb-5{padding-bottom:1.25rem}.pl-2{padding-left:.5rem}.pl-3{padding-left:.75rem}.pl-4{padding-left:1rem}.pl-6{padding-left:1.5rem}.pl-8{padding-left:2rem}.pr-0{padding-right:0}.pr-1\.5{padding-right:.375rem}.pr-3{padding-right:.75rem}.pr-6{padding-right:1.5rem}.pt-0{padding-top:0}.pt-1{padding-top:.25rem}.pt-2{padding-top:.5rem}.pt-4{padding-top:1rem}.pt-5{padding-top:1.25rem}.pt-6{padding-top:1.5rem}.text-left{text-align:left}.text-center{text-align:center}.text-right{text-align:right}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-3xl{font-size:1.875rem;line-height:2.25rem}.text-4xl{font-size:2.25rem;line-height:2.5rem}.text-5xl{font-size:3rem;line-height:1}.text-8xl{font-size:6rem;line-height:1}.text-\[10px\]{font-size:10px}.text-\[11px\]{font-size:11px}.text-\[12px\]{font-size:12px}.text-base{font-size:1rem;line-height:1.5rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xl{font-size:1.25rem;line-height:1.75rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-extrabold{font-weight:800}.font-light{font-weight:300}.font-medium{font-weight:500}.font-normal{font-weight:400}.font-semibold{font-weight:600}.uppercase{text-transform:uppercase}.capitalize{text-transform:capitalize}.italic{font-style:italic}.leading-4{line-height:1rem}.leading-7{line-height:1.75rem}.leading-none{line-height:1}.leading-tight{line-height:1.25}.tracking-tight{letter-spacing:-.025em}.tracking-tighter{letter-spacing:-.05em}.\!text-primary{color:hsl(var(--primary))!important}.text-amber-500{--tw-text-opacity:1;color:rgb(245 158 11/var(--tw-text-opacity,1))}.text-amber-600{--tw-text-opacity:1;color:rgb(217 119 6/var(--tw-text-opacity,1))}.text-black{--tw-text-opacity:1;color:rgb(0 0 0/var(--tw-text-opacity,1))}.text-blue-700{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity,1))}.text-card-foreground{color:hsl(var(--card-foreground))}.text-destructive{color:hsl(var(--destructive))}.text-destructive-foreground{color:hsl(var(--destructive-foreground))}.text-foreground{color:hsl(var(--foreground))}.text-foreground\/50{color:hsl(var(--foreground)/.5)}.text-foreground\/70{color:hsl(var(--foreground)/.7)}.text-foreground\/80{color:hsl(var(--foreground)/.8)}.text-green-500{--tw-text-opacity:1;color:rgb(34 197 94/var(--tw-text-opacity,1))}.text-green-600{--tw-text-opacity:1;color:rgb(22 163 74/var(--tw-text-opacity,1))}.text-muted-foreground{color:hsl(var(--muted-foreground))}.text-popover-foreground{color:hsl(var(--popover-foreground))}.text-primary{color:hsl(var(--primary))}.text-primary-foreground{color:hsl(var(--primary-foreground))}.text-red-500{--tw-text-opacity:1;color:rgb(239 68 68/var(--tw-text-opacity,1))}.text-red-600{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity,1))}.text-secondary-foreground{color:hsl(var(--secondary-foreground))}.text-sky-500{--tw-text-opacity:1;color:rgb(14 165 233/var(--tw-text-opacity,1))}.text-sky-600{--tw-text-opacity:1;color:rgb(2 132 199/var(--tw-text-opacity,1))}.text-violet-600{--tw-text-opacity:1;color:rgb(124 58 237/var(--tw-text-opacity,1))}.text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.text-zinc-100{--tw-text-opacity:1;color:rgb(244 244 245/var(--tw-text-opacity,1))}.text-zinc-400{--tw-text-opacity:1;color:rgb(161 161 170/var(--tw-text-opacity,1))}.text-zinc-500{--tw-text-opacity:1;color:rgb(113 113 122/var(--tw-text-opacity,1))}.text-zinc-900{--tw-text-opacity:1;color:rgb(24 24 27/var(--tw-text-opacity,1))}.underline{text-decoration-line:underline}.no-underline{text-decoration-line:none}.underline-offset-4{text-underline-offset:4px}.opacity-0{opacity:0}.opacity-100{opacity:1}.opacity-50{opacity:.5}.opacity-70{opacity:.7}.opacity-90{opacity:.9}.shadow-lg{--tw-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -4px rgba(0,0,0,.1);--tw-shadow-colored:0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color)}.shadow-lg,.shadow-md{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-md{--tw-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -2px rgba(0,0,0,.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color)}.shadow-none{--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000}.shadow-none,.shadow-sm{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-sm{--tw-shadow:0 1px 2px 0 rgba(0,0,0,.05);--tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color)}.outline-none{outline:2px solid transparent;outline-offset:2px}.outline{outline-style:solid}.ring-offset-background{--tw-ring-offset-color:hsl(var(--background))}.filter{filter:var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow)}.backdrop-blur-lg{--tw-backdrop-blur:blur(16px);-webkit-backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia);backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia)}.transition{transition-duration:.15s;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-all{transition-duration:.15s;transition-property:all;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-colors{transition-duration:.15s;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-none{transition-property:none}.transition-opacity{transition-duration:.15s;transition-property:opacity;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-transform{transition-duration:.15s;transition-property:transform;transition-timing-function:cubic-bezier(.4,0,.2,1)}.duration-200{transition-duration:.2s}.duration-75{transition-duration:75ms}.ease-in-out{transition-timing-function:cubic-bezier(.4,0,.2,1)}@keyframes enter{0%{opacity:var(--tw-enter-opacity,1);transform:translate3d(var(--tw-enter-translate-x,0),var(--tw-enter-translate-y,0),0) scale3d(var(--tw-enter-scale,1),var(--tw-enter-scale,1),var(--tw-enter-scale,1)) rotate(var(--tw-enter-rotate,0))}}@keyframes exit{to{opacity:var(--tw-exit-opacity,1);transform:translate3d(var(--tw-exit-translate-x,0),var(--tw-exit-translate-y,0),0) scale3d(var(--tw-exit-scale,1),var(--tw-exit-scale,1),var(--tw-exit-scale,1)) rotate(var(--tw-exit-rotate,0))}}.duration-200{animation-duration:.2s}.duration-75{animation-duration:75ms}.ease-in-out{animation-timing-function:cubic-bezier(.4,0,.2,1)}.running{animation-play-state:running}.step{counter-increment:step}.step:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[counter-reset\:step\]{counter-reset:step}.placeholder\:text-muted-foreground::-moz-placeholder{color:hsl(var(--muted-foreground))}.placeholder\:text-muted-foreground::placeholder{color:hsl(var(--muted-foreground))}.even\:bg-muted\/50:nth-child(2n){background-color:hsl(var(--muted)/.5)}.hover\:cursor-pointer:hover{cursor:pointer}.hover\:bg-\[\#adfa1d\]:hover{--tw-bg-opacity:1;background-color:rgb(173 250 29/var(--tw-bg-opacity,1))}.hover\:bg-accent:hover{background-color:hsl(var(--accent))}.hover\:bg-amber-400:hover{--tw-bg-opacity:1;background-color:rgb(251 191 36/var(--tw-bg-opacity,1))}.hover\:bg-destructive\/80:hover{background-color:hsl(var(--destructive)/.8)}.hover\:bg-destructive\/90:hover{background-color:hsl(var(--destructive)/.9)}.hover\:bg-green-400:hover{--tw-bg-opacity:1;background-color:rgb(74 222 128/var(--tw-bg-opacity,1))}.hover\:bg-muted:hover{background-color:hsl(var(--muted))}.hover\:bg-muted\/40:hover{background-color:hsl(var(--muted)/.4)}.hover\:bg-muted\/50:hover{background-color:hsl(var(--muted)/.5)}.hover\:bg-primary\/80:hover{background-color:hsl(var(--primary)/.8)}.hover\:bg-primary\/90:hover{background-color:hsl(var(--primary)/.9)}.hover\:bg-red-400:hover{--tw-bg-opacity:1;background-color:rgb(248 113 113/var(--tw-bg-opacity,1))}.hover\:bg-secondary:hover{background-color:hsl(var(--secondary))}.hover\:bg-secondary\/80:hover{background-color:hsl(var(--secondary)/.8)}.hover\:bg-sky-400:hover{--tw-bg-opacity:1;background-color:rgb(56 189 248/var(--tw-bg-opacity,1))}.hover\:text-accent-foreground:hover{color:hsl(var(--accent-foreground))}.hover\:text-foreground:hover{color:hsl(var(--foreground))}.hover\:text-primary:hover{color:hsl(var(--primary))}.hover\:underline:hover{text-decoration-line:underline}.hover\:opacity-100:hover{opacity:1}.focus\:bg-accent:focus{background-color:hsl(var(--accent))}.focus\:text-accent-foreground:focus{color:hsl(var(--accent-foreground))}.focus\:opacity-100:focus{opacity:1}.focus\:outline-none:focus{outline:2px solid transparent;outline-offset:2px}.focus\:ring-1:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color)}.focus\:ring-1:focus,.focus\:ring-2:focus{box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.focus\:ring-2:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color)}.focus\:ring-ring:focus{--tw-ring-color:hsl(var(--ring))}.focus\:ring-offset-2:focus{--tw-ring-offset-width:2px}.focus-visible\:outline-none:focus-visible{outline:2px solid transparent;outline-offset:2px}.focus-visible\:ring-2:focus-visible{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color);box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.focus-visible\:ring-ring:focus-visible{--tw-ring-color:hsl(var(--ring))}.focus-visible\:ring-offset-2:focus-visible{--tw-ring-offset-width:2px}.disabled\:pointer-events-none:disabled{pointer-events:none}.disabled\:cursor-not-allowed:disabled{cursor:not-allowed}.disabled\:opacity-50:disabled{opacity:.5}.group:hover .group-hover\:opacity-100{opacity:1}.group.destructive .group-\[\.destructive\]\:border-muted\/40{border-color:hsl(var(--muted)/.4)}.group.destructive .group-\[\.destructive\]\:text-red-300{--tw-text-opacity:1;color:rgb(252 165 165/var(--tw-text-opacity,1))}.group.destructive .group-\[\.destructive\]\:hover\:border-destructive\/30:hover{border-color:hsl(var(--destructive)/.3)}.group.destructive .group-\[\.destructive\]\:hover\:bg-destructive:hover{background-color:hsl(var(--destructive))}.group.destructive .group-\[\.destructive\]\:hover\:text-destructive-foreground:hover{color:hsl(var(--destructive-foreground))}.group.destructive .group-\[\.destructive\]\:hover\:text-red-50:hover{--tw-text-opacity:1;color:rgb(254 242 242/var(--tw-text-opacity,1))}.group.destructive .group-\[\.destructive\]\:focus\:ring-destructive:focus{--tw-ring-color:hsl(var(--destructive))}.group.destructive .group-\[\.destructive\]\:focus\:ring-red-400:focus{--tw-ring-opacity:1;--tw-ring-color:rgb(248 113 113/var(--tw-ring-opacity,1))}.group.destructive .group-\[\.destructive\]\:focus\:ring-offset-red-600:focus{--tw-ring-offset-color:#dc2626}.peer:disabled~.peer-disabled\:cursor-not-allowed{cursor:not-allowed}.peer:disabled~.peer-disabled\:opacity-70{opacity:.7}.group:has(div) .group-has-\[div\]\:mt-0{margin-top:0}.data-\[disabled\]\:pointer-events-none[data-disabled]{pointer-events:none}.data-\[swipe\=cancel\]\:translate-x-0[data-swipe=cancel]{--tw-translate-x:0px}.data-\[swipe\=cancel\]\:translate-x-0[data-swipe=cancel],.data-\[swipe\=end\]\:translate-x-\[var\(--radix-toast-swipe-end-x\)\][data-swipe=end]{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.data-\[swipe\=end\]\:translate-x-\[var\(--radix-toast-swipe-end-x\)\][data-swipe=end]{--tw-translate-x:var(--radix-toast-swipe-end-x)}.data-\[swipe\=move\]\:translate-x-\[var\(--radix-toast-swipe-move-x\)\][data-swipe=move]{--tw-translate-x:var(--radix-toast-swipe-move-x);transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@keyframes accordion-up{0%{height:var(--radix-accordion-content-height)}to{height:0}}.data-\[state\=closed\]\:animate-accordion-up[data-state=closed]{animation:accordion-up .2s ease-out}@keyframes collapsible-up{0%{height:var(--radix-collapsible-content-height)}to{height:0}}.data-\[state\=closed\]\:animate-collapsible-up[data-state=closed]{animation:collapsible-up .2s ease-in-out}@keyframes accordion-down{0%{height:0}to{height:var(--radix-accordion-content-height)}}.data-\[state\=open\]\:animate-accordion-down[data-state=open]{animation:accordion-down .2s ease-out}@keyframes collapsible-down{0%{height:0}to{height:var(--radix-collapsible-content-height)}}.data-\[state\=open\]\:animate-collapsible-down[data-state=open]{animation:collapsible-down .2s ease-in-out}.data-\[state\=active\]\:border-b-primary[data-state=active]{border-bottom-color:hsl(var(--primary))}.data-\[active\]\:bg-accent\/50[data-active]{background-color:hsl(var(--accent)/.5)}.data-\[highlighted\]\:bg-accent[data-highlighted]{background-color:hsl(var(--accent))}.data-\[state\=active\]\:bg-background[data-state=active]{background-color:hsl(var(--background))}.data-\[state\=open\]\:bg-accent[data-state=open]{background-color:hsl(var(--accent))}.data-\[state\=open\]\:bg-accent\/50[data-state=open]{background-color:hsl(var(--accent)/.5)}.data-\[state\=open\]\:bg-secondary[data-state=open]{background-color:hsl(var(--secondary))}.data-\[state\=active\]\:text-foreground[data-state=active]{color:hsl(var(--foreground))}.data-\[state\=open\]\:text-muted-foreground[data-state=open]{color:hsl(var(--muted-foreground))}.data-\[disabled\]\:opacity-50[data-disabled]{opacity:.5}.data-\[state\=active\]\:shadow-none[data-state=active]{--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.data-\[state\=active\]\:shadow-sm[data-state=active]{--tw-shadow:0 1px 2px 0 rgba(0,0,0,.05);--tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color);box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.data-\[swipe\=move\]\:transition-none[data-swipe=move]{transition-property:none}.data-\[state\=closed\]\:duration-300[data-state=closed]{transition-duration:.3s}.data-\[state\=open\]\:duration-500[data-state=open]{transition-duration:.5s}.data-\[motion\^\=from-\]\:animate-in[data-motion^=from-],.data-\[state\=open\]\:animate-in[data-state=open],.data-\[state\=visible\]\:animate-in[data-state=visible]{animation-duration:.15s;animation-name:enter;--tw-enter-opacity:initial;--tw-enter-scale:initial;--tw-enter-rotate:initial;--tw-enter-translate-x:initial;--tw-enter-translate-y:initial}.data-\[motion\^\=to-\]\:animate-out[data-motion^=to-],.data-\[state\=closed\]\:animate-out[data-state=closed],.data-\[state\=hidden\]\:animate-out[data-state=hidden],.data-\[swipe\=end\]\:animate-out[data-swipe=end]{animation-duration:.15s;animation-name:exit;--tw-exit-opacity:initial;--tw-exit-scale:initial;--tw-exit-rotate:initial;--tw-exit-translate-x:initial;--tw-exit-translate-y:initial}.data-\[motion\^\=from-\]\:fade-in[data-motion^=from-]{--tw-enter-opacity:0}.data-\[motion\^\=to-\]\:fade-out[data-motion^=to-],.data-\[state\=closed\]\:fade-out-0[data-state=closed]{--tw-exit-opacity:0}.data-\[state\=closed\]\:fade-out-80[data-state=closed]{--tw-exit-opacity:.8}.data-\[state\=hidden\]\:fade-out[data-state=hidden]{--tw-exit-opacity:0}.data-\[state\=open\]\:fade-in-0[data-state=open],.data-\[state\=visible\]\:fade-in[data-state=visible]{--tw-enter-opacity:0}.data-\[state\=closed\]\:zoom-out-95[data-state=closed]{--tw-exit-scale:.95}.data-\[state\=open\]\:zoom-in-90[data-state=open]{--tw-enter-scale:.9}.data-\[state\=open\]\:zoom-in-95[data-state=open]{--tw-enter-scale:.95}.data-\[motion\=from-end\]\:slide-in-from-right-52[data-motion=from-end]{--tw-enter-translate-x:13rem}.data-\[motion\=from-start\]\:slide-in-from-left-52[data-motion=from-start]{--tw-enter-translate-x:-13rem}.data-\[motion\=to-end\]\:slide-out-to-right-52[data-motion=to-end]{--tw-exit-translate-x:13rem}.data-\[motion\=to-start\]\:slide-out-to-left-52[data-motion=to-start]{--tw-exit-translate-x:-13rem}.data-\[side\=bottom\]\:slide-in-from-top-2[data-side=bottom]{--tw-enter-translate-y:-.5rem}.data-\[side\=left\]\:slide-in-from-right-2[data-side=left]{--tw-enter-translate-x:.5rem}.data-\[side\=right\]\:slide-in-from-left-2[data-side=right]{--tw-enter-translate-x:-.5rem}.data-\[side\=top\]\:slide-in-from-bottom-2[data-side=top]{--tw-enter-translate-y:.5rem}.data-\[state\=closed\]\:slide-out-to-bottom[data-state=closed]{--tw-exit-translate-y:100%}.data-\[state\=closed\]\:slide-out-to-left[data-state=closed]{--tw-exit-translate-x:-100%}.data-\[state\=closed\]\:slide-out-to-left-1\/2[data-state=closed]{--tw-exit-translate-x:-50%}.data-\[state\=closed\]\:slide-out-to-right-full[data-state=closed],.data-\[state\=closed\]\:slide-out-to-right[data-state=closed]{--tw-exit-translate-x:100%}.data-\[state\=closed\]\:slide-out-to-top[data-state=closed]{--tw-exit-translate-y:-100%}.data-\[state\=closed\]\:slide-out-to-top-\[48\%\][data-state=closed]{--tw-exit-translate-y:-48%}.data-\[state\=open\]\:slide-in-from-bottom[data-state=open]{--tw-enter-translate-y:100%}.data-\[state\=open\]\:slide-in-from-left[data-state=open]{--tw-enter-translate-x:-100%}.data-\[state\=open\]\:slide-in-from-left-1\/2[data-state=open]{--tw-enter-translate-x:-50%}.data-\[state\=open\]\:slide-in-from-right[data-state=open]{--tw-enter-translate-x:100%}.data-\[state\=open\]\:slide-in-from-top[data-state=open]{--tw-enter-translate-y:-100%}.data-\[state\=open\]\:slide-in-from-top-\[48\%\][data-state=open]{--tw-enter-translate-y:-48%}.data-\[state\=open\]\:slide-in-from-top-full[data-state=open]{--tw-enter-translate-y:-100%}.data-\[state\=closed\]\:duration-300[data-state=closed]{animation-duration:.3s}.data-\[state\=open\]\:duration-500[data-state=open]{animation-duration:.5s}.group[data-state=open] .group-data-\[state\=open\]\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:block:is(.dark *){display:block}.dark\:hidden:is(.dark *){display:none}.dark\:-rotate-90:is(.dark *){--tw-rotate:-90deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:rotate-0:is(.dark *){--tw-rotate:0deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:scale-0:is(.dark *){--tw-scale-x:0;--tw-scale-y:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:scale-100:is(.dark *){--tw-scale-x:1;--tw-scale-y:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:border-destructive:is(.dark *){border-color:hsl(var(--destructive))}@media (min-width:640px){.sm\:bottom-0{bottom:0}.sm\:right-0{right:0}.sm\:top-auto{top:auto}.sm\:inline{display:inline}.sm\:hidden{display:none}.sm\:h-\[350px\]{height:350px}.sm\:max-w-sm{max-width:24rem}.sm\:flex-row{flex-direction:row}.sm\:flex-col{flex-direction:column}.sm\:justify-end{justify-content:flex-end}.sm\:gap-2\.5{gap:.625rem}.sm\:gap-x-2{-moz-column-gap:.5rem;column-gap:.5rem}.sm\:rounded-lg{border-radius:var(--radius)}.sm\:text-left{text-align:left}.sm\:text-xl{font-size:1.25rem;line-height:1.75rem}.data-\[state\=open\]\:sm\:slide-in-from-bottom-full[data-state=open]{--tw-enter-translate-y:100%}}@media (min-width:768px){.md\:absolute{position:absolute}.md\:sticky{position:sticky}.md\:top-\[60px\]{top:60px}.md\:order-last{order:9999}.md\:col-span-2{grid-column:span 2/span 2}.md\:block{display:block}.md\:flex{display:flex}.md\:grid{display:grid}.md\:hidden{display:none}.md\:h-24{height:6rem}.md\:w-40{width:10rem}.md\:w-\[--radix-navigation-menu-viewport-width\]{width:var(--radix-navigation-menu-viewport-width)}.md\:w-auto{width:auto}.md\:w-full{width:100%}.md\:max-w-\[420px\]{max-width:420px}.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.md\:grid-cols-\[220px_minmax\(0\,1fr\)\]{grid-template-columns:220px minmax(0,1fr)}.md\:flex-row{flex-direction:row}.md\:gap-6{gap:1.5rem}.md\:px-8{padding-left:2rem;padding-right:2rem}.md\:py-0{padding-bottom:0;padding-top:0}.md\:py-12{padding-bottom:3rem;padding-top:3rem}.md\:pb-10{padding-bottom:2.5rem}.md\:pb-8{padding-bottom:2rem}.md\:pr-4{padding-right:1rem}.md\:text-4xl{font-size:2.25rem;line-height:2.5rem}.md\:text-6xl{font-size:3.75rem;line-height:1}}@media (min-width:1024px){.lg\:mt-6{margin-top:1.5rem}.lg\:block{display:block}.lg\:flex{display:flex}.lg\:grid{display:grid}.lg\:hidden{display:none}.lg\:w-64{width:16rem}.lg\:flex-none{flex:none}.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:grid-cols-\[1fr_220px\]{grid-template-columns:1fr 220px}.lg\:grid-cols-\[240px_minmax\(0\,1fr\)\]{grid-template-columns:240px minmax(0,1fr)}.lg\:flex-row{flex-direction:row}.lg\:flex-col{flex-direction:column}.lg\:gap-10{gap:2.5rem}.lg\:gap-14{gap:3.5rem}.lg\:border-b{border-bottom-width:1px}.lg\:border-b-0{border-bottom-width:0}.lg\:border-r{border-right-width:1px}.lg\:border-none{border-style:none}.lg\:py-12{padding-bottom:3rem;padding-top:3rem}.lg\:py-24{padding-bottom:6rem;padding-top:6rem}.lg\:py-6{padding-bottom:1.5rem;padding-top:1.5rem}.lg\:py-8{padding-bottom:2rem;padding-top:2rem}.lg\:pb-10{padding-bottom:2.5rem}.lg\:pb-20{padding-bottom:5rem}.lg\:text-center{text-align:center}.lg\:text-5xl{font-size:3rem;line-height:1}.lg\:leading-\[1\.1\]{line-height:1.1}}.\[\&\+div\]\:text-xs+div{font-size:.75rem;line-height:1rem}.\[\&\:not\(\:first-child\)\]\:mt-10:not(:first-child){margin-top:2.5rem}.\[\&\:not\(\:first-child\)\]\:mt-4:not(:first-child){margin-top:1rem}.\[\&\:not\(\:first-child\)\]\:mt-5:not(:first-child){margin-top:1.25rem}.\[\&\:not\(\:first-child\)\]\:mt-6:not(:first-child){margin-top:1.5rem}.\[\&\:not\(\:first-child\)\]\:mt-8:not(:first-child){margin-top:2rem}.\[\&\:not\(\:first-child\)\]\:pt-3:not(:first-child){padding-top:.75rem}.\[\&\:not\(\:first-child\)\]\:pt-4:not(:first-child){padding-top:1rem}.\[\&\:not\(\:last-child\)\]\:mb-12:not(:last-child){margin-bottom:3rem}.\[\&\:not\(\:last-child\)\]\:mb-5:not(:last-child){margin-bottom:1.25rem}.\[\&\:not\(\:last-child\)\]\:mb-6:not(:last-child){margin-bottom:1.5rem}.\[\&\:not\(\:last-child\)\]\:mb-8:not(:last-child){margin-bottom:2rem}.\[\&\>h1\]\:step>h1{counter-increment:step}.\[\&\>h1\]\:step>h1:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h2\]\:step>h2{counter-increment:step}.\[\&\>h2\]\:step>h2:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h3\]\:step>h3{counter-increment:step}.\[\&\>h3\]\:step>h3:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h4\]\:step>h4{counter-increment:step}.\[\&\>h4\]\:step>h4:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h5\]\:step>h5{counter-increment:step}.\[\&\>h5\]\:step>h5:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h6\]\:step>h6{counter-increment:step}.\[\&\>h6\]\:step>h6:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>li\:not\(\:first-child\)\]\:mt-2>li:not(:first-child){margin-top:.5rem}.\[\&\>li\]\:step>li{counter-increment:step}.\[\&\>li\]\:step>li:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>ol\]\:\!mt-2>ol{margin-top:.5rem!important}.\[\&\>svg\+div\]\:translate-y-\[-3px\]>svg+div{--tw-translate-y:-3px;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.\[\&\>svg\]\:absolute>svg{position:absolute}.\[\&\>svg\]\:left-4>svg{left:1rem}.\[\&\>svg\]\:top-4>svg{top:1rem}.\[\&\>svg\]\:size-3\.5>svg{height:.875rem;width:.875rem}.\[\&\>svg\]\:text-amber-600>svg{--tw-text-opacity:1;color:rgb(217 119 6/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-blue-700>svg{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-destructive>svg{color:hsl(var(--destructive))}.\[\&\>svg\]\:text-foreground>svg{color:hsl(var(--foreground))}.\[\&\>svg\]\:text-green-600>svg{--tw-text-opacity:1;color:rgb(22 163 74/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-red-600>svg{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-sky-600>svg{--tw-text-opacity:1;color:rgb(2 132 199/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-violet-600>svg{--tw-text-opacity:1;color:rgb(124 58 237/var(--tw-text-opacity,1))}.\[\&\>svg\~\*\]\:pl-7>svg~*{padding-left:1.75rem}.\[\&\>ul\]\:\!mt-2>ul{margin-top:.5rem!important}.\[\&\[align\=center\]\]\:text-center[align=center]{text-align:center}.\[\&\[align\=right\]\]\:text-right[align=right]{text-align:right}.\[\&\[data-state\=open\]\>svg\]\:rotate-180[data-state=open]>svg{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.\[\&_\[cmdk-group-heading\]\]\:px-2 [cmdk-group-heading]{padding-left:.5rem;padding-right:.5rem}.\[\&_\[cmdk-group-heading\]\]\:py-1\.5 [cmdk-group-heading]{padding-bottom:.375rem;padding-top:.375rem}.\[\&_\[cmdk-group-heading\]\]\:text-xs [cmdk-group-heading]{font-size:.75rem;line-height:1rem}.\[\&_\[cmdk-group-heading\]\]\:font-medium [cmdk-group-heading]{font-weight:500}.\[\&_\[cmdk-group-heading\]\]\:text-muted-foreground [cmdk-group-heading]{color:hsl(var(--muted-foreground))}.\[\&_p\]\:leading-relaxed p{line-height:1.625}</style>
<style>html{color-scheme:light}html.dark{color-scheme:dark}.theme-zinc{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:240 5.9% 10%;--primary-foreground:0 0% 98%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:240 5.9% 10%;--radius:.5rem}.dark .theme-zinc{--background:240 10% 3.9%;--foreground:0 0% 98%;--muted:240 3.7% 15.9%;--muted-foreground:240 5% 64.9%;--popover:240 10% 3.9%;--popover-foreground:0 0% 98%;--card:240 10% 3.9%;--card-foreground:0 0% 98%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:0 0% 98%;--primary-foreground:240 5.9% 10%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:240 3.7% 15.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:240 4.9% 83.9%}.theme-slate{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--primary:222.2 47.4% 11.2%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--ring:222.2 84% 4.9%;--radius:.5rem}.dark .theme-slate{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--primary:210 40% 98%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--ring:212.7 26.8% 83.9}.theme-stone{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:24 9.8% 10%;--primary-foreground:60 9.1% 97.8%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:20 14.3% 4.1%;--radius:.5rem}.dark .theme-stone{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:60 9.1% 97.8%;--primary-foreground:24 9.8% 10%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 62.8% 30.6%;--destructive-foreground:60 9.1% 97.8%;--ring:24 5.7% 82.9%}.theme-gray{--background:0 0% 100%;--foreground:224 71.4% 4.1%;--muted:220 14.3% 95.9%;--muted-foreground:220 8.9% 46.1%;--popover:0 0% 100%;--popover-foreground:224 71.4% 4.1%;--card:0 0% 100%;--card-foreground:224 71.4% 4.1%;--border:220 13% 91%;--input:220 13% 91%;--primary:220.9 39.3% 11%;--primary-foreground:210 20% 98%;--secondary:220 14.3% 95.9%;--secondary-foreground:220.9 39.3% 11%;--accent:220 14.3% 95.9%;--accent-foreground:220.9 39.3% 11%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 20% 98%;--ring:224 71.4% 4.1%;--radius:.5rem}.dark .theme-gray{--background:224 71.4% 4.1%;--foreground:210 20% 98%;--muted:215 27.9% 16.9%;--muted-foreground:217.9 10.6% 64.9%;--popover:224 71.4% 4.1%;--popover-foreground:210 20% 98%;--card:224 71.4% 4.1%;--card-foreground:210 20% 98%;--border:215 27.9% 16.9%;--input:215 27.9% 16.9%;--primary:210 20% 98%;--primary-foreground:220.9 39.3% 11%;--secondary:215 27.9% 16.9%;--secondary-foreground:210 20% 98%;--accent:215 27.9% 16.9%;--accent-foreground:210 20% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 20% 98%;--ring:216 12.2% 83.9%}.theme-neutral{--background:0 0% 100%;--foreground:0 0% 3.9%;--muted:0 0% 96.1%;--muted-foreground:0 0% 45.1%;--popover:0 0% 100%;--popover-foreground:0 0% 3.9%;--card:0 0% 100%;--card-foreground:0 0% 3.9%;--border:0 0% 89.8%;--input:0 0% 89.8%;--primary:0 0% 9%;--primary-foreground:0 0% 98%;--secondary:0 0% 96.1%;--secondary-foreground:0 0% 9%;--accent:0 0% 96.1%;--accent-foreground:0 0% 9%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:0 0% 3.9%;--radius:.5rem}.dark .theme-neutral{--background:0 0% 3.9%;--foreground:0 0% 98%;--muted:0 0% 14.9%;--muted-foreground:0 0% 63.9%;--popover:0 0% 3.9%;--popover-foreground:0 0% 98%;--card:0 0% 3.9%;--card-foreground:0 0% 98%;--border:0 0% 14.9%;--input:0 0% 14.9%;--primary:0 0% 98%;--primary-foreground:0 0% 9%;--secondary:0 0% 14.9%;--secondary-foreground:0 0% 98%;--accent:0 0% 14.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:0 0% 83.1%}.theme-red{--background:0 0% 100%;--foreground:0 0% 3.9%;--muted:0 0% 96.1%;--muted-foreground:0 0% 45.1%;--popover:0 0% 100%;--popover-foreground:0 0% 3.9%;--card:0 0% 100%;--card-foreground:0 0% 3.9%;--border:0 0% 89.8%;--input:0 0% 89.8%;--primary:0 72.2% 50.6%;--primary-foreground:0 85.7% 97.3%;--secondary:0 0% 96.1%;--secondary-foreground:0 0% 9%;--accent:0 0% 96.1%;--accent-foreground:0 0% 9%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:0 72.2% 50.6%;--radius:.5rem}.dark .theme-red{--background:0 0% 3.9%;--foreground:0 0% 98%;--muted:0 0% 14.9%;--muted-foreground:0 0% 63.9%;--popover:0 0% 3.9%;--popover-foreground:0 0% 98%;--card:0 0% 3.9%;--card-foreground:0 0% 98%;--border:0 0% 14.9%;--input:0 0% 14.9%;--primary:0 72.2% 50.6%;--primary-foreground:0 85.7% 97.3%;--secondary:0 0% 14.9%;--secondary-foreground:0 0% 98%;--accent:0 0% 14.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:0 72.2% 50.6%}.theme-rose{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:346.8 77.2% 49.8%;--primary-foreground:355.7 100% 97.3%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:346.8 77.2% 49.8%;--radius:.5rem}.dark .theme-rose{--background:20 14.3% 4.1%;--foreground:0 0% 95%;--muted:0 0% 15%;--muted-foreground:240 5% 64.9%;--popover:0 0% 9%;--popover-foreground:0 0% 95%;--card:24 9.8% 10%;--card-foreground:0 0% 95%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:346.8 77.2% 49.8%;--primary-foreground:355.7 100% 97.3%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:12 6.5% 15.1%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 85.7% 97.3%;--ring:346.8 77.2% 49.8%}.theme-orange{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:24.6 95% 53.1%;--primary-foreground:60 9.1% 97.8%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:24.6 95% 53.1%;--radius:.5rem}.dark .theme-orange{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:20.5 90.2% 48.2%;--primary-foreground:60 9.1% 97.8%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 72.2% 50.6%;--destructive-foreground:60 9.1% 97.8%;--ring:20.5 90.2% 48.2%}.theme-green{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:142.1 76.2% 36.3%;--primary-foreground:355.7 100% 97.3%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:142.1 76.2% 36.3%;--radius:.5rem}.dark .theme-green{--background:20 14.3% 4.1%;--foreground:0 0% 95%;--muted:0 0% 15%;--muted-foreground:240 5% 64.9%;--popover:0 0% 9%;--popover-foreground:0 0% 95%;--card:24 9.8% 10%;--card-foreground:0 0% 95%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:142.1 70.6% 45.3%;--primary-foreground:144.9 80.4% 10%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:12 6.5% 15.1%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 85.7% 97.3%;--ring:142.4 71.8% 29.2%}.theme-blue{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--primary:221.2 83.2% 53.3%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--ring:221.2 83.2% 53.3%;--radius:.5rem}.dark .theme-blue{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--primary:217.2 91.2% 59.8%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--ring:224.3 76.3% 48%}.theme-yellow{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:47.9 95.8% 53.1%;--primary-foreground:26 83.3% 14.1%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:20 14.3% 4.1%;--radius:.5rem}.dark .theme-yellow{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:47.9 95.8% 53.1%;--primary-foreground:26 83.3% 14.1%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 62.8% 30.6%;--destructive-foreground:60 9.1% 97.8%;--ring:35.5 91.7% 32.9%}.theme-violet{--background:0 0% 100%;--foreground:224 71.4% 4.1%;--muted:220 14.3% 95.9%;--muted-foreground:220 8.9% 46.1%;--popover:0 0% 100%;--popover-foreground:224 71.4% 4.1%;--card:0 0% 100%;--card-foreground:224 71.4% 4.1%;--border:220 13% 91%;--input:220 13% 91%;--primary:262.1 83.3% 57.8%;--primary-foreground:210 20% 98%;--secondary:220 14.3% 95.9%;--secondary-foreground:220.9 39.3% 11%;--accent:220 14.3% 95.9%;--accent-foreground:220.9 39.3% 11%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 20% 98%;--ring:262.1 83.3% 57.8%;--radius:.5rem}.dark .theme-violet{--background:224 71.4% 4.1%;--foreground:210 20% 98%;--muted:215 27.9% 16.9%;--muted-foreground:217.9 10.6% 64.9%;--popover:224 71.4% 4.1%;--popover-foreground:210 20% 98%;--card:224 71.4% 4.1%;--card-foreground:210 20% 98%;--border:215 27.9% 16.9%;--input:215 27.9% 16.9%;--primary:263.4 70% 50.4%;--primary-foreground:210 20% 98%;--secondary:215 27.9% 16.9%;--secondary-foreground:210 20% 98%;--accent:215 27.9% 16.9%;--accent-foreground:210 20% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 20% 98%;--ring:263.4 70% 50.4%}</style>
<style>.carbon-responsive-wrap{align-items:center!important;background-color:hsl(var(--background))!important;border-color:hsl(var(--border))!important;border-radius:var(--radius)!important;display:flex!important;padding:1rem!important}@media (min-width:1024px){.carbon-responsive-wrap{flex-direction:column!important;padding-bottom:1.5rem!important;padding-top:1.5rem!important}}.carbon-responsive-wrap .carbon-img{border-radius:.25rem!important;overflow:hidden!important}@media (min-width:1024px){.carbon-responsive-wrap .carbon-img{flex:none!important}}.carbon-responsive-wrap .carbon-text{color:hsl(var(--muted-foreground))!important;font-size:.875rem!important;line-height:1.25rem!important}@media (min-width:1024px){.carbon-responsive-wrap .carbon-text{flex:none!important;text-align:center!important}}#carbonads .carbon-poweredby{background-color:hsl(var(--background))!important;color:hsl(var(--muted-foreground))!important;display:block!important;font-size:10px!important;text-align:right!important;text-decoration-line:none!important;text-transform:uppercase!important}</style>
<link rel="stylesheet" href="/_nuxt/entry.BZKa7Edp.css" crossorigin>
<style>:where(.i-lucide\:arrow-left){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m12 19l-7-7l7-7m7 7H5'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-right){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M5 12h14m-7-7l7 7l-7 7'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-up){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m5 12l7-7l7 7m-7 7V5'/%3E%3C/svg%3E")}:where(.i-lucide\:bot){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M12 8V4H8'/%3E%3Crect width='16' height='12' x='4' y='8' rx='2'/%3E%3Cpath d='M2 14h2m16 0h2m-7-1v2m-6-2v2'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:brain){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M12 5a3 3 0 1 0-5.997.125a4 4 0 0 0-2.526 5.77a4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z'/%3E%3Cpath d='M12 5a3 3 0 1 1 5.997.125a4 4 0 0 1 2.526 5.77a4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z'/%3E%3Cpath d='M15 13a4.5 4.5 0 0 1-3-4a4.5 4.5 0 0 1-3 4m8.599-6.5a3 3 0 0 0 .399-1.375m-11.995 0A3 3 0 0 0 6.401 6.5m-2.924 4.396a4 4 0 0 1 .585-.396m15.876 0a4 4 0 0 1 .585.396M6 18a4 4 0 0 1-1.967-.516m15.934 0A4 4 0 0 1 18 18'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:database){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cellipse cx='12' cy='5' rx='9' ry='3'/%3E%3Cpath d='M3 5v14a9 3 0 0 0 18 0V5'/%3E%3Cpath d='M3 12a9 3 0 0 0 18 0'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:download){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4m4-5l5 5l5-5m-5 5V3'/%3E%3C/svg%3E")}:where(.i-lucide\:flag){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 15s1-1 4-1s5 2 8 2s4-1 4-1V3s-1 1-4 1s-5-2-8-2s-4 1-4 1zm0 7v-7'/%3E%3C/svg%3E")}:where(.i-lucide\:github){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5c.08-1.25-.27-2.48-1-3.5c.28-1.15.28-2.35 0-3.5c0 0-1 0-3 1.5c-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.4 5.4 0 0 0 4 9c0 3.5 3 5.5 6 5.5c-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4'/%3E%3Cpath d='M9 18c-4.51 2-5-2-7-2'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:hand){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M18 11V6a2 2 0 0 0-2-2a2 2 0 0 0-2 2m0 4V4a2 2 0 0 0-2-2a2 2 0 0 0-2 2v2m0 4.5V6a2 2 0 0 0-2-2a2 2 0 0 0-2 2v8'/%3E%3Cpath d='M18 8a2 2 0 1 1 4 0v6a8 8 0 0 1-8 8h-2c-2.8 0-4.5-.86-5.99-2.34l-3.6-3.6a2 2 0 0 1 2.83-2.82L7 15'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:menu){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 12h16M4 6h16M4 18h16'/%3E%3C/svg%3E")}:where(.i-lucide\:moon){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M12 3a6 6 0 0 0 9 9a9 9 0 1 1-9-9'/%3E%3C/svg%3E")}:where(.i-lucide\:paintbrush){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m14.622 17.897l-10.68-2.913M18.376 2.622a1 1 0 1 1 3.002 3.002L17.36 9.643a.5.5 0 0 0 0 .707l.944.944a2.41 2.41 0 0 1 0 3.408l-.944.944a.5.5 0 0 1-.707 0L8.354 7.348a.5.5 0 0 1 0-.707l.944-.944a2.41 2.41 0 0 1 3.408 0l.944.944a.5.5 0 0 0 .707 0zM9 8c-1.804 2.71-3.97 3.46-6.583 3.948a.507.507 0 0 0-.302.819l7.32 8.883a1 1 0 0 0 1.185.204C12.735 20.405 16 16.792 16 15'/%3E%3C/svg%3E")}:where(.i-lucide\:sun){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Ccircle cx='12' cy='12' r='4'/%3E%3Cpath d='M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-material-symbols\:water-drop-outline){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='black' d='M12.275 19q.3-.025.513-.238T13 18.25q0-.35-.225-.562T12.2 17.5q-1.025.075-2.175-.562t-1.45-2.313q-.05-.275-.262-.45T7.825 14q-.35 0-.575.263t-.15.612q.425 2.275 2 3.25t3.175.875M12 22q-3.425 0-5.712-2.35T4 13.8q0-2.5 1.988-5.437T12 2q4.025 3.425 6.013 6.363T20 13.8q0 3.5-2.287 5.85T12 22m0-2q2.6 0 4.3-1.763T18 13.8q0-1.825-1.513-4.125T12 4.65Q9.025 7.375 7.513 9.675T6 13.8q0 2.675 1.7 4.438T12 20m0-8'/%3E%3C/svg%3E")}</style>
<link rel="preload" as="fetch" crossorigin="anonymous" href="/model-dataset-comp/robotics-datasets/_payload.json?c16d95b9-45cb-4472-b6c7-54ac3c4ea957">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/VZYpm_JH.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/D5GhikOu.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BAQlc2qz.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BNFp-_v-.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BNYc9HVe.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CnltZOQb.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/NL1Ok194.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDaElFmS.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C9Nc3n_L.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/skYhgTkM.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CHtvKSho.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BE1tQNpv.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/tjc8PXlU.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CbKfh25p.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CEjkPGCe.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BHwK5op5.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BD0mnJfC.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/B1xPcWv_.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DTlbrafY.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C_-EWMcg.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DXs5LbVo.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/-R7oBYrK.js">
<link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/c16d95b9-45cb-4472-b6c7-54ac3c4ea957.json">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/OyymHwKz.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/CAX69q1B.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/bNaE6FFb.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/CDwhDq_j.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Ciz-CQHx.js">
<meta property="og:image" content="https://cybernachos.github.io/__og-image__/static/model-dataset-comp/robotics-datasets/og.png">
<meta property="og:image:type" content="image/png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cybernachos.github.io/__og-image__/static/model-dataset-comp/robotics-datasets/og.png">
<meta name="twitter:image:src" content="https://cybernachos.github.io/__og-image__/static/model-dataset-comp/robotics-datasets/og.png">
<meta property="og:image:width" content="1200">
<meta name="twitter:image:width" content="1200">
<meta property="og:image:height" content="600">
<meta name="twitter:image:height" content="600">
<meta name="description" content="A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, Dobb·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.">
<meta property="og:description" content="A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, Dobb·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.">
<meta property="og:title" content="Robotics Dataset Comparison">
<script type="module" src="/_nuxt/VZYpm_JH.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body  class="theme-zinc" style="--radius:0.5rem"><div id="__nuxt"><!--[--><div class="nuxt-loading-indicator z-100 bg-primary/80" style="position:fixed;top:0;right:0;left:0;pointer-events:none;width:auto;height:3px;opacity:0;background-size:Infinity% auto;transform:scaleX(0%);transform-origin:left;transition:transform 0.1s, height 0.4s, opacity 0.4s;z-index:999999;"></div><!----><header class="sticky top-0 z-40 bg-background/80 backdrop-blur-lg"><div class="container max-w-screen-2xl flex h-14 items-center justify-between gap-2 px-4 md:px-8"><div class="hidden flex-1 md:flex"><a href="/" class="flex"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" data-nuxt-img srcset="/_ipx/_/transparent-logo.svg 1x, /_ipx/_/transparent-logo.svg 2x" class="h-7 dark:hidden" src="/_ipx/_/transparent-logo.svg"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" data-nuxt-img srcset="/_ipx/_/transparent-logo.svg 1x, /_ipx/_/transparent-logo.svg 2x" class="hidden h-7 dark:block" src="/_ipx/_/transparent-logo.svg"><span class="ml-3 self-center font-bold">Cyber Nachos</span></a></div><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 md:hidden" type="button" aria-haspopup="dialog" aria-expanded="false" data-state="closed"><!--[--><span class="iconify i-lucide:menu" aria-hidden="true" style="font-size:18px;"></span><!--]--></button><!----><!--]--><!--]--><!----><nav aria-label="Main" data-orientation="horizontal" dir="ltr" data-radix-navigation-menu class="relative z-10 max-w-max items-center justify-center hidden flex-1 lg:flex"><!--[--><!--[--><div style="position:relative;"><ul class="group flex flex-1 list-none items-center justify-center gap-x-1" data-orientation="horizontal"><!--[--><!--]--></ul></div><!--]--><div class="absolute left-0 top-full flex justify-center"><!----></div><!--]--></nav><div class="flex flex-1 justify-end gap-2"><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap text-sm ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent px-4 py-2 h-8 w-full self-center rounded-md pr-1.5 font-normal text-muted-foreground hover:text-accent-foreground md:w-40 lg:w-64"><!--[--><span class="mr-auto overflow-hidden">Search...</span><kbd class="pointer-events-none inline-flex h-5 select-none items-center gap-1 rounded border border-border bg-muted font-sans font-medium min-h-5 text-[11px] h-5 px-1 ml-auto hidden md:block"><!--[--><span class="text-xs">⌘</span>K <!--]--></kbd><!--]--></button><!--]--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><div class="flex"><!----><!--[--><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls data-state="closed"><!--[--><span class="iconify i-lucide:paintbrush" aria-hidden="true" style="font-size:16px;"></span><!--]--></button><!----><!--]--><!--]--><!--]--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10"><!--[--><span class="iconify i-lucide:sun rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0" aria-hidden="true" style="font-size:18px;"></span><span class="iconify i-lucide:moon absolute rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100" aria-hidden="true" style="font-size:18px;"></span><span class="sr-only">Toggle theme</span><!--]--></button><!--[--><a href="https://github.com/CyberNachos" rel="noopener noreferrer" target="_blank"><button class="items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 flex gap-2"><!--[--><span class="iconify i-lucide:github" aria-hidden="true" style="font-size:18px;"></span><!--]--></button></a><!--]--></div></div></div><!----></header><div class="min-h-screen border-b"><div class="container md:grid-cols-[220px_minmax(0,1fr)] lg:grid-cols-[240px_minmax(0,1fr)] flex-1 items-start px-4 md:grid md:gap-6 md:px-8 lg:gap-10"><aside class="fixed top-[102px] z-30 -ml-2 hidden h-[calc(100vh-3.5rem)] w-full shrink-0 overflow-y-auto md:sticky md:top-[60px] md:block"><div dir="ltr" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px;" class="relative h-full overflow-hidden py-6 pr-6 text-sm md:pr-4" orientation="vertical"><!--[--><!--[--><div data-radix-scroll-area-viewport style="overflow-x:hidden;overflow-y:hidden;" class="size-full rounded-[inherit]" tabindex="0"><div style=""><!--[--><!--[--><!----><!----><!----><ul class="flex flex-col gap-1 py-1 pt-1"><!--[--><li><a href="/cybernachos" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:hand min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Cyber Nachos</span><!--[--><!--]--><!--]--></a></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Published Products</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/published/cybernachos-gpt" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Cyber Nachos GPT</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Tutorial Isaac Lab</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/tutorial-isaaclab/introduction" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Introduction</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/installation" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:download min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Installation Guide</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/getting-started" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:flag min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Getting Started</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/enable-fluid" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-material-symbols:water-drop-outline min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Enabling fluid simulation</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Published Research Papers</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/published-research/real-time-dexterous" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Real-time Dexterous Telemanipulation</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/hallucination" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Hallucination Prevention in LLMs</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Model &amp; Dataset Comparisons</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a aria-current="page" href="/model-dataset-comp/robotics-datasets" class="router-link-active router-link-exact-active flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary bg-muted !text-primary h-8"><!--[--><span class="iconify i-lucide:database min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Robotics Dataset Comparison</span><!--[--><!--]--><!--]--></a></li><li><a href="/model-dataset-comp/robotics-models" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">General-Purpose Robot Models Analysis</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><!--]--></ul><!--]--><!--]--></div></div><style> /* Hide scrollbars cross-browser and enable momentum scroll for touch devices */ [data-radix-scroll-area-viewport] { scrollbar-width:none; -ms-overflow-style:none; -webkit-overflow-scrolling:touch; } [data-radix-scroll-area-viewport]::-webkit-scrollbar { display:none; } </style><!--]--><!----><!----><!--]--></div></aside><!--[--><main class="lg:grid lg:grid-cols-[1fr_220px] lg:gap-14 lg:py-8 relative py-6"><div class="mx-auto w-full min-w-0"><!----><div class="mb-6"><h1 class="scroll-m-20 text-4xl font-extrabold tracking-tight lg:text-5xl"><!--[-->Robotics Dataset Comparison<!--]--></h1><p class="pt-1 text-lg text-muted-foreground"></p><!----><!----></div><div class="docs-content"><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[-->Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table (similar in spirit to Table 1 of the referenced paper) that highlights key characteristics, and second, detailed descriptions of each dataset’s scope, technical features, advantages, and disadvantages.<!--]--></p><h2 id="summary-table" class="scroll-m-20 border-b pb-2 text-3xl font-semibold tracking-tight transition-colors [&amp;:not(:first-child)]:mt-10"><a href="#summary-table"><!--[-->Summary Table<!--]--></a></h2><div class="w-full overflow-y-auto [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6"><table class="w-full"><!--[--><thead><!--[--><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Dataset / Framework<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Scale &amp; Modalities<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Key Advantages<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Key Disadvantages<!--]--></strong><!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->LeRobot<!--]--></strong> <br> (<a href="https://github.com/huggingface/lerobot" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->GitHub<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->End-to-end learning with community support; integrated simulation environments.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Complex setup; may require substantial computing and sensor calibration.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Open X-Embodiment<!--]--></strong> <br> (<a href="https://robotics-transformer-x.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Massive diversity enabling cross-robot transfer and positive knowledge sharing.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Heterogeneous quality and potential standardization issues across varied sources.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->DROID<!--]--></strong> <br> (<a href="https://droid-dataset.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->In-the-wild robot manipulation for robust imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Diverse, large-scale manipulation data that improves policy robustness.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Mostly limited to manipulation with a specific hardware setup; less diversity in task types.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->RoboTurk<!--]--></strong> <br> (<a href="https://roboturk.stanford.edu/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Leverages non-expert, scalable human demonstrations; supports collaborative tasks.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Variation in demonstration quality and potential limits in scale compared to fully automated data collection.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->MIME<!--]--></strong> <br> (<a href="https://sites.google.com/view/mimedataset/dataset?authuser=0" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Google Sites<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Imitation learning for robot manipulation using human demonstrations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Focus on high-quality manipulation trajectories; well-suited for imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->May be smaller in scale and less diverse than some large-scale multi-robot datasets.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Meta-World<!--]--></strong> <br> (<a href="https://meta-world.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Benchmark for multi-task and meta-reinforcement learning in simulation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->50 distinct simulated manipulation environments; task variations with visual observations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Standardized benchmark for meta-RL; structured for evaluating generalization.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Limited to simulation and may not capture the full variability of real-world settings.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->RoboNet<!--]--></strong> <br> (<a href="https://www.robonet.wiki/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Open database of real robotic experience for manipulation tasks across multiple platforms.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->~15M video frames, collected from 7 robot platforms with diverse camera viewpoints.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale, multi-platform real-world data that facilitates cross-robot generalization.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very high storage and processing requirements; complex data integration.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->RoboSet<!--]--></strong> <br> (<a href="https://robopen.github.io/roboset/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-task dataset for household (kitchen) manipulation tasks, including language instructions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Rich, multi-modal data in realistic home environments; supports language-guided sequencing.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Domain-specific (largely kitchens); may not generalize to non-domestic scenarios.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->BridgeData V2<!--]--></strong> <br> (<a href="https://rail-berkeley.github.io/bridgedata/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale robotic manipulation across diverse environments and skills with language annotations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->RT-1<!--]--></strong> <br> (<a href="https://robotics-transformer1.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Real-world imitation learning for multi-task manipulation using transformer architectures.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Outstanding generalization and performance on diverse tasks; scalable transformer model.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->High training and computational requirements; system complexity may be a barrier.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Dobb·E<!--]--></strong> <br> (<a href="https://dobb-e.com/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Framework for home robotics: learning household manipulation tasks quickly in real homes.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->“HoNY” dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30 fps; also includes hardware (the “Stick”) for data collection.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Cost-effective, rapid task learning with real household data; designed for generalist home robots.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->RH20T<!--]--></strong> <br> (<a href="https://rh20t.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very large and complex; requires significant computational and storage resources; complex data processing pipeline.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->BC-Z<!--]--></strong> <br> (<a href="https://sites.google.com/view/bc-z/home?pli=1" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale behavior cloning for robotic manipulation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->MT-Opt<!--]--></strong> <br> (<a href="https://karolhausman.github.io/mt-opt/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-task reinforcement learning at scale across many manipulation skills.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->VIMA<!--]--></strong> <br> (<a href="https://vimalabs.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->General robot manipulation via multimodal prompts (combining language and vision) for unified task specification.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Unified formulation that “prompts” the robot to perform diverse tasks; highly scalable and sample-efficient.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->SPOC<!--]--></strong> <br> (<a href="https://spoc-robot.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world).<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios.<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table></div><hr class="[&amp;:not(:first-child)]:mt-6"><h2 id="detailed-comparison" class="scroll-m-20 border-b pb-2 text-3xl font-semibold tracking-tight transition-colors [&amp;:not(:first-child)]:mt-10"><a href="#detailed-comparison"><!--[-->Detailed Comparison<!--]--></a></h2><h3 id="_1-lerobot" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_1-lerobot"><!--[-->1. LeRobot<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
LeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Built in PyTorch with modular dataset classes that support multi-frame temporal sampling.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Community-driven with active contributions and hosted on Hugging Face.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Facilitates rapid prototyping in robotics with an accessible codebase.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_2-open-x-embodiment" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_2-open-x-embodiment"><!--[-->2. Open X-Embodiment<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A collaborative effort pooling robot data from 21 institutions, it is aimed at training “generalist” policies across 22 different robot embodiments.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Aggregates 1M+ trajectories from diverse robots and tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Supports learning via transformer-based architectures that can generalize across different embodiments.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Unmatched diversity, which is ideal for studying cross-robot transfer.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Large scale increases the potential for generalization.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_3-droid" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_3-droid"><!--[-->3. DROID<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Focused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Multi-camera views (including wrist and exterior images) enable rich visual inputs.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Large, diverse dataset that significantly boosts policy performance and robustness.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Extensive coverage of real-world scenarios.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Being collected with a specific hardware platform, its applicability to other robots may be limited.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_4-roboturk" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_4-roboturk"><!--[-->4. RoboTurk<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset).<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes system features for low-latency teleoperation and human-in-the-loop interventions.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Proven effectiveness in enabling imitation learning on challenging tasks.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->The quality of demonstrations may vary due to differences in human teleoperation skills.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_5-mime" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_5-mime"><!--[-->5. MIME<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
MIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Focused on detailed manipulation tasks, making it ideal for imitation learning studies.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Generally smaller in scale compared to some of the largest datasets; might offer limited diversity.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_6-meta-world" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_6-meta-world"><!--[-->6. Meta-World<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Structured environments with varying goal positions and task variations to test generalization.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_7-robonet" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_7-robonet"><!--[-->7. RoboNet<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains over 15M video frames and data from multiple camera viewpoints.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Offers vast amounts of real-world data to study generalization across different robot hardware.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Requires heavy storage and processing; integrating multi-platform data can be challenging.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_8-roboset" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_8-roboset"><!--[-->8. RoboSet<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Rich multi-modal information (visual + language) supports language-guided robotic learning.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_9-bridgedata-v2" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_9-bridgedata-v2"><!--[-->9. BridgeData V2<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Designed to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Approximately 60K trajectories with multi-view RGB (and some depth) data.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes both teleoperated and scripted demonstrations.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->High diversity in environments and tasks; strong support for language-conditioned policy learning.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_10-rt-1" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_10-rt-1"><!--[-->10. RT-1<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Over 130K episodes covering more than 700 tasks collected from 13 robots.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Utilizes vision and natural language inputs to produce discretized action tokens.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates superior performance and generalization, including sim-to-real transfer.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Scalability through high-capacity transformer models.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demands extensive data, compute, and engineering expertise; system complexity is high.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_11-dobbe" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_11-dobbe"><!--[-->11. Dobb·E<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Dobb·E focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->“HoNY” dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30 fps).<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes a low-cost hardware “Stick” for demonstration collection.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Cost-effective and designed for rapid task learning in domestic environments.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates strong real-world applicability in home settings.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_12-rh20t" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_12-rh20t"><!--[-->12. RH20T<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Detailed synchronization and calibration across multiple sensors.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Supports research on contact-rich and dexterous manipulation.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_13-bc-z" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_13-bc-z"><!--[-->13. BC-Z<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
BC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Serves as a standardized resource for evaluating behavior cloning algorithms.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_14-mt-opt" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_14-mt-opt"><!--[-->14. MT-Opt<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
MT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Effective at sharing experience across tasks, significantly boosting performance on rare tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates both zero-shot and rapid fine-tuning capabilities.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_15-vima" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_15-vima"><!--[-->15. VIMA<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
VIMA presents a novel formulation in which diverse robot manipulation tasks are “prompted” via interleaved language and visual tokens, unifying task specification.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Allows integration of various forms of task instructions (text + image).<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_16-spoc" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_16-spoc"><!--[-->16. SPOC<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
SPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Trains entirely in simulation and transfers effectively.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"></div><div class="mt-16"><div class="mb-6 flex w-full items-center justify-between"><!----><div class="w-fit"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 underline-offset-4 hover:underline h-10 px-4 py-2 text-sm font-semibold text-primary"><!--[--><div class="flex items-center gap-2"><span class="iconify i-lucide:arrow-up" aria-hidden="true" style="font-size:16px;"></span><span>Back to Top</span></div><!--]--></button></div></div><div class="border-t pt-6 lg:flex lg:flex-row"><a href="/published-research/hallucination" class="basis-1/3"><div class="mb-4 space-y-2 rounded-lg border p-4 transition-all hover:bg-muted/50"><div class="flex flex-row gap-3"><div class="flex size-6 min-w-6"><span class="iconify i-lucide:arrow-left mx-auto self-center" aria-hidden="true" style="font-size:20px;"></span></div><span class="space-y-2 self-center"><div class="text-lg font-semibold">Hallucination Prevention in LLMs</div><div class="text-sm text-muted-foreground">Zero-Resource Hallucination Prevention for Large Language Models</div></span><!----></div></div></a><span class="flex-1"></span><a href="/model-dataset-comp/robotics-models" class="basis-1/3"><div class="mb-4 space-y-2 rounded-lg border p-4 transition-all hover:bg-muted/50"><div class="flex flex-row gap-3"><!----><span class="space-y-2 self-center"><div class="text-lg font-semibold">General-Purpose Robot Models Analysis</div><div class="text-sm text-muted-foreground">Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.</div></span><div class="ml-auto flex size-6 min-w-6"><span class="iconify i-lucide:arrow-right mx-auto self-center" aria-hidden="true" style="font-size:20px;"></span></div></div></div></a></div><div class="flex"><!----></div></div></div><div class="hidden text-sm lg:block"><div class="sticky top-[90px] h-[calc(100vh-3.5rem)] overflow-hidden"><div dir="ltr" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px;" class="relative overflow-hidden z-30 hidden h-[calc(100vh-6.5rem)] overflow-y-auto md:block lg:block" orientation="vertical"><!--[--><!--[--><div data-radix-scroll-area-viewport style="overflow-x:hidden;overflow-y:hidden;" class="size-full rounded-[inherit]" tabindex="0"><div style=""><!--[--><!--[--><div class="flex h-[calc(100vh-6.5rem)] flex-col"><div><p class="mb-2 text-base font-semibold">On This Page</p><ul class=""><!--[--><li class="pt-2"><a href="#summary-table" class="text-muted-foreground transition-all hover:text-primary">Summary Table</a><!----></li><li class="pt-2"><a href="#detailed-comparison" class="text-muted-foreground transition-all hover:text-primary">Detailed Comparison</a><ul class="pl-4"><!--[--><li class="pt-2"><a href="#_1-lerobot" class="text-muted-foreground transition-all hover:text-primary">1. LeRobot</a><!----></li><li class="pt-2"><a href="#_2-open-x-embodiment" class="text-muted-foreground transition-all hover:text-primary">2. Open X-Embodiment</a><!----></li><li class="pt-2"><a href="#_3-droid" class="text-muted-foreground transition-all hover:text-primary">3. DROID</a><!----></li><li class="pt-2"><a href="#_4-roboturk" class="text-muted-foreground transition-all hover:text-primary">4. RoboTurk</a><!----></li><li class="pt-2"><a href="#_5-mime" class="text-muted-foreground transition-all hover:text-primary">5. MIME</a><!----></li><li class="pt-2"><a href="#_6-meta-world" class="text-muted-foreground transition-all hover:text-primary">6. Meta-World</a><!----></li><li class="pt-2"><a href="#_7-robonet" class="text-muted-foreground transition-all hover:text-primary">7. RoboNet</a><!----></li><li class="pt-2"><a href="#_8-roboset" class="text-muted-foreground transition-all hover:text-primary">8. RoboSet</a><!----></li><li class="pt-2"><a href="#_9-bridgedata-v2" class="text-muted-foreground transition-all hover:text-primary">9. BridgeData V2</a><!----></li><li class="pt-2"><a href="#_10-rt-1" class="text-muted-foreground transition-all hover:text-primary">10. RT-1</a><!----></li><li class="pt-2"><a href="#_11-dobbe" class="text-muted-foreground transition-all hover:text-primary">11. Dobb·E</a><!----></li><li class="pt-2"><a href="#_12-rh20t" class="text-muted-foreground transition-all hover:text-primary">12. RH20T</a><!----></li><li class="pt-2"><a href="#_13-bc-z" class="text-muted-foreground transition-all hover:text-primary">13. BC-Z</a><!----></li><li class="pt-2"><a href="#_14-mt-opt" class="text-muted-foreground transition-all hover:text-primary">14. MT-Opt</a><!----></li><li class="pt-2"><a href="#_15-vima" class="text-muted-foreground transition-all hover:text-primary">15. VIMA</a><!----></li><li class="pt-2"><a href="#_16-spoc" class="text-muted-foreground transition-all hover:text-primary">16. SPOC</a><!----></li><!--]--></ul></li><!--]--></ul><div class="pt-5 text-muted-foreground"><!--[--><!--]--></div></div><div class="flex-grow"></div><!----></div><!--]--><!--]--></div></div><style> /* Hide scrollbars cross-browser and enable momentum scroll for touch devices */ [data-radix-scroll-area-viewport] { scrollbar-width:none; -ms-overflow-style:none; -webkit-overflow-scrolling:touch; } [data-radix-scroll-area-viewport]::-webkit-scrollbar { display:none; } </style><!--]--><!----><!----><!--]--></div></div></div></main><!--]--></div></div><!--[--><!--[--><!--[--><!--]--><div role="region" aria-label="Notifications (F8)" tabindex="-1" style="pointer-events:none;"><!--[--><!----><ol tabindex="-1" class="fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]"><!--[--><!--]--></ol><!----><!--]--></div><!--]--><!--]--><footer class="py-6 text-muted-foreground md:px-8 md:py-0"><div class="container flex flex-col items-center justify-between gap-2 md:h-24 md:flex-row"><!--[--><div class="text-sm"><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[-->Copyright © 2025<!--]--></p></div><!--]--><span class="flex-1"></span><!--[--><a href="https://github.com/CyberNachos" rel="noopener noreferrer" target="_blank"><button class="items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 flex gap-2"><!--[--><span class="iconify i-lucide:github" aria-hidden="true" style="font-size:20px;"></span><!----><!--]--></button></a><!--]--></div></footer><!--]--></div><div id="teleports"></div>
<script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/model-dataset-comp/robotics-datasets/_payload.json?c16d95b9-45cb-4472-b6c7-54ac3c4ea957">[{"state":1,"once":2066,"_errors":2067,"serverRendered":5,"path":11,"prerenderedAt":2070},["Reactive",2],{"$scolor-mode":3,"$sdd-pages":7,"$sdd-surrounds":1989,"$sdd-globals":2010,"$sdd-navigation":2012,"$sdocs-collapsed-map":2057,"$ssite-config":2058},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,["ShallowRef",8],["ShallowReactive",9],{"/model-dataset-comp/robotics-datasets":10},{"_path":11,"_dir":12,"_draft":6,"_partial":6,"_locale":13,"title":14,"description":15,"icon":16,"body":17,"_type":1982,"_id":1983,"_source":1984,"_file":1985,"_stem":1986,"_extension":1987,"layout":1988},"/model-dataset-comp/robotics-datasets","model-dataset-comp","","Robotics Dataset Comparison","A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, Dobb·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.","lucide:database",{"type":18,"children":19,"toc":1959},"root",[20,28,35,791,795,801,808,821,829,844,852,865,873,881,884,890,902,909,922,929,942,949,957,960,966,978,985,998,1005,1018,1025,1033,1036,1042,1054,1061,1074,1081,1094,1101,1109,1112,1118,1130,1137,1145,1152,1160,1167,1175,1178,1184,1196,1203,1211,1218,1226,1233,1241,1244,1250,1262,1269,1277,1284,1292,1299,1307,1310,1316,1328,1335,1343,1350,1358,1365,1373,1376,1382,1394,1401,1414,1421,1429,1436,1444,1447,1453,1465,1472,1485,1492,1505,1512,1520,1523,1529,1541,1548,1561,1568,1581,1588,1596,1599,1605,1617,1624,1637,1644,1657,1664,1672,1675,1681,1693,1700,1708,1715,1723,1730,1738,1741,1747,1759,1766,1774,1781,1794,1801,1809,1812,1818,1830,1837,1845,1852,1865,1872,1880,1883,1889,1901,1908,1921,1928,1941,1948,1956],{"type":21,"tag":22,"props":23,"children":24},"element","p",{},[25],{"type":26,"value":27},"text","Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table (similar in spirit to Table 1 of the referenced paper) that highlights key characteristics, and second, detailed descriptions of each dataset’s scope, technical features, advantages, and disadvantages.",{"type":21,"tag":29,"props":30,"children":32},"h2",{"id":31},"summary-table",[33],{"type":26,"value":34},"Summary Table",{"type":21,"tag":36,"props":37,"children":38},"table",{},[39,89],{"type":21,"tag":40,"props":41,"children":42},"thead",{},[43],{"type":21,"tag":44,"props":45,"children":46},"tr",{},[47,57,65,73,81],{"type":21,"tag":48,"props":49,"children":50},"th",{},[51],{"type":21,"tag":52,"props":53,"children":54},"strong",{},[55],{"type":26,"value":56},"Dataset / Framework",{"type":21,"tag":48,"props":58,"children":59},{},[60],{"type":21,"tag":52,"props":61,"children":62},{},[63],{"type":26,"value":64},"Scope & Application",{"type":21,"tag":48,"props":66,"children":67},{},[68],{"type":21,"tag":52,"props":69,"children":70},{},[71],{"type":26,"value":72},"Scale & Modalities",{"type":21,"tag":48,"props":74,"children":75},{},[76],{"type":21,"tag":52,"props":77,"children":78},{},[79],{"type":26,"value":80},"Key Advantages",{"type":21,"tag":48,"props":82,"children":83},{},[84],{"type":21,"tag":52,"props":85,"children":86},{},[87],{"type":26,"value":88},"Key Disadvantages",{"type":21,"tag":90,"props":91,"children":92},"tbody",{},[93,144,188,231,274,318,361,404,447,490,533,576,619,662,705,748],{"type":21,"tag":44,"props":94,"children":95},{},[96,124,129,134,139],{"type":21,"tag":97,"props":98,"children":99},"td",{},[100,105,107,111,113,122],{"type":21,"tag":52,"props":101,"children":102},{},[103],{"type":26,"value":104},"LeRobot",{"type":26,"value":106}," ",{"type":21,"tag":108,"props":109,"children":110},"br",{},[],{"type":26,"value":112}," (",{"type":21,"tag":114,"props":115,"children":119},"a",{"href":116,"rel":117},"https://github.com/huggingface/lerobot",[118],"nofollow",[120],{"type":26,"value":121},"GitHub",{"type":26,"value":123},")",{"type":21,"tag":97,"props":125,"children":126},{},[127],{"type":26,"value":128},"Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots.",{"type":21,"tag":97,"props":130,"children":131},{},[132],{"type":26,"value":133},"Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context.",{"type":21,"tag":97,"props":135,"children":136},{},[137],{"type":26,"value":138},"End-to-end learning with community support; integrated simulation environments.",{"type":21,"tag":97,"props":140,"children":141},{},[142],{"type":26,"value":143},"Complex setup; may require substantial computing and sensor calibration.",{"type":21,"tag":44,"props":145,"children":146},{},[147,168,173,178,183],{"type":21,"tag":97,"props":148,"children":149},{},[150,155,156,159,160,167],{"type":21,"tag":52,"props":151,"children":152},{},[153],{"type":26,"value":154},"Open X-Embodiment",{"type":26,"value":106},{"type":21,"tag":108,"props":157,"children":158},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":161,"children":164},{"href":162,"rel":163},"https://robotics-transformer-x.github.io/",[118],[165],{"type":26,"value":166},"Website",{"type":26,"value":123},{"type":21,"tag":97,"props":169,"children":170},{},[171],{"type":26,"value":172},"Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions.",{"type":21,"tag":97,"props":174,"children":175},{},[176],{"type":26,"value":177},"1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data.",{"type":21,"tag":97,"props":179,"children":180},{},[181],{"type":26,"value":182},"Massive diversity enabling cross-robot transfer and positive knowledge sharing.",{"type":21,"tag":97,"props":184,"children":185},{},[186],{"type":26,"value":187},"Heterogeneous quality and potential standardization issues across varied sources.",{"type":21,"tag":44,"props":189,"children":190},{},[191,211,216,221,226],{"type":21,"tag":97,"props":192,"children":193},{},[194,199,200,203,204,210],{"type":21,"tag":52,"props":195,"children":196},{},[197],{"type":26,"value":198},"DROID",{"type":26,"value":106},{"type":21,"tag":108,"props":201,"children":202},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":205,"children":208},{"href":206,"rel":207},"https://droid-dataset.github.io/",[118],[209],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":212,"children":213},{},[214],{"type":26,"value":215},"In-the-wild robot manipulation for robust imitation learning.",{"type":21,"tag":97,"props":217,"children":218},{},[219],{"type":26,"value":220},"76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints.",{"type":21,"tag":97,"props":222,"children":223},{},[224],{"type":26,"value":225},"Diverse, large-scale manipulation data that improves policy robustness.",{"type":21,"tag":97,"props":227,"children":228},{},[229],{"type":26,"value":230},"Mostly limited to manipulation with a specific hardware setup; less diversity in task types.",{"type":21,"tag":44,"props":232,"children":233},{},[234,254,259,264,269],{"type":21,"tag":97,"props":235,"children":236},{},[237,242,243,246,247,253],{"type":21,"tag":52,"props":238,"children":239},{},[240],{"type":26,"value":241},"RoboTurk",{"type":26,"value":106},{"type":21,"tag":108,"props":244,"children":245},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":248,"children":251},{"href":249,"rel":250},"https://roboturk.stanford.edu/",[118],[252],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":255,"children":256},{},[257],{"type":26,"value":258},"Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection.",{"type":21,"tag":97,"props":260,"children":261},{},[262],{"type":26,"value":263},"Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions.",{"type":21,"tag":97,"props":265,"children":266},{},[267],{"type":26,"value":268},"Leverages non-expert, scalable human demonstrations; supports collaborative tasks.",{"type":21,"tag":97,"props":270,"children":271},{},[272],{"type":26,"value":273},"Variation in demonstration quality and potential limits in scale compared to fully automated data collection.",{"type":21,"tag":44,"props":275,"children":276},{},[277,298,303,308,313],{"type":21,"tag":97,"props":278,"children":279},{},[280,285,286,289,290,297],{"type":21,"tag":52,"props":281,"children":282},{},[283],{"type":26,"value":284},"MIME",{"type":26,"value":106},{"type":21,"tag":108,"props":287,"children":288},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":291,"children":294},{"href":292,"rel":293},"https://sites.google.com/view/mimedataset/dataset?authuser=0",[118],[295],{"type":26,"value":296},"Google Sites",{"type":26,"value":123},{"type":21,"tag":97,"props":299,"children":300},{},[301],{"type":26,"value":302},"Imitation learning for robot manipulation using human demonstrations.",{"type":21,"tag":97,"props":304,"children":305},{},[306],{"type":26,"value":307},"Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories.",{"type":21,"tag":97,"props":309,"children":310},{},[311],{"type":26,"value":312},"Focus on high-quality manipulation trajectories; well-suited for imitation learning.",{"type":21,"tag":97,"props":314,"children":315},{},[316],{"type":26,"value":317},"May be smaller in scale and less diverse than some large-scale multi-robot datasets.",{"type":21,"tag":44,"props":319,"children":320},{},[321,341,346,351,356],{"type":21,"tag":97,"props":322,"children":323},{},[324,329,330,333,334,340],{"type":21,"tag":52,"props":325,"children":326},{},[327],{"type":26,"value":328},"Meta-World",{"type":26,"value":106},{"type":21,"tag":108,"props":331,"children":332},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":335,"children":338},{"href":336,"rel":337},"https://meta-world.github.io/",[118],[339],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":342,"children":343},{},[344],{"type":26,"value":345},"Benchmark for multi-task and meta-reinforcement learning in simulation.",{"type":21,"tag":97,"props":347,"children":348},{},[349],{"type":26,"value":350},"50 distinct simulated manipulation environments; task variations with visual observations.",{"type":21,"tag":97,"props":352,"children":353},{},[354],{"type":26,"value":355},"Standardized benchmark for meta-RL; structured for evaluating generalization.",{"type":21,"tag":97,"props":357,"children":358},{},[359],{"type":26,"value":360},"Limited to simulation and may not capture the full variability of real-world settings.",{"type":21,"tag":44,"props":362,"children":363},{},[364,384,389,394,399],{"type":21,"tag":97,"props":365,"children":366},{},[367,372,373,376,377,383],{"type":21,"tag":52,"props":368,"children":369},{},[370],{"type":26,"value":371},"RoboNet",{"type":26,"value":106},{"type":21,"tag":108,"props":374,"children":375},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":378,"children":381},{"href":379,"rel":380},"https://www.robonet.wiki/",[118],[382],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":385,"children":386},{},[387],{"type":26,"value":388},"Open database of real robotic experience for manipulation tasks across multiple platforms.",{"type":21,"tag":97,"props":390,"children":391},{},[392],{"type":26,"value":393},"~15M video frames, collected from 7 robot platforms with diverse camera viewpoints.",{"type":21,"tag":97,"props":395,"children":396},{},[397],{"type":26,"value":398},"Large-scale, multi-platform real-world data that facilitates cross-robot generalization.",{"type":21,"tag":97,"props":400,"children":401},{},[402],{"type":26,"value":403},"Very high storage and processing requirements; complex data integration.",{"type":21,"tag":44,"props":405,"children":406},{},[407,427,432,437,442],{"type":21,"tag":97,"props":408,"children":409},{},[410,415,416,419,420,426],{"type":21,"tag":52,"props":411,"children":412},{},[413],{"type":26,"value":414},"RoboSet",{"type":26,"value":106},{"type":21,"tag":108,"props":417,"children":418},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":421,"children":424},{"href":422,"rel":423},"https://robopen.github.io/roboset/",[118],[425],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":428,"children":429},{},[430],{"type":26,"value":431},"Multi-task dataset for household (kitchen) manipulation tasks, including language instructions.",{"type":21,"tag":97,"props":433,"children":434},{},[435],{"type":26,"value":436},"28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame.",{"type":21,"tag":97,"props":438,"children":439},{},[440],{"type":26,"value":441},"Rich, multi-modal data in realistic home environments; supports language-guided sequencing.",{"type":21,"tag":97,"props":443,"children":444},{},[445],{"type":26,"value":446},"Domain-specific (largely kitchens); may not generalize to non-domestic scenarios.",{"type":21,"tag":44,"props":448,"children":449},{},[450,470,475,480,485],{"type":21,"tag":97,"props":451,"children":452},{},[453,458,459,462,463,469],{"type":21,"tag":52,"props":454,"children":455},{},[456],{"type":26,"value":457},"BridgeData V2",{"type":26,"value":106},{"type":21,"tag":108,"props":460,"children":461},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":464,"children":467},{"href":465,"rel":466},"https://rail-berkeley.github.io/bridgedata/",[118],[468],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":471,"children":472},{},[473],{"type":26,"value":474},"Large-scale robotic manipulation across diverse environments and skills with language annotations.",{"type":21,"tag":97,"props":476,"children":477},{},[478],{"type":26,"value":479},"~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language.",{"type":21,"tag":97,"props":481,"children":482},{},[483],{"type":26,"value":484},"Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning.",{"type":21,"tag":97,"props":486,"children":487},{},[488],{"type":26,"value":489},"Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges.",{"type":21,"tag":44,"props":491,"children":492},{},[493,513,518,523,528],{"type":21,"tag":97,"props":494,"children":495},{},[496,501,502,505,506,512],{"type":21,"tag":52,"props":497,"children":498},{},[499],{"type":26,"value":500},"RT-1",{"type":26,"value":106},{"type":21,"tag":108,"props":503,"children":504},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":507,"children":510},{"href":508,"rel":509},"https://robotics-transformer1.github.io/",[118],[511],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":514,"children":515},{},[516],{"type":26,"value":517},"Real-world imitation learning for multi-task manipulation using transformer architectures.",{"type":21,"tag":97,"props":519,"children":520},{},[521],{"type":26,"value":522},"Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control.",{"type":21,"tag":97,"props":524,"children":525},{},[526],{"type":26,"value":527},"Outstanding generalization and performance on diverse tasks; scalable transformer model.",{"type":21,"tag":97,"props":529,"children":530},{},[531],{"type":26,"value":532},"High training and computational requirements; system complexity may be a barrier.",{"type":21,"tag":44,"props":534,"children":535},{},[536,556,561,566,571],{"type":21,"tag":97,"props":537,"children":538},{},[539,544,545,548,549,555],{"type":21,"tag":52,"props":540,"children":541},{},[542],{"type":26,"value":543},"Dobb·E",{"type":26,"value":106},{"type":21,"tag":108,"props":546,"children":547},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":550,"children":553},{"href":551,"rel":552},"https://dobb-e.com/",[118],[554],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":557,"children":558},{},[559],{"type":26,"value":560},"Framework for home robotics: learning household manipulation tasks quickly in real homes.",{"type":21,"tag":97,"props":562,"children":563},{},[564],{"type":26,"value":565},"“HoNY” dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30 fps; also includes hardware (the “Stick”) for data collection.",{"type":21,"tag":97,"props":567,"children":568},{},[569],{"type":26,"value":570},"Cost-effective, rapid task learning with real household data; designed for generalist home robots.",{"type":21,"tag":97,"props":572,"children":573},{},[574],{"type":26,"value":575},"Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations.",{"type":21,"tag":44,"props":577,"children":578},{},[579,599,604,609,614],{"type":21,"tag":97,"props":580,"children":581},{},[582,587,588,591,592,598],{"type":21,"tag":52,"props":583,"children":584},{},[585],{"type":26,"value":586},"RH20T",{"type":26,"value":106},{"type":21,"tag":108,"props":589,"children":590},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":593,"children":596},{"href":594,"rel":595},"https://rh20t.github.io/",[118],[597],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":600,"children":601},{},[602],{"type":26,"value":603},"Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world.",{"type":21,"tag":97,"props":605,"children":606},{},[607],{"type":26,"value":608},"Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data.",{"type":21,"tag":97,"props":610,"children":611},{},[612],{"type":26,"value":613},"Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning.",{"type":21,"tag":97,"props":615,"children":616},{},[617],{"type":26,"value":618},"Very large and complex; requires significant computational and storage resources; complex data processing pipeline.",{"type":21,"tag":44,"props":620,"children":621},{},[622,642,647,652,657],{"type":21,"tag":97,"props":623,"children":624},{},[625,630,631,634,635,641],{"type":21,"tag":52,"props":626,"children":627},{},[628],{"type":26,"value":629},"BC-Z",{"type":26,"value":106},{"type":21,"tag":108,"props":632,"children":633},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":636,"children":639},{"href":637,"rel":638},"https://sites.google.com/view/bc-z/home?pli=1",[118],[640],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":643,"children":644},{},[645],{"type":26,"value":646},"Large-scale behavior cloning for robotic manipulation.",{"type":21,"tag":97,"props":648,"children":649},{},[650],{"type":26,"value":651},"(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)",{"type":21,"tag":97,"props":653,"children":654},{},[655],{"type":26,"value":656},"Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms.",{"type":21,"tag":97,"props":658,"children":659},{},[660],{"type":26,"value":661},"May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets.",{"type":21,"tag":44,"props":663,"children":664},{},[665,685,690,695,700],{"type":21,"tag":97,"props":666,"children":667},{},[668,673,674,677,678,684],{"type":21,"tag":52,"props":669,"children":670},{},[671],{"type":26,"value":672},"MT-Opt",{"type":26,"value":106},{"type":21,"tag":108,"props":675,"children":676},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":679,"children":682},{"href":680,"rel":681},"https://karolhausman.github.io/mt-opt/",[118],[683],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":686,"children":687},{},[688],{"type":26,"value":689},"Multi-task reinforcement learning at scale across many manipulation skills.",{"type":21,"tag":97,"props":691,"children":692},{},[693],{"type":26,"value":694},"Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework.",{"type":21,"tag":97,"props":696,"children":697},{},[698],{"type":26,"value":699},"Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience.",{"type":21,"tag":97,"props":701,"children":702},{},[703],{"type":26,"value":704},"Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination.",{"type":21,"tag":44,"props":706,"children":707},{},[708,728,733,738,743],{"type":21,"tag":97,"props":709,"children":710},{},[711,716,717,720,721,727],{"type":21,"tag":52,"props":712,"children":713},{},[714],{"type":26,"value":715},"VIMA",{"type":26,"value":106},{"type":21,"tag":108,"props":718,"children":719},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":722,"children":725},{"href":723,"rel":724},"https://vimalabs.github.io/",[118],[726],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":729,"children":730},{},[731],{"type":26,"value":732},"General robot manipulation via multimodal prompts (combining language and vision) for unified task specification.",{"type":21,"tag":97,"props":734,"children":735},{},[736],{"type":26,"value":737},"Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models.",{"type":21,"tag":97,"props":739,"children":740},{},[741],{"type":26,"value":742},"Unified formulation that “prompts” the robot to perform diverse tasks; highly scalable and sample-efficient.",{"type":21,"tag":97,"props":744,"children":745},{},[746],{"type":26,"value":747},"Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation.",{"type":21,"tag":44,"props":749,"children":750},{},[751,771,776,781,786],{"type":21,"tag":97,"props":752,"children":753},{},[754,759,760,763,764,770],{"type":21,"tag":52,"props":755,"children":756},{},[757],{"type":26,"value":758},"SPOC",{"type":26,"value":106},{"type":21,"tag":108,"props":761,"children":762},{},[],{"type":26,"value":112},{"type":21,"tag":114,"props":765,"children":768},{"href":766,"rel":767},"https://spoc-robot.github.io/",[118],[769],{"type":26,"value":166},{"type":26,"value":123},{"type":21,"tag":97,"props":772,"children":773},{},[774],{"type":26,"value":775},"Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world).",{"type":21,"tag":97,"props":777,"children":778},{},[779],{"type":26,"value":780},"Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation.",{"type":21,"tag":97,"props":782,"children":783},{},[784],{"type":26,"value":785},"Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info.",{"type":21,"tag":97,"props":787,"children":788},{},[789],{"type":26,"value":790},"RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios.",{"type":21,"tag":792,"props":793,"children":794},"hr",{},[],{"type":21,"tag":29,"props":796,"children":798},{"id":797},"detailed-comparison",[799],{"type":26,"value":800},"Detailed Comparison",{"type":21,"tag":802,"props":803,"children":805},"h3",{"id":804},"_1-lerobot",[806],{"type":26,"value":807},"1. LeRobot",{"type":21,"tag":22,"props":809,"children":810},{},[811,816,819],{"type":21,"tag":52,"props":812,"children":813},{},[814],{"type":26,"value":815},"Scope & Application:",{"type":21,"tag":108,"props":817,"children":818},{},[],{"type":26,"value":820},"\nLeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots.",{"type":21,"tag":22,"props":822,"children":823},{},[824],{"type":21,"tag":52,"props":825,"children":826},{},[827],{"type":26,"value":828},"Technical Features:",{"type":21,"tag":830,"props":831,"children":832},"ul",{},[833,839],{"type":21,"tag":834,"props":835,"children":836},"li",{},[837],{"type":26,"value":838},"Built in PyTorch with modular dataset classes that support multi-frame temporal sampling.",{"type":21,"tag":834,"props":840,"children":841},{},[842],{"type":26,"value":843},"Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments.",{"type":21,"tag":22,"props":845,"children":846},{},[847],{"type":21,"tag":52,"props":848,"children":849},{},[850],{"type":26,"value":851},"Advantages:",{"type":21,"tag":830,"props":853,"children":854},{},[855,860],{"type":21,"tag":834,"props":856,"children":857},{},[858],{"type":26,"value":859},"Community-driven with active contributions and hosted on Hugging Face.",{"type":21,"tag":834,"props":861,"children":862},{},[863],{"type":26,"value":864},"Facilitates rapid prototyping in robotics with an accessible codebase.",{"type":21,"tag":22,"props":866,"children":867},{},[868],{"type":21,"tag":52,"props":869,"children":870},{},[871],{"type":26,"value":872},"Disadvantages:",{"type":21,"tag":830,"props":874,"children":875},{},[876],{"type":21,"tag":834,"props":877,"children":878},{},[879],{"type":26,"value":880},"Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise.",{"type":21,"tag":792,"props":882,"children":883},{},[],{"type":21,"tag":802,"props":885,"children":887},{"id":886},"_2-open-x-embodiment",[888],{"type":26,"value":889},"2. Open X-Embodiment",{"type":21,"tag":22,"props":891,"children":892},{},[893,897,900],{"type":21,"tag":52,"props":894,"children":895},{},[896],{"type":26,"value":815},{"type":21,"tag":108,"props":898,"children":899},{},[],{"type":26,"value":901},"\nA collaborative effort pooling robot data from 21 institutions, it is aimed at training “generalist” policies across 22 different robot embodiments.",{"type":21,"tag":22,"props":903,"children":904},{},[905],{"type":21,"tag":52,"props":906,"children":907},{},[908],{"type":26,"value":828},{"type":21,"tag":830,"props":910,"children":911},{},[912,917],{"type":21,"tag":834,"props":913,"children":914},{},[915],{"type":26,"value":916},"Aggregates 1M+ trajectories from diverse robots and tasks.",{"type":21,"tag":834,"props":918,"children":919},{},[920],{"type":26,"value":921},"Supports learning via transformer-based architectures that can generalize across different embodiments.",{"type":21,"tag":22,"props":923,"children":924},{},[925],{"type":21,"tag":52,"props":926,"children":927},{},[928],{"type":26,"value":851},{"type":21,"tag":830,"props":930,"children":931},{},[932,937],{"type":21,"tag":834,"props":933,"children":934},{},[935],{"type":26,"value":936},"Unmatched diversity, which is ideal for studying cross-robot transfer.",{"type":21,"tag":834,"props":938,"children":939},{},[940],{"type":26,"value":941},"Large scale increases the potential for generalization.",{"type":21,"tag":22,"props":943,"children":944},{},[945],{"type":21,"tag":52,"props":946,"children":947},{},[948],{"type":26,"value":872},{"type":21,"tag":830,"props":950,"children":951},{},[952],{"type":21,"tag":834,"props":953,"children":954},{},[955],{"type":26,"value":956},"The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging.",{"type":21,"tag":792,"props":958,"children":959},{},[],{"type":21,"tag":802,"props":961,"children":963},{"id":962},"_3-droid",[964],{"type":26,"value":965},"3. DROID",{"type":21,"tag":22,"props":967,"children":968},{},[969,973,976],{"type":21,"tag":52,"props":970,"children":971},{},[972],{"type":26,"value":815},{"type":21,"tag":108,"props":974,"children":975},{},[],{"type":26,"value":977},"\nFocused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots.",{"type":21,"tag":22,"props":979,"children":980},{},[981],{"type":21,"tag":52,"props":982,"children":983},{},[984],{"type":26,"value":828},{"type":21,"tag":830,"props":986,"children":987},{},[988,993],{"type":21,"tag":834,"props":989,"children":990},{},[991],{"type":26,"value":992},"Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks.",{"type":21,"tag":834,"props":994,"children":995},{},[996],{"type":26,"value":997},"Multi-camera views (including wrist and exterior images) enable rich visual inputs.",{"type":21,"tag":22,"props":999,"children":1000},{},[1001],{"type":21,"tag":52,"props":1002,"children":1003},{},[1004],{"type":26,"value":851},{"type":21,"tag":830,"props":1006,"children":1007},{},[1008,1013],{"type":21,"tag":834,"props":1009,"children":1010},{},[1011],{"type":26,"value":1012},"Large, diverse dataset that significantly boosts policy performance and robustness.",{"type":21,"tag":834,"props":1014,"children":1015},{},[1016],{"type":26,"value":1017},"Extensive coverage of real-world scenarios.",{"type":21,"tag":22,"props":1019,"children":1020},{},[1021],{"type":21,"tag":52,"props":1022,"children":1023},{},[1024],{"type":26,"value":872},{"type":21,"tag":830,"props":1026,"children":1027},{},[1028],{"type":21,"tag":834,"props":1029,"children":1030},{},[1031],{"type":26,"value":1032},"Being collected with a specific hardware platform, its applicability to other robots may be limited.",{"type":21,"tag":792,"props":1034,"children":1035},{},[],{"type":21,"tag":802,"props":1037,"children":1039},{"id":1038},"_4-roboturk",[1040],{"type":26,"value":1041},"4. RoboTurk",{"type":21,"tag":22,"props":1043,"children":1044},{},[1045,1049,1052],{"type":21,"tag":52,"props":1046,"children":1047},{},[1048],{"type":26,"value":815},{"type":21,"tag":108,"props":1050,"children":1051},{},[],{"type":26,"value":1053},"\nRoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks.",{"type":21,"tag":22,"props":1055,"children":1056},{},[1057],{"type":21,"tag":52,"props":1058,"children":1059},{},[1060],{"type":26,"value":828},{"type":21,"tag":830,"props":1062,"children":1063},{},[1064,1069],{"type":21,"tag":834,"props":1065,"children":1066},{},[1067],{"type":26,"value":1068},"Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset).",{"type":21,"tag":834,"props":1070,"children":1071},{},[1072],{"type":26,"value":1073},"Includes system features for low-latency teleoperation and human-in-the-loop interventions.",{"type":21,"tag":22,"props":1075,"children":1076},{},[1077],{"type":21,"tag":52,"props":1078,"children":1079},{},[1080],{"type":26,"value":851},{"type":21,"tag":830,"props":1082,"children":1083},{},[1084,1089],{"type":21,"tag":834,"props":1085,"children":1086},{},[1087],{"type":26,"value":1088},"Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations.",{"type":21,"tag":834,"props":1090,"children":1091},{},[1092],{"type":26,"value":1093},"Proven effectiveness in enabling imitation learning on challenging tasks.",{"type":21,"tag":22,"props":1095,"children":1096},{},[1097],{"type":21,"tag":52,"props":1098,"children":1099},{},[1100],{"type":26,"value":872},{"type":21,"tag":830,"props":1102,"children":1103},{},[1104],{"type":21,"tag":834,"props":1105,"children":1106},{},[1107],{"type":26,"value":1108},"The quality of demonstrations may vary due to differences in human teleoperation skills.",{"type":21,"tag":792,"props":1110,"children":1111},{},[],{"type":21,"tag":802,"props":1113,"children":1115},{"id":1114},"_5-mime",[1116],{"type":26,"value":1117},"5. MIME",{"type":21,"tag":22,"props":1119,"children":1120},{},[1121,1125,1128],{"type":21,"tag":52,"props":1122,"children":1123},{},[1124],{"type":26,"value":815},{"type":21,"tag":108,"props":1126,"children":1127},{},[],{"type":26,"value":1129},"\nMIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors.",{"type":21,"tag":22,"props":1131,"children":1132},{},[1133],{"type":21,"tag":52,"props":1134,"children":1135},{},[1136],{"type":26,"value":828},{"type":21,"tag":830,"props":1138,"children":1139},{},[1140],{"type":21,"tag":834,"props":1141,"children":1142},{},[1143],{"type":26,"value":1144},"Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation.",{"type":21,"tag":22,"props":1146,"children":1147},{},[1148],{"type":21,"tag":52,"props":1149,"children":1150},{},[1151],{"type":26,"value":851},{"type":21,"tag":830,"props":1153,"children":1154},{},[1155],{"type":21,"tag":834,"props":1156,"children":1157},{},[1158],{"type":26,"value":1159},"Focused on detailed manipulation tasks, making it ideal for imitation learning studies.",{"type":21,"tag":22,"props":1161,"children":1162},{},[1163],{"type":21,"tag":52,"props":1164,"children":1165},{},[1166],{"type":26,"value":872},{"type":21,"tag":830,"props":1168,"children":1169},{},[1170],{"type":21,"tag":834,"props":1171,"children":1172},{},[1173],{"type":26,"value":1174},"Generally smaller in scale compared to some of the largest datasets; might offer limited diversity.",{"type":21,"tag":792,"props":1176,"children":1177},{},[],{"type":21,"tag":802,"props":1179,"children":1181},{"id":1180},"_6-meta-world",[1182],{"type":26,"value":1183},"6. Meta-World",{"type":21,"tag":22,"props":1185,"children":1186},{},[1187,1191,1194],{"type":21,"tag":52,"props":1188,"children":1189},{},[1190],{"type":26,"value":815},{"type":21,"tag":108,"props":1192,"children":1193},{},[],{"type":26,"value":1195},"\nA simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments.",{"type":21,"tag":22,"props":1197,"children":1198},{},[1199],{"type":21,"tag":52,"props":1200,"children":1201},{},[1202],{"type":26,"value":828},{"type":21,"tag":830,"props":1204,"children":1205},{},[1206],{"type":21,"tag":834,"props":1207,"children":1208},{},[1209],{"type":26,"value":1210},"Structured environments with varying goal positions and task variations to test generalization.",{"type":21,"tag":22,"props":1212,"children":1213},{},[1214],{"type":21,"tag":52,"props":1215,"children":1216},{},[1217],{"type":26,"value":851},{"type":21,"tag":830,"props":1219,"children":1220},{},[1221],{"type":21,"tag":834,"props":1222,"children":1223},{},[1224],{"type":26,"value":1225},"Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms.",{"type":21,"tag":22,"props":1227,"children":1228},{},[1229],{"type":21,"tag":52,"props":1230,"children":1231},{},[1232],{"type":26,"value":872},{"type":21,"tag":830,"props":1234,"children":1235},{},[1236],{"type":21,"tag":834,"props":1237,"children":1238},{},[1239],{"type":26,"value":1240},"Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured.",{"type":21,"tag":792,"props":1242,"children":1243},{},[],{"type":21,"tag":802,"props":1245,"children":1247},{"id":1246},"_7-robonet",[1248],{"type":26,"value":1249},"7. RoboNet",{"type":21,"tag":22,"props":1251,"children":1252},{},[1253,1257,1260],{"type":21,"tag":52,"props":1254,"children":1255},{},[1256],{"type":26,"value":815},{"type":21,"tag":108,"props":1258,"children":1259},{},[],{"type":26,"value":1261},"\nRoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation.",{"type":21,"tag":22,"props":1263,"children":1264},{},[1265],{"type":21,"tag":52,"props":1266,"children":1267},{},[1268],{"type":26,"value":828},{"type":21,"tag":830,"props":1270,"children":1271},{},[1272],{"type":21,"tag":834,"props":1273,"children":1274},{},[1275],{"type":26,"value":1276},"Contains over 15M video frames and data from multiple camera viewpoints.",{"type":21,"tag":22,"props":1278,"children":1279},{},[1280],{"type":21,"tag":52,"props":1281,"children":1282},{},[1283],{"type":26,"value":851},{"type":21,"tag":830,"props":1285,"children":1286},{},[1287],{"type":21,"tag":834,"props":1288,"children":1289},{},[1290],{"type":26,"value":1291},"Offers vast amounts of real-world data to study generalization across different robot hardware.",{"type":21,"tag":22,"props":1293,"children":1294},{},[1295],{"type":21,"tag":52,"props":1296,"children":1297},{},[1298],{"type":26,"value":872},{"type":21,"tag":830,"props":1300,"children":1301},{},[1302],{"type":21,"tag":834,"props":1303,"children":1304},{},[1305],{"type":26,"value":1306},"Requires heavy storage and processing; integrating multi-platform data can be challenging.",{"type":21,"tag":792,"props":1308,"children":1309},{},[],{"type":21,"tag":802,"props":1311,"children":1313},{"id":1312},"_8-roboset",[1314],{"type":26,"value":1315},"8. RoboSet",{"type":21,"tag":22,"props":1317,"children":1318},{},[1319,1323,1326],{"type":21,"tag":52,"props":1320,"children":1321},{},[1322],{"type":26,"value":815},{"type":21,"tag":108,"props":1324,"children":1325},{},[],{"type":26,"value":1327},"\nA dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions.",{"type":21,"tag":22,"props":1329,"children":1330},{},[1331],{"type":21,"tag":52,"props":1332,"children":1333},{},[1334],{"type":26,"value":828},{"type":21,"tag":830,"props":1336,"children":1337},{},[1338],{"type":21,"tag":834,"props":1339,"children":1340},{},[1341],{"type":26,"value":1342},"28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped.",{"type":21,"tag":22,"props":1344,"children":1345},{},[1346],{"type":21,"tag":52,"props":1347,"children":1348},{},[1349],{"type":26,"value":851},{"type":21,"tag":830,"props":1351,"children":1352},{},[1353],{"type":21,"tag":834,"props":1354,"children":1355},{},[1356],{"type":26,"value":1357},"Rich multi-modal information (visual + language) supports language-guided robotic learning.",{"type":21,"tag":22,"props":1359,"children":1360},{},[1361],{"type":21,"tag":52,"props":1362,"children":1363},{},[1364],{"type":26,"value":872},{"type":21,"tag":830,"props":1366,"children":1367},{},[1368],{"type":21,"tag":834,"props":1369,"children":1370},{},[1371],{"type":26,"value":1372},"Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios.",{"type":21,"tag":792,"props":1374,"children":1375},{},[],{"type":21,"tag":802,"props":1377,"children":1379},{"id":1378},"_9-bridgedata-v2",[1380],{"type":26,"value":1381},"9. BridgeData V2",{"type":21,"tag":22,"props":1383,"children":1384},{},[1385,1389,1392],{"type":21,"tag":52,"props":1386,"children":1387},{},[1388],{"type":26,"value":815},{"type":21,"tag":108,"props":1390,"children":1391},{},[],{"type":26,"value":1393},"\nDesigned to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning.",{"type":21,"tag":22,"props":1395,"children":1396},{},[1397],{"type":21,"tag":52,"props":1398,"children":1399},{},[1400],{"type":26,"value":828},{"type":21,"tag":830,"props":1402,"children":1403},{},[1404,1409],{"type":21,"tag":834,"props":1405,"children":1406},{},[1407],{"type":26,"value":1408},"Approximately 60K trajectories with multi-view RGB (and some depth) data.",{"type":21,"tag":834,"props":1410,"children":1411},{},[1412],{"type":26,"value":1413},"Includes both teleoperated and scripted demonstrations.",{"type":21,"tag":22,"props":1415,"children":1416},{},[1417],{"type":21,"tag":52,"props":1418,"children":1419},{},[1420],{"type":26,"value":851},{"type":21,"tag":830,"props":1422,"children":1423},{},[1424],{"type":21,"tag":834,"props":1425,"children":1426},{},[1427],{"type":26,"value":1428},"High diversity in environments and tasks; strong support for language-conditioned policy learning.",{"type":21,"tag":22,"props":1430,"children":1431},{},[1432],{"type":21,"tag":52,"props":1433,"children":1434},{},[1435],{"type":26,"value":872},{"type":21,"tag":830,"props":1437,"children":1438},{},[1439],{"type":21,"tag":834,"props":1440,"children":1441},{},[1442],{"type":26,"value":1443},"Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing.",{"type":21,"tag":792,"props":1445,"children":1446},{},[],{"type":21,"tag":802,"props":1448,"children":1450},{"id":1449},"_10-rt-1",[1451],{"type":26,"value":1452},"10. RT-1",{"type":21,"tag":22,"props":1454,"children":1455},{},[1456,1460,1463],{"type":21,"tag":52,"props":1457,"children":1458},{},[1459],{"type":26,"value":815},{"type":21,"tag":108,"props":1461,"children":1462},{},[],{"type":26,"value":1464},"\nRT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks.",{"type":21,"tag":22,"props":1466,"children":1467},{},[1468],{"type":21,"tag":52,"props":1469,"children":1470},{},[1471],{"type":26,"value":828},{"type":21,"tag":830,"props":1473,"children":1474},{},[1475,1480],{"type":21,"tag":834,"props":1476,"children":1477},{},[1478],{"type":26,"value":1479},"Over 130K episodes covering more than 700 tasks collected from 13 robots.",{"type":21,"tag":834,"props":1481,"children":1482},{},[1483],{"type":26,"value":1484},"Utilizes vision and natural language inputs to produce discretized action tokens.",{"type":21,"tag":22,"props":1486,"children":1487},{},[1488],{"type":21,"tag":52,"props":1489,"children":1490},{},[1491],{"type":26,"value":851},{"type":21,"tag":830,"props":1493,"children":1494},{},[1495,1500],{"type":21,"tag":834,"props":1496,"children":1497},{},[1498],{"type":26,"value":1499},"Demonstrates superior performance and generalization, including sim-to-real transfer.",{"type":21,"tag":834,"props":1501,"children":1502},{},[1503],{"type":26,"value":1504},"Scalability through high-capacity transformer models.",{"type":21,"tag":22,"props":1506,"children":1507},{},[1508],{"type":21,"tag":52,"props":1509,"children":1510},{},[1511],{"type":26,"value":872},{"type":21,"tag":830,"props":1513,"children":1514},{},[1515],{"type":21,"tag":834,"props":1516,"children":1517},{},[1518],{"type":26,"value":1519},"Demands extensive data, compute, and engineering expertise; system complexity is high.",{"type":21,"tag":792,"props":1521,"children":1522},{},[],{"type":21,"tag":802,"props":1524,"children":1526},{"id":1525},"_11-dobbe",[1527],{"type":26,"value":1528},"11. Dobb·E",{"type":21,"tag":22,"props":1530,"children":1531},{},[1532,1536,1539],{"type":21,"tag":52,"props":1533,"children":1534},{},[1535],{"type":26,"value":815},{"type":21,"tag":108,"props":1537,"children":1538},{},[],{"type":26,"value":1540},"\nDobb·E focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time.",{"type":21,"tag":22,"props":1542,"children":1543},{},[1544],{"type":21,"tag":52,"props":1545,"children":1546},{},[1547],{"type":26,"value":828},{"type":21,"tag":830,"props":1549,"children":1550},{},[1551,1556],{"type":21,"tag":834,"props":1552,"children":1553},{},[1554],{"type":26,"value":1555},"“HoNY” dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30 fps).",{"type":21,"tag":834,"props":1557,"children":1558},{},[1559],{"type":26,"value":1560},"Includes a low-cost hardware “Stick” for demonstration collection.",{"type":21,"tag":22,"props":1562,"children":1563},{},[1564],{"type":21,"tag":52,"props":1565,"children":1566},{},[1567],{"type":26,"value":851},{"type":21,"tag":830,"props":1569,"children":1570},{},[1571,1576],{"type":21,"tag":834,"props":1572,"children":1573},{},[1574],{"type":26,"value":1575},"Cost-effective and designed for rapid task learning in domestic environments.",{"type":21,"tag":834,"props":1577,"children":1578},{},[1579],{"type":26,"value":1580},"Demonstrates strong real-world applicability in home settings.",{"type":21,"tag":22,"props":1582,"children":1583},{},[1584],{"type":21,"tag":52,"props":1585,"children":1586},{},[1587],{"type":26,"value":872},{"type":21,"tag":830,"props":1589,"children":1590},{},[1591],{"type":21,"tag":834,"props":1592,"children":1593},{},[1594],{"type":26,"value":1595},"Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability.",{"type":21,"tag":792,"props":1597,"children":1598},{},[],{"type":21,"tag":802,"props":1600,"children":1602},{"id":1601},"_12-rh20t",[1603],{"type":26,"value":1604},"12. RH20T",{"type":21,"tag":22,"props":1606,"children":1607},{},[1608,1612,1615],{"type":21,"tag":52,"props":1609,"children":1610},{},[1611],{"type":26,"value":815},{"type":21,"tag":108,"props":1613,"children":1614},{},[],{"type":26,"value":1616},"\nRH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information.",{"type":21,"tag":22,"props":1618,"children":1619},{},[1620],{"type":21,"tag":52,"props":1621,"children":1622},{},[1623],{"type":26,"value":828},{"type":21,"tag":830,"props":1625,"children":1626},{},[1627,1632],{"type":21,"tag":834,"props":1628,"children":1629},{},[1630],{"type":26,"value":1631},"Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing.",{"type":21,"tag":834,"props":1633,"children":1634},{},[1635],{"type":26,"value":1636},"Detailed synchronization and calibration across multiple sensors.",{"type":21,"tag":22,"props":1638,"children":1639},{},[1640],{"type":21,"tag":52,"props":1641,"children":1642},{},[1643],{"type":26,"value":851},{"type":21,"tag":830,"props":1645,"children":1646},{},[1647,1652],{"type":21,"tag":834,"props":1648,"children":1649},{},[1650],{"type":26,"value":1651},"Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion.",{"type":21,"tag":834,"props":1653,"children":1654},{},[1655],{"type":26,"value":1656},"Supports research on contact-rich and dexterous manipulation.",{"type":21,"tag":22,"props":1658,"children":1659},{},[1660],{"type":21,"tag":52,"props":1661,"children":1662},{},[1663],{"type":26,"value":872},{"type":21,"tag":830,"props":1665,"children":1666},{},[1667],{"type":21,"tag":834,"props":1668,"children":1669},{},[1670],{"type":26,"value":1671},"Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing.",{"type":21,"tag":792,"props":1673,"children":1674},{},[],{"type":21,"tag":802,"props":1676,"children":1678},{"id":1677},"_13-bc-z",[1679],{"type":26,"value":1680},"13. BC-Z",{"type":21,"tag":22,"props":1682,"children":1683},{},[1684,1688,1691],{"type":21,"tag":52,"props":1685,"children":1686},{},[1687],{"type":26,"value":815},{"type":21,"tag":108,"props":1689,"children":1690},{},[],{"type":26,"value":1692},"\nBC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches.",{"type":21,"tag":22,"props":1694,"children":1695},{},[1696],{"type":21,"tag":52,"props":1697,"children":1698},{},[1699],{"type":26,"value":828},{"type":21,"tag":830,"props":1701,"children":1702},{},[1703],{"type":21,"tag":834,"props":1704,"children":1705},{},[1706],{"type":26,"value":1707},"Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets.",{"type":21,"tag":22,"props":1709,"children":1710},{},[1711],{"type":21,"tag":52,"props":1712,"children":1713},{},[1714],{"type":26,"value":851},{"type":21,"tag":830,"props":1716,"children":1717},{},[1718],{"type":21,"tag":834,"props":1719,"children":1720},{},[1721],{"type":26,"value":1722},"Serves as a standardized resource for evaluating behavior cloning algorithms.",{"type":21,"tag":22,"props":1724,"children":1725},{},[1726],{"type":21,"tag":52,"props":1727,"children":1728},{},[1729],{"type":26,"value":872},{"type":21,"tag":830,"props":1731,"children":1732},{},[1733],{"type":21,"tag":834,"props":1734,"children":1735},{},[1736],{"type":26,"value":1737},"May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets.",{"type":21,"tag":792,"props":1739,"children":1740},{},[],{"type":21,"tag":802,"props":1742,"children":1744},{"id":1743},"_14-mt-opt",[1745],{"type":26,"value":1746},"14. MT-Opt",{"type":21,"tag":22,"props":1748,"children":1749},{},[1750,1754,1757],{"type":21,"tag":52,"props":1751,"children":1752},{},[1753],{"type":26,"value":815},{"type":21,"tag":108,"props":1755,"children":1756},{},[],{"type":26,"value":1758},"\nMT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently.",{"type":21,"tag":22,"props":1760,"children":1761},{},[1762],{"type":21,"tag":52,"props":1763,"children":1764},{},[1765],{"type":26,"value":828},{"type":21,"tag":830,"props":1767,"children":1768},{},[1769],{"type":21,"tag":834,"props":1770,"children":1771},{},[1772],{"type":26,"value":1773},"Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method.",{"type":21,"tag":22,"props":1775,"children":1776},{},[1777],{"type":21,"tag":52,"props":1778,"children":1779},{},[1780],{"type":26,"value":851},{"type":21,"tag":830,"props":1782,"children":1783},{},[1784,1789],{"type":21,"tag":834,"props":1785,"children":1786},{},[1787],{"type":26,"value":1788},"Effective at sharing experience across tasks, significantly boosting performance on rare tasks.",{"type":21,"tag":834,"props":1790,"children":1791},{},[1792],{"type":26,"value":1793},"Demonstrates both zero-shot and rapid fine-tuning capabilities.",{"type":21,"tag":22,"props":1795,"children":1796},{},[1797],{"type":21,"tag":52,"props":1798,"children":1799},{},[1800],{"type":26,"value":872},{"type":21,"tag":830,"props":1802,"children":1803},{},[1804],{"type":21,"tag":834,"props":1805,"children":1806},{},[1807],{"type":26,"value":1808},"Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines.",{"type":21,"tag":792,"props":1810,"children":1811},{},[],{"type":21,"tag":802,"props":1813,"children":1815},{"id":1814},"_15-vima",[1816],{"type":26,"value":1817},"15. VIMA",{"type":21,"tag":22,"props":1819,"children":1820},{},[1821,1825,1828],{"type":21,"tag":52,"props":1822,"children":1823},{},[1824],{"type":26,"value":815},{"type":21,"tag":108,"props":1826,"children":1827},{},[],{"type":26,"value":1829},"\nVIMA presents a novel formulation in which diverse robot manipulation tasks are “prompted” via interleaved language and visual tokens, unifying task specification.",{"type":21,"tag":22,"props":1831,"children":1832},{},[1833],{"type":21,"tag":52,"props":1834,"children":1835},{},[1836],{"type":26,"value":828},{"type":21,"tag":830,"props":1838,"children":1839},{},[1840],{"type":21,"tag":834,"props":1841,"children":1842},{},[1843],{"type":26,"value":1844},"Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances.",{"type":21,"tag":22,"props":1846,"children":1847},{},[1848],{"type":21,"tag":52,"props":1849,"children":1850},{},[1851],{"type":26,"value":851},{"type":21,"tag":830,"props":1853,"children":1854},{},[1855,1860],{"type":21,"tag":834,"props":1856,"children":1857},{},[1858],{"type":26,"value":1859},"Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency.",{"type":21,"tag":834,"props":1861,"children":1862},{},[1863],{"type":26,"value":1864},"Allows integration of various forms of task instructions (text + image).",{"type":21,"tag":22,"props":1866,"children":1867},{},[1868],{"type":21,"tag":52,"props":1869,"children":1870},{},[1871],{"type":26,"value":872},{"type":21,"tag":830,"props":1873,"children":1874},{},[1875],{"type":21,"tag":834,"props":1876,"children":1877},{},[1878],{"type":26,"value":1879},"Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment.",{"type":21,"tag":792,"props":1881,"children":1882},{},[],{"type":21,"tag":802,"props":1884,"children":1886},{"id":1885},"_16-spoc",[1887],{"type":26,"value":1888},"16. SPOC",{"type":21,"tag":22,"props":1890,"children":1891},{},[1892,1896,1899],{"type":21,"tag":52,"props":1893,"children":1894},{},[1895],{"type":26,"value":815},{"type":21,"tag":108,"props":1897,"children":1898},{},[],{"type":26,"value":1900},"\nSPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation.",{"type":21,"tag":22,"props":1902,"children":1903},{},[1904],{"type":21,"tag":52,"props":1905,"children":1906},{},[1907],{"type":26,"value":828},{"type":21,"tag":830,"props":1909,"children":1910},{},[1911,1916],{"type":21,"tag":834,"props":1912,"children":1913},{},[1914],{"type":26,"value":1915},"Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames.",{"type":21,"tag":834,"props":1917,"children":1918},{},[1919],{"type":26,"value":1920},"Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion.",{"type":21,"tag":22,"props":1922,"children":1923},{},[1924],{"type":21,"tag":52,"props":1925,"children":1926},{},[1927],{"type":26,"value":851},{"type":21,"tag":830,"props":1929,"children":1930},{},[1931,1936],{"type":21,"tag":834,"props":1932,"children":1933},{},[1934],{"type":26,"value":1935},"Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities.",{"type":21,"tag":834,"props":1937,"children":1938},{},[1939],{"type":26,"value":1940},"Trains entirely in simulation and transfers effectively.",{"type":21,"tag":22,"props":1942,"children":1943},{},[1944],{"type":21,"tag":52,"props":1945,"children":1946},{},[1947],{"type":26,"value":872},{"type":21,"tag":830,"props":1949,"children":1950},{},[1951],{"type":21,"tag":834,"props":1952,"children":1953},{},[1954],{"type":26,"value":1955},"RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios.",{"type":21,"tag":792,"props":1957,"children":1958},{},[],{"title":13,"searchDepth":1960,"depth":1960,"links":1961},2,[1962,1963],{"id":31,"depth":1960,"text":34},{"id":797,"depth":1960,"text":800,"children":1964},[1965,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981],{"id":804,"depth":1966,"text":807},3,{"id":886,"depth":1966,"text":889},{"id":962,"depth":1966,"text":965},{"id":1038,"depth":1966,"text":1041},{"id":1114,"depth":1966,"text":1117},{"id":1180,"depth":1966,"text":1183},{"id":1246,"depth":1966,"text":1249},{"id":1312,"depth":1966,"text":1315},{"id":1378,"depth":1966,"text":1381},{"id":1449,"depth":1966,"text":1452},{"id":1525,"depth":1966,"text":1528},{"id":1601,"depth":1966,"text":1604},{"id":1677,"depth":1966,"text":1680},{"id":1743,"depth":1966,"text":1746},{"id":1814,"depth":1966,"text":1817},{"id":1885,"depth":1966,"text":1888},"markdown","content:5.model-dataset-comp:1.robotics-datasets.md","content","5.model-dataset-comp/1.robotics-datasets.md","5.model-dataset-comp/1.robotics-datasets","md","default",["ShallowRef",1990],["ShallowReactive",1991],{"/model-dataset-comp/robotics-datasets":1992},[1993,2002],{"_path":1994,"_dir":1995,"_draft":6,"_partial":6,"_locale":13,"title":1996,"description":1997,"icon":1998,"_type":1982,"_id":1999,"_source":1984,"_file":2000,"_stem":2001,"_extension":1987},"/published-research/hallucination","published-research","Hallucination Prevention in LLMs","Zero-Resource Hallucination Prevention for Large Language Models","lucide:brain","content:4.published-research:2.hallucination.md","4.published-research/2.hallucination.md","4.published-research/2.hallucination",{"_path":2003,"_dir":12,"_draft":6,"_partial":6,"_locale":13,"title":2004,"description":2005,"icon":2006,"_type":1982,"_id":2007,"_source":1984,"_file":2008,"_stem":2009,"_extension":1987},"/model-dataset-comp/robotics-models","General-Purpose Robot Models Analysis","Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.","lucide:bot","content:5.model-dataset-comp:2.robotics-models.md","5.model-dataset-comp/2.robotics-models.md","5.model-dataset-comp/2.robotics-models",["ShallowRef",2011],{},[2013,2017,2024,2043,2051],{"title":2014,"_path":2015,"icon":2016},"Cyber Nachos","/cybernachos","lucide:hand",{"title":2018,"_path":2019,"children":2020},"Published Products","/published",[2021],{"title":2022,"_path":2023,"icon":1998},"Cyber Nachos GPT","/published/cybernachos-gpt",{"title":2025,"_path":2026,"children":2027},"Tutorial Isaac Lab","/tutorial-isaaclab",[2028,2031,2035,2039],{"title":2029,"_path":2030,"icon":2006},"Introduction","/tutorial-isaaclab/introduction",{"title":2032,"_path":2033,"icon":2034},"Installation Guide","/tutorial-isaaclab/installation","lucide:download",{"title":2036,"_path":2037,"icon":2038},"Getting Started","/tutorial-isaaclab/getting-started","lucide:flag",{"title":2040,"_path":2041,"icon":2042},"Enabling fluid simulation","/tutorial-isaaclab/enable-fluid","material-symbols:water-drop-outline",{"title":2044,"_path":2045,"children":2046},"Published Research Papers","/published-research",[2047,2050],{"title":2048,"_path":2049,"icon":2006},"Real-time Dexterous Telemanipulation","/published-research/real-time-dexterous",{"title":1996,"_path":1994,"icon":1998},{"title":2052,"_path":2053,"children":2054},"Model & Dataset Comparisons","/model-dataset-comp",[2055,2056],{"title":14,"_path":11,"icon":16},{"title":2004,"_path":2003,"icon":2006},["Map"],{"_priority":2059,"env":2063,"name":2064,"url":2065},{"name":2060,"env":2061,"url":2062},-10,-15,-3,"production","shadcn-docs-nuxt-starter","https://cybernachos.github.io",["Set"],["ShallowReactive",2068],{"XFVaCZk9MC":2069},null,1743199448702]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{mdc:{components:{prose:true,map:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},content:{locales:[],defaultLocale:"",integrity:1743199429432,experimental:{stripQueryParameters:false,advanceQuery:false,clientDB:false},respectPathCase:false,api:{baseURL:"/api/_content"},navigation:{fields:["icon","navBadges","navTruncate","badges","toc","sidebar","collapse","editLink","prevNext","breadcrumb","fullpage","layout"]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:{theme:{default:"github-light",dark:"github-dark"},preload:["json","js","ts","html","css","vue","diff","shell","markdown","mdc","yaml","bash","ini","dotenv"]},wsUrl:"",documentDriven:{page:true,navigation:true,surround:true,globals:{},layoutFallbacks:["theme"],injectPage:true},host:"",trailingSlash:false,search:{indexed:true,ignoredTags:["script","style","pre"],filterQuery:{_draft:false,_partial:false},options:{fields:["title","content","titles"],storeFields:["title","content","titles"],searchOptions:{prefix:true,fuzzy:.2,boost:{title:4,content:2,titles:1}}}},contentHead:true,anchorLinks:{depth:4,exclude:[1]}},"nuxt-scripts":{version:"",defaultScriptOptions:{trigger:"onNuxtReady"}}},app:{baseURL:"/",buildId:"c16d95b9-45cb-4472-b6c7-54ac3c4ea957",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>