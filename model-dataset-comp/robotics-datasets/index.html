<!DOCTYPE html><html><head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Robotics Dataset Comparison - Cyber Nachos</title>
<style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }/*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{border:0 solid #e5e7eb;box-sizing:border-box}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-tap-highlight-color:transparent}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-size:1em;font-variation-settings:normal}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{color:inherit;font-family:inherit;font-feature-settings:inherit;font-size:100%;font-variation-settings:inherit;font-weight:inherit;letter-spacing:inherit;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{height:auto;max-width:100%}[hidden]:where(:not([hidden=until-found])){display:none}:root{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--primary:221.2 83.2% 53.3%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--ring:221.2 83.2% 53.3%;--radius:.5rem}.dark{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--primary:217.2 91.2% 59.8%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--ring:224.3 76.3% 48%}*{border-color:hsl(var(--border))}body{background-color:hsl(var(--background));color:hsl(var(--foreground))}.container{margin-left:auto;margin-right:auto;padding-left:2rem;padding-right:2rem;width:100%}@media (min-width:1400px){.container{max-width:1400px}}.sr-only{height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;clip:rect(0,0,0,0);border-width:0;white-space:nowrap}.pointer-events-none{pointer-events:none}.pointer-events-auto{pointer-events:auto}.visible{visibility:visible}.\!collapse{visibility:collapse!important}.collapse{visibility:collapse}.static{position:static}.fixed{position:fixed}.absolute{position:absolute}.relative{position:relative}.sticky{position:sticky}.inset-0{top:0;right:0;bottom:0;left:0}.inset-x-0{left:0;right:0}.inset-y-0{bottom:0;top:0}.bottom-0{bottom:0}.left-0{left:0}.left-1\/2{left:50%}.right-0{right:0}.right-1{right:.25rem}.right-2{right:.5rem}.right-3{right:.75rem}.right-4{right:1rem}.top-0{top:0}.top-1{top:.25rem}.top-1\/2{top:50%}.top-2{top:.5rem}.top-3{top:.75rem}.top-4{top:1rem}.top-\[102px\]{top:102px}.top-\[60\%\]{top:60%}.top-\[90px\]{top:90px}.top-full{top:100%}.top-px{top:1px}.z-10{z-index:10}.z-30{z-index:30}.z-40{z-index:40}.z-50{z-index:50}.z-\[100\]{z-index:100}.z-\[1\]{z-index:1}.order-first{order:-9999}.order-last{order:9999}.col-span-2{grid-column:span 2/span 2}.m-0{margin:0}.-mx-1{margin-left:-.25rem;margin-right:-.25rem}.-mx-4{margin-left:-1rem;margin-right:-1rem}.mx-0\.5{margin-left:.125rem;margin-right:.125rem}.mx-2{margin-left:.5rem;margin-right:.5rem}.mx-3\.5{margin-left:.875rem;margin-right:.875rem}.mx-4{margin-left:1rem;margin-right:1rem}.mx-auto{margin-left:auto;margin-right:auto}.my-8{margin-bottom:2rem;margin-top:2rem}.-ml-2{margin-left:-.5rem}.mb-0{margin-bottom:0}.mb-1{margin-bottom:.25rem}.mb-2{margin-bottom:.5rem}.mb-3{margin-bottom:.75rem}.mb-4{margin-bottom:1rem}.mb-5{margin-bottom:1.25rem}.mb-6{margin-bottom:1.5rem}.mb-\[2px\]{margin-bottom:2px}.ml-1{margin-left:.25rem}.ml-2{margin-left:.5rem}.ml-3{margin-left:.75rem}.ml-4{margin-left:1rem}.ml-6{margin-left:1.5rem}.ml-auto{margin-left:auto}.mr-1{margin-right:.25rem}.mr-1\.5{margin-right:.375rem}.mr-2{margin-right:.5rem}.mr-3{margin-right:.75rem}.mr-4{margin-right:1rem}.mr-auto{margin-right:auto}.mt-0{margin-top:0}.mt-1{margin-top:.25rem}.mt-1\.5{margin-top:.375rem}.mt-16{margin-top:4rem}.mt-2{margin-top:.5rem}.mt-4{margin-top:1rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.inline-flex{display:inline-flex}.table{display:table}.grid{display:grid}.contents{display:contents}.hidden{display:none}.size-10{height:2.5rem;width:2.5rem}.size-16{height:4rem;width:4rem}.size-2{height:.5rem;width:.5rem}.size-3{height:.75rem;width:.75rem}.size-3\.5{height:.875rem;width:.875rem}.size-32{height:8rem;width:8rem}.size-4{height:1rem;width:1rem}.size-5{height:1.25rem;width:1.25rem}.size-6{height:1.5rem;width:1.5rem}.size-8{height:2rem;width:2rem}.size-full{height:100%;width:100%}.h-1\.5{height:.375rem}.h-10{height:2.5rem}.h-11{height:2.75rem}.h-12{height:3rem}.h-14{height:3.5rem}.h-2\.5{height:.625rem}.h-4{height:1rem}.h-48{height:12rem}.h-5{height:1.25rem}.h-6{height:1.5rem}.h-7{height:1.75rem}.h-8{height:2rem}.h-9{height:2.25rem}.h-\[--radix-navigation-menu-viewport-height\]{height:var(--radix-navigation-menu-viewport-height)}.h-\[calc\(100vh-3\.5rem\)\]{height:calc(100vh - 3.5rem)}.h-\[calc\(100vh-6\.5rem\)\]{height:calc(100vh - 6.5rem)}.h-full{height:100%}.h-px{height:1px}.h-svh{height:100svh}.max-h-screen{max-height:100vh}.min-h-4{min-height:1rem}.min-h-5{min-height:1.25rem}.min-h-6{min-height:1.5rem}.min-h-screen{min-height:100vh}.w-2\.5{width:.625rem}.w-3\/4{width:75%}.w-4{width:1rem}.w-5{width:1.25rem}.w-72{width:18rem}.w-9{width:2.25rem}.w-\[200px\]{width:200px}.w-\[23rem\]{width:23rem}.w-\[250px\]{width:250px}.w-auto{width:auto}.w-fit{width:-moz-fit-content;width:fit-content}.w-full{width:100%}.w-max{width:-moz-max-content;width:max-content}.w-px{width:1px}.min-w-0{min-width:0}.min-w-4{min-width:1rem}.min-w-5{min-width:1.25rem}.min-w-6{min-width:1.5rem}.max-w-2xl{max-width:42rem}.max-w-\[750px\]{max-width:750px}.max-w-\[980px\]{max-width:980px}.max-w-lg{max-width:32rem}.max-w-max{max-width:-moz-max-content;max-width:max-content}.max-w-screen-2xl{max-width:1536px}.flex-1{flex:1 1 0%}.shrink-0{flex-shrink:0}.flex-grow{flex-grow:1}.basis-1\/3{flex-basis:33.333333%}.-translate-x-1\/2{--tw-translate-x:-50%}.-translate-x-1\/2,.-translate-y-1\/2{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.-translate-y-1\/2{--tw-translate-y:-50%}.-rotate-90{--tw-rotate:-90deg}.-rotate-90,.rotate-0{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.rotate-0{--tw-rotate:0deg}.rotate-45{--tw-rotate:45deg}.rotate-45,.rotate-90{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.rotate-90{--tw-rotate:90deg}.scale-0{--tw-scale-x:0;--tw-scale-y:0}.scale-0,.scale-100{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.scale-100{--tw-scale-x:1;--tw-scale-y:1}.transform{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@keyframes spin{to{transform:rotate(1turn)}}.animate-spin{animation:spin 1s linear infinite}.cursor-default{cursor:default}.cursor-pointer{cursor:pointer}.touch-none{touch-action:none}.select-none{-webkit-user-select:none;-moz-user-select:none;user-select:none}.scroll-m-20{scroll-margin:5rem}.list-decimal{list-style-type:decimal}.list-disc{list-style-type:disc}.list-none{list-style-type:none}.grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.grid-cols-5{grid-template-columns:repeat(5,minmax(0,1fr))}.grid-cols-\[repeat\(auto-fit\,_minmax\(270px\,_1fr\)\)\]{grid-template-columns:repeat(auto-fit,minmax(270px,1fr))}.flex-row{flex-direction:row}.flex-col{flex-direction:column}.flex-col-reverse{flex-direction:column-reverse}.flex-wrap{flex-wrap:wrap}.place-items-center{place-items:center}.items-start{align-items:flex-start}.items-end{align-items:flex-end}.items-center{align-items:center}.justify-start{justify-content:flex-start}.justify-end{justify-content:flex-end}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.gap-1{gap:.25rem}.gap-1\.5{gap:.375rem}.gap-2{gap:.5rem}.gap-3{gap:.75rem}.gap-4{gap:1rem}.gap-5{gap:1.25rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.gap-x-1{-moz-column-gap:.25rem;column-gap:.25rem}.gap-y-1\.5{row-gap:.375rem}.gap-y-2{row-gap:.5rem}.space-x-2>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(.5rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(.5rem*var(--tw-space-x-reverse))}.space-x-4>:not([hidden])~:not([hidden]){--tw-space-x-reverse:0;margin-left:calc(1rem*(1 - var(--tw-space-x-reverse)));margin-right:calc(1rem*var(--tw-space-x-reverse))}.space-y-1>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.25rem*var(--tw-space-y-reverse));margin-top:calc(.25rem*(1 - var(--tw-space-y-reverse)))}.space-y-1\.5>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.375rem*var(--tw-space-y-reverse));margin-top:calc(.375rem*(1 - var(--tw-space-y-reverse)))}.space-y-10>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(2.5rem*var(--tw-space-y-reverse));margin-top:calc(2.5rem*(1 - var(--tw-space-y-reverse)))}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-bottom:calc(.5rem*var(--tw-space-y-reverse));margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)))}.divide-x>:not([hidden])~:not([hidden]){--tw-divide-x-reverse:0;border-left-width:calc(1px*(1 - var(--tw-divide-x-reverse)));border-right-width:calc(1px*var(--tw-divide-x-reverse))}.divide-y>:not([hidden])~:not([hidden]){--tw-divide-y-reverse:0;border-bottom-width:calc(1px*var(--tw-divide-y-reverse));border-top-width:calc(1px*(1 - var(--tw-divide-y-reverse)))}.self-center{align-self:center}.overflow-hidden{overflow:hidden}.overflow-clip{overflow:clip}.overflow-x-auto{overflow-x:auto}.overflow-y-auto{overflow-y:auto}.overflow-x-hidden{overflow-x:hidden}.truncate{overflow:hidden;text-overflow:ellipsis}.truncate,.whitespace-nowrap{white-space:nowrap}.text-nowrap{text-wrap:nowrap}.break-words{overflow-wrap:break-word}.rounded{border-radius:.25rem}.rounded-\[inherit\]{border-radius:inherit}.rounded-full{border-radius:9999px}.rounded-lg{border-radius:var(--radius)}.rounded-md{border-radius:calc(var(--radius) - 2px)}.rounded-none{border-radius:0}.rounded-sm{border-radius:calc(var(--radius) - 4px)}.rounded-t-none{border-top-left-radius:0;border-top-right-radius:0}.rounded-tl-sm{border-top-left-radius:calc(var(--radius) - 4px)}.border{border-width:1px}.border-2{border-width:2px}.border-b{border-bottom-width:1px}.border-b-2{border-bottom-width:2px}.border-l{border-left-width:1px}.border-l-2{border-left-width:2px}.border-r{border-right-width:1px}.border-t{border-top-width:1px}.border-none{border-style:none}.border-\[\#adfa1d\]{--tw-border-opacity:1;border-color:rgb(173 250 29/var(--tw-border-opacity,1))}.border-amber-500{--tw-border-opacity:1;border-color:rgb(245 158 11/var(--tw-border-opacity,1))}.border-amber-600{--tw-border-opacity:1;border-color:rgb(217 119 6/var(--tw-border-opacity,1))}.border-blue-700{--tw-border-opacity:1;border-color:rgb(29 78 216/var(--tw-border-opacity,1))}.border-border{border-color:hsl(var(--border))}.border-destructive{border-color:hsl(var(--destructive))}.border-destructive\/50{border-color:hsl(var(--destructive)/.5)}.border-green-500{--tw-border-opacity:1;border-color:rgb(34 197 94/var(--tw-border-opacity,1))}.border-green-600{--tw-border-opacity:1;border-color:rgb(22 163 74/var(--tw-border-opacity,1))}.border-input{border-color:hsl(var(--input))}.border-primary{border-color:hsl(var(--primary))}.border-red-500{--tw-border-opacity:1;border-color:rgb(239 68 68/var(--tw-border-opacity,1))}.border-red-600{--tw-border-opacity:1;border-color:rgb(220 38 38/var(--tw-border-opacity,1))}.border-sky-500{--tw-border-opacity:1;border-color:rgb(14 165 233/var(--tw-border-opacity,1))}.border-sky-600{--tw-border-opacity:1;border-color:rgb(2 132 199/var(--tw-border-opacity,1))}.border-transparent{border-color:transparent}.border-violet-600{--tw-border-opacity:1;border-color:rgb(124 58 237/var(--tw-border-opacity,1))}.border-b-transparent{border-bottom-color:transparent}.border-l-transparent{border-left-color:transparent}.border-t-transparent{border-top-color:transparent}.bg-\[\#adfa1d\]{--tw-bg-opacity:1;background-color:rgb(173 250 29/var(--tw-bg-opacity,1))}.bg-amber-500{--tw-bg-opacity:1;background-color:rgb(245 158 11/var(--tw-bg-opacity,1))}.bg-background{background-color:hsl(var(--background))}.bg-background\/80{background-color:hsl(var(--background)/.8)}.bg-black\/80{background-color:#000c}.bg-border{background-color:hsl(var(--border))}.bg-card{background-color:hsl(var(--card))}.bg-destructive{background-color:hsl(var(--destructive))}.bg-green-100{--tw-bg-opacity:1;background-color:rgb(220 252 231/var(--tw-bg-opacity,1))}.bg-green-500{--tw-bg-opacity:1;background-color:rgb(34 197 94/var(--tw-bg-opacity,1))}.bg-muted{background-color:hsl(var(--muted))}.bg-muted\/30{background-color:hsl(var(--muted)/.3)}.bg-muted\/40{background-color:hsl(var(--muted)/.4)}.bg-muted\/50{background-color:hsl(var(--muted)/.5)}.bg-popover{background-color:hsl(var(--popover))}.bg-primary{background-color:hsl(var(--primary))}.bg-primary\/80{background-color:hsl(var(--primary)/.8)}.bg-red-100{--tw-bg-opacity:1;background-color:rgb(254 226 226/var(--tw-bg-opacity,1))}.bg-red-500{--tw-bg-opacity:1;background-color:rgb(239 68 68/var(--tw-bg-opacity,1))}.bg-secondary{background-color:hsl(var(--secondary))}.bg-sky-500{--tw-bg-opacity:1;background-color:rgb(14 165 233/var(--tw-bg-opacity,1))}.bg-transparent{background-color:transparent}.bg-white{--tw-bg-opacity:1;background-color:rgb(255 255 255/var(--tw-bg-opacity,1))}.bg-zinc-950{--tw-bg-opacity:1;background-color:rgb(9 9 11/var(--tw-bg-opacity,1))}.object-cover{-o-object-fit:cover;object-fit:cover}.object-\[50\%_53\%\]{-o-object-position:50% 53%;object-position:50% 53%}.object-center{-o-object-position:center;object-position:center}.\!p-2{padding:.5rem!important}.p-0{padding:0}.p-0\.5{padding:.125rem}.p-1{padding:.25rem}.p-1\.5{padding:.375rem}.p-16{padding:4rem}.p-2{padding:.5rem}.p-3{padding:.75rem}.p-4{padding:1rem}.p-6{padding:1.5rem}.p-px{padding:1px}.px-0\.5{padding-left:.125rem;padding-right:.125rem}.px-1{padding-left:.25rem;padding-right:.25rem}.px-1\.5{padding-left:.375rem;padding-right:.375rem}.px-2{padding-left:.5rem;padding-right:.5rem}.px-2\.5{padding-left:.625rem;padding-right:.625rem}.px-3{padding-left:.75rem;padding-right:.75rem}.px-4{padding-left:1rem;padding-right:1rem}.px-8{padding-left:2rem;padding-right:2rem}.px-\[0\.3rem\]{padding-left:.3rem;padding-right:.3rem}.py-0\.5{padding-bottom:.125rem;padding-top:.125rem}.py-1{padding-bottom:.25rem;padding-top:.25rem}.py-1\.5{padding-bottom:.375rem;padding-top:.375rem}.py-2{padding-bottom:.5rem;padding-top:.5rem}.py-3{padding-bottom:.75rem;padding-top:.75rem}.py-4{padding-bottom:1rem;padding-top:1rem}.py-6{padding-bottom:1.5rem;padding-top:1.5rem}.py-8{padding-bottom:2rem;padding-top:2rem}.py-\[0\.2rem\]{padding-bottom:.2rem;padding-top:.2rem}.pb-2{padding-bottom:.5rem}.pb-3{padding-bottom:.75rem}.pb-4{padding-bottom:1rem}.pb-5{padding-bottom:1.25rem}.pl-2{padding-left:.5rem}.pl-3{padding-left:.75rem}.pl-4{padding-left:1rem}.pl-6{padding-left:1.5rem}.pl-8{padding-left:2rem}.pr-0{padding-right:0}.pr-1\.5{padding-right:.375rem}.pr-3{padding-right:.75rem}.pr-6{padding-right:1.5rem}.pt-0{padding-top:0}.pt-1{padding-top:.25rem}.pt-2{padding-top:.5rem}.pt-4{padding-top:1rem}.pt-5{padding-top:1.25rem}.pt-6{padding-top:1.5rem}.text-left{text-align:left}.text-center{text-align:center}.text-right{text-align:right}.font-mono{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-3xl{font-size:1.875rem;line-height:2.25rem}.text-4xl{font-size:2.25rem;line-height:2.5rem}.text-5xl{font-size:3rem;line-height:1}.text-8xl{font-size:6rem;line-height:1}.text-\[10px\]{font-size:10px}.text-\[11px\]{font-size:11px}.text-\[12px\]{font-size:12px}.text-base{font-size:1rem;line-height:1.5rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xl{font-size:1.25rem;line-height:1.75rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-extrabold{font-weight:800}.font-light{font-weight:300}.font-medium{font-weight:500}.font-normal{font-weight:400}.font-semibold{font-weight:600}.uppercase{text-transform:uppercase}.capitalize{text-transform:capitalize}.italic{font-style:italic}.leading-4{line-height:1rem}.leading-7{line-height:1.75rem}.leading-none{line-height:1}.leading-tight{line-height:1.25}.tracking-tight{letter-spacing:-.025em}.tracking-tighter{letter-spacing:-.05em}.\!text-primary{color:hsl(var(--primary))!important}.text-amber-500{--tw-text-opacity:1;color:rgb(245 158 11/var(--tw-text-opacity,1))}.text-amber-600{--tw-text-opacity:1;color:rgb(217 119 6/var(--tw-text-opacity,1))}.text-black{--tw-text-opacity:1;color:rgb(0 0 0/var(--tw-text-opacity,1))}.text-blue-700{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity,1))}.text-card-foreground{color:hsl(var(--card-foreground))}.text-destructive{color:hsl(var(--destructive))}.text-destructive-foreground{color:hsl(var(--destructive-foreground))}.text-foreground{color:hsl(var(--foreground))}.text-foreground\/50{color:hsl(var(--foreground)/.5)}.text-foreground\/70{color:hsl(var(--foreground)/.7)}.text-foreground\/80{color:hsl(var(--foreground)/.8)}.text-green-500{--tw-text-opacity:1;color:rgb(34 197 94/var(--tw-text-opacity,1))}.text-green-600{--tw-text-opacity:1;color:rgb(22 163 74/var(--tw-text-opacity,1))}.text-muted-foreground{color:hsl(var(--muted-foreground))}.text-popover-foreground{color:hsl(var(--popover-foreground))}.text-primary{color:hsl(var(--primary))}.text-primary-foreground{color:hsl(var(--primary-foreground))}.text-red-500{--tw-text-opacity:1;color:rgb(239 68 68/var(--tw-text-opacity,1))}.text-red-600{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity,1))}.text-secondary-foreground{color:hsl(var(--secondary-foreground))}.text-sky-500{--tw-text-opacity:1;color:rgb(14 165 233/var(--tw-text-opacity,1))}.text-sky-600{--tw-text-opacity:1;color:rgb(2 132 199/var(--tw-text-opacity,1))}.text-violet-600{--tw-text-opacity:1;color:rgb(124 58 237/var(--tw-text-opacity,1))}.text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.text-zinc-100{--tw-text-opacity:1;color:rgb(244 244 245/var(--tw-text-opacity,1))}.text-zinc-400{--tw-text-opacity:1;color:rgb(161 161 170/var(--tw-text-opacity,1))}.text-zinc-500{--tw-text-opacity:1;color:rgb(113 113 122/var(--tw-text-opacity,1))}.text-zinc-900{--tw-text-opacity:1;color:rgb(24 24 27/var(--tw-text-opacity,1))}.underline{text-decoration-line:underline}.no-underline{text-decoration-line:none}.underline-offset-4{text-underline-offset:4px}.opacity-0{opacity:0}.opacity-100{opacity:1}.opacity-50{opacity:.5}.opacity-70{opacity:.7}.opacity-90{opacity:.9}.shadow-lg{--tw-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -4px rgba(0,0,0,.1);--tw-shadow-colored:0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color)}.shadow-lg,.shadow-md{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-md{--tw-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -2px rgba(0,0,0,.1);--tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color)}.shadow-none{--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000}.shadow-none,.shadow-sm{box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.shadow-sm{--tw-shadow:0 1px 2px 0 rgba(0,0,0,.05);--tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color)}.outline-none{outline:2px solid transparent;outline-offset:2px}.outline{outline-style:solid}.ring-offset-background{--tw-ring-offset-color:hsl(var(--background))}.filter{filter:var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow)}.backdrop-blur-lg{--tw-backdrop-blur:blur(16px);-webkit-backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia);backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia)}.transition{transition-duration:.15s;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-all{transition-duration:.15s;transition-property:all;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-colors{transition-duration:.15s;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-none{transition-property:none}.transition-opacity{transition-duration:.15s;transition-property:opacity;transition-timing-function:cubic-bezier(.4,0,.2,1)}.transition-transform{transition-duration:.15s;transition-property:transform;transition-timing-function:cubic-bezier(.4,0,.2,1)}.duration-200{transition-duration:.2s}.duration-75{transition-duration:75ms}.ease-in-out{transition-timing-function:cubic-bezier(.4,0,.2,1)}@keyframes enter{0%{opacity:var(--tw-enter-opacity,1);transform:translate3d(var(--tw-enter-translate-x,0),var(--tw-enter-translate-y,0),0) scale3d(var(--tw-enter-scale,1),var(--tw-enter-scale,1),var(--tw-enter-scale,1)) rotate(var(--tw-enter-rotate,0))}}@keyframes exit{to{opacity:var(--tw-exit-opacity,1);transform:translate3d(var(--tw-exit-translate-x,0),var(--tw-exit-translate-y,0),0) scale3d(var(--tw-exit-scale,1),var(--tw-exit-scale,1),var(--tw-exit-scale,1)) rotate(var(--tw-exit-rotate,0))}}.duration-200{animation-duration:.2s}.duration-75{animation-duration:75ms}.ease-in-out{animation-timing-function:cubic-bezier(.4,0,.2,1)}.running{animation-play-state:running}.step{counter-increment:step}.step:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[counter-reset\:step\]{counter-reset:step}.placeholder\:text-muted-foreground::-moz-placeholder{color:hsl(var(--muted-foreground))}.placeholder\:text-muted-foreground::placeholder{color:hsl(var(--muted-foreground))}.even\:bg-muted\/50:nth-child(2n){background-color:hsl(var(--muted)/.5)}.hover\:cursor-pointer:hover{cursor:pointer}.hover\:bg-\[\#adfa1d\]:hover{--tw-bg-opacity:1;background-color:rgb(173 250 29/var(--tw-bg-opacity,1))}.hover\:bg-accent:hover{background-color:hsl(var(--accent))}.hover\:bg-amber-400:hover{--tw-bg-opacity:1;background-color:rgb(251 191 36/var(--tw-bg-opacity,1))}.hover\:bg-destructive\/80:hover{background-color:hsl(var(--destructive)/.8)}.hover\:bg-destructive\/90:hover{background-color:hsl(var(--destructive)/.9)}.hover\:bg-green-400:hover{--tw-bg-opacity:1;background-color:rgb(74 222 128/var(--tw-bg-opacity,1))}.hover\:bg-muted:hover{background-color:hsl(var(--muted))}.hover\:bg-muted\/40:hover{background-color:hsl(var(--muted)/.4)}.hover\:bg-muted\/50:hover{background-color:hsl(var(--muted)/.5)}.hover\:bg-primary\/80:hover{background-color:hsl(var(--primary)/.8)}.hover\:bg-primary\/90:hover{background-color:hsl(var(--primary)/.9)}.hover\:bg-red-400:hover{--tw-bg-opacity:1;background-color:rgb(248 113 113/var(--tw-bg-opacity,1))}.hover\:bg-secondary:hover{background-color:hsl(var(--secondary))}.hover\:bg-secondary\/80:hover{background-color:hsl(var(--secondary)/.8)}.hover\:bg-sky-400:hover{--tw-bg-opacity:1;background-color:rgb(56 189 248/var(--tw-bg-opacity,1))}.hover\:text-accent-foreground:hover{color:hsl(var(--accent-foreground))}.hover\:text-foreground:hover{color:hsl(var(--foreground))}.hover\:text-primary:hover{color:hsl(var(--primary))}.hover\:underline:hover{text-decoration-line:underline}.hover\:opacity-100:hover{opacity:1}.focus\:bg-accent:focus{background-color:hsl(var(--accent))}.focus\:text-accent-foreground:focus{color:hsl(var(--accent-foreground))}.focus\:opacity-100:focus{opacity:1}.focus\:outline-none:focus{outline:2px solid transparent;outline-offset:2px}.focus\:ring-1:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color)}.focus\:ring-1:focus,.focus\:ring-2:focus{box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.focus\:ring-2:focus{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color)}.focus\:ring-ring:focus{--tw-ring-color:hsl(var(--ring))}.focus\:ring-offset-2:focus{--tw-ring-offset-width:2px}.focus-visible\:outline-none:focus-visible{outline:2px solid transparent;outline-offset:2px}.focus-visible\:ring-2:focus-visible{--tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);--tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color);box-shadow:var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 #0000)}.focus-visible\:ring-ring:focus-visible{--tw-ring-color:hsl(var(--ring))}.focus-visible\:ring-offset-2:focus-visible{--tw-ring-offset-width:2px}.disabled\:pointer-events-none:disabled{pointer-events:none}.disabled\:cursor-not-allowed:disabled{cursor:not-allowed}.disabled\:opacity-50:disabled{opacity:.5}.group:hover .group-hover\:opacity-100{opacity:1}.group.destructive .group-\[\.destructive\]\:border-muted\/40{border-color:hsl(var(--muted)/.4)}.group.destructive .group-\[\.destructive\]\:text-red-300{--tw-text-opacity:1;color:rgb(252 165 165/var(--tw-text-opacity,1))}.group.destructive .group-\[\.destructive\]\:hover\:border-destructive\/30:hover{border-color:hsl(var(--destructive)/.3)}.group.destructive .group-\[\.destructive\]\:hover\:bg-destructive:hover{background-color:hsl(var(--destructive))}.group.destructive .group-\[\.destructive\]\:hover\:text-destructive-foreground:hover{color:hsl(var(--destructive-foreground))}.group.destructive .group-\[\.destructive\]\:hover\:text-red-50:hover{--tw-text-opacity:1;color:rgb(254 242 242/var(--tw-text-opacity,1))}.group.destructive .group-\[\.destructive\]\:focus\:ring-destructive:focus{--tw-ring-color:hsl(var(--destructive))}.group.destructive .group-\[\.destructive\]\:focus\:ring-red-400:focus{--tw-ring-opacity:1;--tw-ring-color:rgb(248 113 113/var(--tw-ring-opacity,1))}.group.destructive .group-\[\.destructive\]\:focus\:ring-offset-red-600:focus{--tw-ring-offset-color:#dc2626}.peer:disabled~.peer-disabled\:cursor-not-allowed{cursor:not-allowed}.peer:disabled~.peer-disabled\:opacity-70{opacity:.7}.group:has(div) .group-has-\[div\]\:mt-0{margin-top:0}.data-\[disabled\]\:pointer-events-none[data-disabled]{pointer-events:none}.data-\[swipe\=cancel\]\:translate-x-0[data-swipe=cancel]{--tw-translate-x:0px}.data-\[swipe\=cancel\]\:translate-x-0[data-swipe=cancel],.data-\[swipe\=end\]\:translate-x-\[var\(--radix-toast-swipe-end-x\)\][data-swipe=end]{transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.data-\[swipe\=end\]\:translate-x-\[var\(--radix-toast-swipe-end-x\)\][data-swipe=end]{--tw-translate-x:var(--radix-toast-swipe-end-x)}.data-\[swipe\=move\]\:translate-x-\[var\(--radix-toast-swipe-move-x\)\][data-swipe=move]{--tw-translate-x:var(--radix-toast-swipe-move-x);transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@keyframes accordion-up{0%{height:var(--radix-accordion-content-height)}to{height:0}}.data-\[state\=closed\]\:animate-accordion-up[data-state=closed]{animation:accordion-up .2s ease-out}@keyframes collapsible-up{0%{height:var(--radix-collapsible-content-height)}to{height:0}}.data-\[state\=closed\]\:animate-collapsible-up[data-state=closed]{animation:collapsible-up .2s ease-in-out}@keyframes accordion-down{0%{height:0}to{height:var(--radix-accordion-content-height)}}.data-\[state\=open\]\:animate-accordion-down[data-state=open]{animation:accordion-down .2s ease-out}@keyframes collapsible-down{0%{height:0}to{height:var(--radix-collapsible-content-height)}}.data-\[state\=open\]\:animate-collapsible-down[data-state=open]{animation:collapsible-down .2s ease-in-out}.data-\[state\=active\]\:border-b-primary[data-state=active]{border-bottom-color:hsl(var(--primary))}.data-\[active\]\:bg-accent\/50[data-active]{background-color:hsl(var(--accent)/.5)}.data-\[highlighted\]\:bg-accent[data-highlighted]{background-color:hsl(var(--accent))}.data-\[state\=active\]\:bg-background[data-state=active]{background-color:hsl(var(--background))}.data-\[state\=open\]\:bg-accent[data-state=open]{background-color:hsl(var(--accent))}.data-\[state\=open\]\:bg-accent\/50[data-state=open]{background-color:hsl(var(--accent)/.5)}.data-\[state\=open\]\:bg-secondary[data-state=open]{background-color:hsl(var(--secondary))}.data-\[state\=active\]\:text-foreground[data-state=active]{color:hsl(var(--foreground))}.data-\[state\=open\]\:text-muted-foreground[data-state=open]{color:hsl(var(--muted-foreground))}.data-\[disabled\]\:opacity-50[data-disabled]{opacity:.5}.data-\[state\=active\]\:shadow-none[data-state=active]{--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.data-\[state\=active\]\:shadow-sm[data-state=active]{--tw-shadow:0 1px 2px 0 rgba(0,0,0,.05);--tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color);box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow)}.data-\[swipe\=move\]\:transition-none[data-swipe=move]{transition-property:none}.data-\[state\=closed\]\:duration-300[data-state=closed]{transition-duration:.3s}.data-\[state\=open\]\:duration-500[data-state=open]{transition-duration:.5s}.data-\[motion\^\=from-\]\:animate-in[data-motion^=from-],.data-\[state\=open\]\:animate-in[data-state=open],.data-\[state\=visible\]\:animate-in[data-state=visible]{animation-duration:.15s;animation-name:enter;--tw-enter-opacity:initial;--tw-enter-scale:initial;--tw-enter-rotate:initial;--tw-enter-translate-x:initial;--tw-enter-translate-y:initial}.data-\[motion\^\=to-\]\:animate-out[data-motion^=to-],.data-\[state\=closed\]\:animate-out[data-state=closed],.data-\[state\=hidden\]\:animate-out[data-state=hidden],.data-\[swipe\=end\]\:animate-out[data-swipe=end]{animation-duration:.15s;animation-name:exit;--tw-exit-opacity:initial;--tw-exit-scale:initial;--tw-exit-rotate:initial;--tw-exit-translate-x:initial;--tw-exit-translate-y:initial}.data-\[motion\^\=from-\]\:fade-in[data-motion^=from-]{--tw-enter-opacity:0}.data-\[motion\^\=to-\]\:fade-out[data-motion^=to-],.data-\[state\=closed\]\:fade-out-0[data-state=closed]{--tw-exit-opacity:0}.data-\[state\=closed\]\:fade-out-80[data-state=closed]{--tw-exit-opacity:.8}.data-\[state\=hidden\]\:fade-out[data-state=hidden]{--tw-exit-opacity:0}.data-\[state\=open\]\:fade-in-0[data-state=open],.data-\[state\=visible\]\:fade-in[data-state=visible]{--tw-enter-opacity:0}.data-\[state\=closed\]\:zoom-out-95[data-state=closed]{--tw-exit-scale:.95}.data-\[state\=open\]\:zoom-in-90[data-state=open]{--tw-enter-scale:.9}.data-\[state\=open\]\:zoom-in-95[data-state=open]{--tw-enter-scale:.95}.data-\[motion\=from-end\]\:slide-in-from-right-52[data-motion=from-end]{--tw-enter-translate-x:13rem}.data-\[motion\=from-start\]\:slide-in-from-left-52[data-motion=from-start]{--tw-enter-translate-x:-13rem}.data-\[motion\=to-end\]\:slide-out-to-right-52[data-motion=to-end]{--tw-exit-translate-x:13rem}.data-\[motion\=to-start\]\:slide-out-to-left-52[data-motion=to-start]{--tw-exit-translate-x:-13rem}.data-\[side\=bottom\]\:slide-in-from-top-2[data-side=bottom]{--tw-enter-translate-y:-.5rem}.data-\[side\=left\]\:slide-in-from-right-2[data-side=left]{--tw-enter-translate-x:.5rem}.data-\[side\=right\]\:slide-in-from-left-2[data-side=right]{--tw-enter-translate-x:-.5rem}.data-\[side\=top\]\:slide-in-from-bottom-2[data-side=top]{--tw-enter-translate-y:.5rem}.data-\[state\=closed\]\:slide-out-to-bottom[data-state=closed]{--tw-exit-translate-y:100%}.data-\[state\=closed\]\:slide-out-to-left[data-state=closed]{--tw-exit-translate-x:-100%}.data-\[state\=closed\]\:slide-out-to-left-1\/2[data-state=closed]{--tw-exit-translate-x:-50%}.data-\[state\=closed\]\:slide-out-to-right-full[data-state=closed],.data-\[state\=closed\]\:slide-out-to-right[data-state=closed]{--tw-exit-translate-x:100%}.data-\[state\=closed\]\:slide-out-to-top[data-state=closed]{--tw-exit-translate-y:-100%}.data-\[state\=closed\]\:slide-out-to-top-\[48\%\][data-state=closed]{--tw-exit-translate-y:-48%}.data-\[state\=open\]\:slide-in-from-bottom[data-state=open]{--tw-enter-translate-y:100%}.data-\[state\=open\]\:slide-in-from-left[data-state=open]{--tw-enter-translate-x:-100%}.data-\[state\=open\]\:slide-in-from-left-1\/2[data-state=open]{--tw-enter-translate-x:-50%}.data-\[state\=open\]\:slide-in-from-right[data-state=open]{--tw-enter-translate-x:100%}.data-\[state\=open\]\:slide-in-from-top[data-state=open]{--tw-enter-translate-y:-100%}.data-\[state\=open\]\:slide-in-from-top-\[48\%\][data-state=open]{--tw-enter-translate-y:-48%}.data-\[state\=open\]\:slide-in-from-top-full[data-state=open]{--tw-enter-translate-y:-100%}.data-\[state\=closed\]\:duration-300[data-state=closed]{animation-duration:.3s}.data-\[state\=open\]\:duration-500[data-state=open]{animation-duration:.5s}.group[data-state=open] .group-data-\[state\=open\]\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:block:is(.dark *){display:block}.dark\:hidden:is(.dark *){display:none}.dark\:-rotate-90:is(.dark *){--tw-rotate:-90deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:rotate-0:is(.dark *){--tw-rotate:0deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:scale-0:is(.dark *){--tw-scale-x:0;--tw-scale-y:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:scale-100:is(.dark *){--tw-scale-x:1;--tw-scale-y:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.dark\:border-destructive:is(.dark *){border-color:hsl(var(--destructive))}@media (min-width:640px){.sm\:bottom-0{bottom:0}.sm\:right-0{right:0}.sm\:top-auto{top:auto}.sm\:inline{display:inline}.sm\:hidden{display:none}.sm\:h-\[350px\]{height:350px}.sm\:max-w-sm{max-width:24rem}.sm\:flex-row{flex-direction:row}.sm\:flex-col{flex-direction:column}.sm\:justify-end{justify-content:flex-end}.sm\:gap-2\.5{gap:.625rem}.sm\:gap-x-2{-moz-column-gap:.5rem;column-gap:.5rem}.sm\:rounded-lg{border-radius:var(--radius)}.sm\:text-left{text-align:left}.sm\:text-xl{font-size:1.25rem;line-height:1.75rem}.data-\[state\=open\]\:sm\:slide-in-from-bottom-full[data-state=open]{--tw-enter-translate-y:100%}}@media (min-width:768px){.md\:absolute{position:absolute}.md\:sticky{position:sticky}.md\:top-\[60px\]{top:60px}.md\:order-last{order:9999}.md\:col-span-2{grid-column:span 2/span 2}.md\:block{display:block}.md\:flex{display:flex}.md\:grid{display:grid}.md\:hidden{display:none}.md\:h-24{height:6rem}.md\:w-40{width:10rem}.md\:w-\[--radix-navigation-menu-viewport-width\]{width:var(--radix-navigation-menu-viewport-width)}.md\:w-auto{width:auto}.md\:w-full{width:100%}.md\:max-w-\[420px\]{max-width:420px}.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.md\:grid-cols-\[220px_minmax\(0\,1fr\)\]{grid-template-columns:220px minmax(0,1fr)}.md\:flex-row{flex-direction:row}.md\:gap-6{gap:1.5rem}.md\:px-8{padding-left:2rem;padding-right:2rem}.md\:py-0{padding-bottom:0;padding-top:0}.md\:py-12{padding-bottom:3rem;padding-top:3rem}.md\:pb-10{padding-bottom:2.5rem}.md\:pb-8{padding-bottom:2rem}.md\:pr-4{padding-right:1rem}.md\:text-4xl{font-size:2.25rem;line-height:2.5rem}.md\:text-6xl{font-size:3.75rem;line-height:1}}@media (min-width:1024px){.lg\:mt-6{margin-top:1.5rem}.lg\:block{display:block}.lg\:flex{display:flex}.lg\:grid{display:grid}.lg\:hidden{display:none}.lg\:w-64{width:16rem}.lg\:flex-none{flex:none}.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:grid-cols-\[1fr_220px\]{grid-template-columns:1fr 220px}.lg\:grid-cols-\[240px_minmax\(0\,1fr\)\]{grid-template-columns:240px minmax(0,1fr)}.lg\:flex-row{flex-direction:row}.lg\:flex-col{flex-direction:column}.lg\:gap-10{gap:2.5rem}.lg\:gap-14{gap:3.5rem}.lg\:border-b{border-bottom-width:1px}.lg\:border-b-0{border-bottom-width:0}.lg\:border-r{border-right-width:1px}.lg\:border-none{border-style:none}.lg\:py-12{padding-bottom:3rem;padding-top:3rem}.lg\:py-24{padding-bottom:6rem;padding-top:6rem}.lg\:py-6{padding-bottom:1.5rem;padding-top:1.5rem}.lg\:py-8{padding-bottom:2rem;padding-top:2rem}.lg\:pb-10{padding-bottom:2.5rem}.lg\:pb-20{padding-bottom:5rem}.lg\:text-center{text-align:center}.lg\:text-5xl{font-size:3rem;line-height:1}.lg\:leading-\[1\.1\]{line-height:1.1}}.\[\&\+div\]\:text-xs+div{font-size:.75rem;line-height:1rem}.\[\&\:not\(\:first-child\)\]\:mt-10:not(:first-child){margin-top:2.5rem}.\[\&\:not\(\:first-child\)\]\:mt-4:not(:first-child){margin-top:1rem}.\[\&\:not\(\:first-child\)\]\:mt-5:not(:first-child){margin-top:1.25rem}.\[\&\:not\(\:first-child\)\]\:mt-6:not(:first-child){margin-top:1.5rem}.\[\&\:not\(\:first-child\)\]\:mt-8:not(:first-child){margin-top:2rem}.\[\&\:not\(\:first-child\)\]\:pt-3:not(:first-child){padding-top:.75rem}.\[\&\:not\(\:first-child\)\]\:pt-4:not(:first-child){padding-top:1rem}.\[\&\:not\(\:last-child\)\]\:mb-12:not(:last-child){margin-bottom:3rem}.\[\&\:not\(\:last-child\)\]\:mb-5:not(:last-child){margin-bottom:1.25rem}.\[\&\:not\(\:last-child\)\]\:mb-6:not(:last-child){margin-bottom:1.5rem}.\[\&\:not\(\:last-child\)\]\:mb-8:not(:last-child){margin-bottom:2rem}.\[\&\>h1\]\:step>h1{counter-increment:step}.\[\&\>h1\]\:step>h1:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h2\]\:step>h2{counter-increment:step}.\[\&\>h2\]\:step>h2:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h3\]\:step>h3{counter-increment:step}.\[\&\>h3\]\:step>h3:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h4\]\:step>h4{counter-increment:step}.\[\&\>h4\]\:step>h4:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h5\]\:step>h5{counter-increment:step}.\[\&\>h5\]\:step>h5:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>h6\]\:step>h6{counter-increment:step}.\[\&\>h6\]\:step>h6:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>li\:not\(\:first-child\)\]\:mt-2>li:not(:first-child){margin-top:.5rem}.\[\&\>li\]\:step>li{counter-increment:step}.\[\&\>li\]\:step>li:before{align-items:center;background-color:hsl(var(--muted));border-color:hsl(var(--background));border-radius:9999px;border-width:4px;content:counter(step);display:inline-flex;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1rem;font-weight:500;height:2.25rem;justify-content:center;line-height:1.5rem;margin-left:-50px;margin-top:-.25rem;position:absolute;text-align:center;text-indent:-1px;width:2.25rem}.\[\&\>ol\]\:\!mt-2>ol{margin-top:.5rem!important}.\[\&\>svg\+div\]\:translate-y-\[-3px\]>svg+div{--tw-translate-y:-3px;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.\[\&\>svg\]\:absolute>svg{position:absolute}.\[\&\>svg\]\:left-4>svg{left:1rem}.\[\&\>svg\]\:top-4>svg{top:1rem}.\[\&\>svg\]\:size-3\.5>svg{height:.875rem;width:.875rem}.\[\&\>svg\]\:text-amber-600>svg{--tw-text-opacity:1;color:rgb(217 119 6/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-blue-700>svg{--tw-text-opacity:1;color:rgb(29 78 216/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-destructive>svg{color:hsl(var(--destructive))}.\[\&\>svg\]\:text-foreground>svg{color:hsl(var(--foreground))}.\[\&\>svg\]\:text-green-600>svg{--tw-text-opacity:1;color:rgb(22 163 74/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-red-600>svg{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-sky-600>svg{--tw-text-opacity:1;color:rgb(2 132 199/var(--tw-text-opacity,1))}.\[\&\>svg\]\:text-violet-600>svg{--tw-text-opacity:1;color:rgb(124 58 237/var(--tw-text-opacity,1))}.\[\&\>svg\~\*\]\:pl-7>svg~*{padding-left:1.75rem}.\[\&\>ul\]\:\!mt-2>ul{margin-top:.5rem!important}.\[\&\[align\=center\]\]\:text-center[align=center]{text-align:center}.\[\&\[align\=right\]\]\:text-right[align=right]{text-align:right}.\[\&\[data-state\=open\]\>svg\]\:rotate-180[data-state=open]>svg{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.\[\&_\[cmdk-group-heading\]\]\:px-2 [cmdk-group-heading]{padding-left:.5rem;padding-right:.5rem}.\[\&_\[cmdk-group-heading\]\]\:py-1\.5 [cmdk-group-heading]{padding-bottom:.375rem;padding-top:.375rem}.\[\&_\[cmdk-group-heading\]\]\:text-xs [cmdk-group-heading]{font-size:.75rem;line-height:1rem}.\[\&_\[cmdk-group-heading\]\]\:font-medium [cmdk-group-heading]{font-weight:500}.\[\&_\[cmdk-group-heading\]\]\:text-muted-foreground [cmdk-group-heading]{color:hsl(var(--muted-foreground))}.\[\&_p\]\:leading-relaxed p{line-height:1.625}</style>
<style>html{color-scheme:light}html.dark{color-scheme:dark}.theme-zinc{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:240 5.9% 10%;--primary-foreground:0 0% 98%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:240 5.9% 10%;--radius:.5rem}.dark .theme-zinc{--background:240 10% 3.9%;--foreground:0 0% 98%;--muted:240 3.7% 15.9%;--muted-foreground:240 5% 64.9%;--popover:240 10% 3.9%;--popover-foreground:0 0% 98%;--card:240 10% 3.9%;--card-foreground:0 0% 98%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:0 0% 98%;--primary-foreground:240 5.9% 10%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:240 3.7% 15.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:240 4.9% 83.9%}.theme-slate{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--primary:222.2 47.4% 11.2%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--ring:222.2 84% 4.9%;--radius:.5rem}.dark .theme-slate{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--primary:210 40% 98%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--ring:212.7 26.8% 83.9}.theme-stone{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:24 9.8% 10%;--primary-foreground:60 9.1% 97.8%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:20 14.3% 4.1%;--radius:.5rem}.dark .theme-stone{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:60 9.1% 97.8%;--primary-foreground:24 9.8% 10%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 62.8% 30.6%;--destructive-foreground:60 9.1% 97.8%;--ring:24 5.7% 82.9%}.theme-gray{--background:0 0% 100%;--foreground:224 71.4% 4.1%;--muted:220 14.3% 95.9%;--muted-foreground:220 8.9% 46.1%;--popover:0 0% 100%;--popover-foreground:224 71.4% 4.1%;--card:0 0% 100%;--card-foreground:224 71.4% 4.1%;--border:220 13% 91%;--input:220 13% 91%;--primary:220.9 39.3% 11%;--primary-foreground:210 20% 98%;--secondary:220 14.3% 95.9%;--secondary-foreground:220.9 39.3% 11%;--accent:220 14.3% 95.9%;--accent-foreground:220.9 39.3% 11%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 20% 98%;--ring:224 71.4% 4.1%;--radius:.5rem}.dark .theme-gray{--background:224 71.4% 4.1%;--foreground:210 20% 98%;--muted:215 27.9% 16.9%;--muted-foreground:217.9 10.6% 64.9%;--popover:224 71.4% 4.1%;--popover-foreground:210 20% 98%;--card:224 71.4% 4.1%;--card-foreground:210 20% 98%;--border:215 27.9% 16.9%;--input:215 27.9% 16.9%;--primary:210 20% 98%;--primary-foreground:220.9 39.3% 11%;--secondary:215 27.9% 16.9%;--secondary-foreground:210 20% 98%;--accent:215 27.9% 16.9%;--accent-foreground:210 20% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 20% 98%;--ring:216 12.2% 83.9%}.theme-neutral{--background:0 0% 100%;--foreground:0 0% 3.9%;--muted:0 0% 96.1%;--muted-foreground:0 0% 45.1%;--popover:0 0% 100%;--popover-foreground:0 0% 3.9%;--card:0 0% 100%;--card-foreground:0 0% 3.9%;--border:0 0% 89.8%;--input:0 0% 89.8%;--primary:0 0% 9%;--primary-foreground:0 0% 98%;--secondary:0 0% 96.1%;--secondary-foreground:0 0% 9%;--accent:0 0% 96.1%;--accent-foreground:0 0% 9%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:0 0% 3.9%;--radius:.5rem}.dark .theme-neutral{--background:0 0% 3.9%;--foreground:0 0% 98%;--muted:0 0% 14.9%;--muted-foreground:0 0% 63.9%;--popover:0 0% 3.9%;--popover-foreground:0 0% 98%;--card:0 0% 3.9%;--card-foreground:0 0% 98%;--border:0 0% 14.9%;--input:0 0% 14.9%;--primary:0 0% 98%;--primary-foreground:0 0% 9%;--secondary:0 0% 14.9%;--secondary-foreground:0 0% 98%;--accent:0 0% 14.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:0 0% 83.1%}.theme-red{--background:0 0% 100%;--foreground:0 0% 3.9%;--muted:0 0% 96.1%;--muted-foreground:0 0% 45.1%;--popover:0 0% 100%;--popover-foreground:0 0% 3.9%;--card:0 0% 100%;--card-foreground:0 0% 3.9%;--border:0 0% 89.8%;--input:0 0% 89.8%;--primary:0 72.2% 50.6%;--primary-foreground:0 85.7% 97.3%;--secondary:0 0% 96.1%;--secondary-foreground:0 0% 9%;--accent:0 0% 96.1%;--accent-foreground:0 0% 9%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:0 72.2% 50.6%;--radius:.5rem}.dark .theme-red{--background:0 0% 3.9%;--foreground:0 0% 98%;--muted:0 0% 14.9%;--muted-foreground:0 0% 63.9%;--popover:0 0% 3.9%;--popover-foreground:0 0% 98%;--card:0 0% 3.9%;--card-foreground:0 0% 98%;--border:0 0% 14.9%;--input:0 0% 14.9%;--primary:0 72.2% 50.6%;--primary-foreground:0 85.7% 97.3%;--secondary:0 0% 14.9%;--secondary-foreground:0 0% 98%;--accent:0 0% 14.9%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 0% 98%;--ring:0 72.2% 50.6%}.theme-rose{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:346.8 77.2% 49.8%;--primary-foreground:355.7 100% 97.3%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:346.8 77.2% 49.8%;--radius:.5rem}.dark .theme-rose{--background:20 14.3% 4.1%;--foreground:0 0% 95%;--muted:0 0% 15%;--muted-foreground:240 5% 64.9%;--popover:0 0% 9%;--popover-foreground:0 0% 95%;--card:24 9.8% 10%;--card-foreground:0 0% 95%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:346.8 77.2% 49.8%;--primary-foreground:355.7 100% 97.3%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:12 6.5% 15.1%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 85.7% 97.3%;--ring:346.8 77.2% 49.8%}.theme-orange{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:24.6 95% 53.1%;--primary-foreground:60 9.1% 97.8%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:24.6 95% 53.1%;--radius:.5rem}.dark .theme-orange{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:20.5 90.2% 48.2%;--primary-foreground:60 9.1% 97.8%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 72.2% 50.6%;--destructive-foreground:60 9.1% 97.8%;--ring:20.5 90.2% 48.2%}.theme-green{--background:0 0% 100%;--foreground:240 10% 3.9%;--muted:240 4.8% 95.9%;--muted-foreground:240 3.8% 46.1%;--popover:0 0% 100%;--popover-foreground:240 10% 3.9%;--card:0 0% 100%;--card-foreground:240 10% 3.9%;--border:240 5.9% 90%;--input:240 5.9% 90%;--primary:142.1 76.2% 36.3%;--primary-foreground:355.7 100% 97.3%;--secondary:240 4.8% 95.9%;--secondary-foreground:240 5.9% 10%;--accent:240 4.8% 95.9%;--accent-foreground:240 5.9% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:0 0% 98%;--ring:142.1 76.2% 36.3%;--radius:.5rem}.dark .theme-green{--background:20 14.3% 4.1%;--foreground:0 0% 95%;--muted:0 0% 15%;--muted-foreground:240 5% 64.9%;--popover:0 0% 9%;--popover-foreground:0 0% 95%;--card:24 9.8% 10%;--card-foreground:0 0% 95%;--border:240 3.7% 15.9%;--input:240 3.7% 15.9%;--primary:142.1 70.6% 45.3%;--primary-foreground:144.9 80.4% 10%;--secondary:240 3.7% 15.9%;--secondary-foreground:0 0% 98%;--accent:12 6.5% 15.1%;--accent-foreground:0 0% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:0 85.7% 97.3%;--ring:142.4 71.8% 29.2%}.theme-blue{--background:0 0% 100%;--foreground:222.2 84% 4.9%;--muted:210 40% 96.1%;--muted-foreground:215.4 16.3% 46.9%;--popover:0 0% 100%;--popover-foreground:222.2 84% 4.9%;--card:0 0% 100%;--card-foreground:222.2 84% 4.9%;--border:214.3 31.8% 91.4%;--input:214.3 31.8% 91.4%;--primary:221.2 83.2% 53.3%;--primary-foreground:210 40% 98%;--secondary:210 40% 96.1%;--secondary-foreground:222.2 47.4% 11.2%;--accent:210 40% 96.1%;--accent-foreground:222.2 47.4% 11.2%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 40% 98%;--ring:221.2 83.2% 53.3%;--radius:.5rem}.dark .theme-blue{--background:222.2 84% 4.9%;--foreground:210 40% 98%;--muted:217.2 32.6% 17.5%;--muted-foreground:215 20.2% 65.1%;--popover:222.2 84% 4.9%;--popover-foreground:210 40% 98%;--card:222.2 84% 4.9%;--card-foreground:210 40% 98%;--border:217.2 32.6% 17.5%;--input:217.2 32.6% 17.5%;--primary:217.2 91.2% 59.8%;--primary-foreground:222.2 47.4% 11.2%;--secondary:217.2 32.6% 17.5%;--secondary-foreground:210 40% 98%;--accent:217.2 32.6% 17.5%;--accent-foreground:210 40% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 40% 98%;--ring:224.3 76.3% 48%}.theme-yellow{--background:0 0% 100%;--foreground:20 14.3% 4.1%;--muted:60 4.8% 95.9%;--muted-foreground:25 5.3% 44.7%;--popover:0 0% 100%;--popover-foreground:20 14.3% 4.1%;--card:0 0% 100%;--card-foreground:20 14.3% 4.1%;--border:20 5.9% 90%;--input:20 5.9% 90%;--primary:47.9 95.8% 53.1%;--primary-foreground:26 83.3% 14.1%;--secondary:60 4.8% 95.9%;--secondary-foreground:24 9.8% 10%;--accent:60 4.8% 95.9%;--accent-foreground:24 9.8% 10%;--destructive:0 84.2% 60.2%;--destructive-foreground:60 9.1% 97.8%;--ring:20 14.3% 4.1%;--radius:.5rem}.dark .theme-yellow{--background:20 14.3% 4.1%;--foreground:60 9.1% 97.8%;--muted:12 6.5% 15.1%;--muted-foreground:24 5.4% 63.9%;--popover:20 14.3% 4.1%;--popover-foreground:60 9.1% 97.8%;--card:20 14.3% 4.1%;--card-foreground:60 9.1% 97.8%;--border:12 6.5% 15.1%;--input:12 6.5% 15.1%;--primary:47.9 95.8% 53.1%;--primary-foreground:26 83.3% 14.1%;--secondary:12 6.5% 15.1%;--secondary-foreground:60 9.1% 97.8%;--accent:12 6.5% 15.1%;--accent-foreground:60 9.1% 97.8%;--destructive:0 62.8% 30.6%;--destructive-foreground:60 9.1% 97.8%;--ring:35.5 91.7% 32.9%}.theme-violet{--background:0 0% 100%;--foreground:224 71.4% 4.1%;--muted:220 14.3% 95.9%;--muted-foreground:220 8.9% 46.1%;--popover:0 0% 100%;--popover-foreground:224 71.4% 4.1%;--card:0 0% 100%;--card-foreground:224 71.4% 4.1%;--border:220 13% 91%;--input:220 13% 91%;--primary:262.1 83.3% 57.8%;--primary-foreground:210 20% 98%;--secondary:220 14.3% 95.9%;--secondary-foreground:220.9 39.3% 11%;--accent:220 14.3% 95.9%;--accent-foreground:220.9 39.3% 11%;--destructive:0 84.2% 60.2%;--destructive-foreground:210 20% 98%;--ring:262.1 83.3% 57.8%;--radius:.5rem}.dark .theme-violet{--background:224 71.4% 4.1%;--foreground:210 20% 98%;--muted:215 27.9% 16.9%;--muted-foreground:217.9 10.6% 64.9%;--popover:224 71.4% 4.1%;--popover-foreground:210 20% 98%;--card:224 71.4% 4.1%;--card-foreground:210 20% 98%;--border:215 27.9% 16.9%;--input:215 27.9% 16.9%;--primary:263.4 70% 50.4%;--primary-foreground:210 20% 98%;--secondary:215 27.9% 16.9%;--secondary-foreground:210 20% 98%;--accent:215 27.9% 16.9%;--accent-foreground:210 20% 98%;--destructive:0 62.8% 30.6%;--destructive-foreground:210 20% 98%;--ring:263.4 70% 50.4%}</style>
<style>.carbon-responsive-wrap{align-items:center!important;background-color:hsl(var(--background))!important;border-color:hsl(var(--border))!important;border-radius:var(--radius)!important;display:flex!important;padding:1rem!important}@media (min-width:1024px){.carbon-responsive-wrap{flex-direction:column!important;padding-bottom:1.5rem!important;padding-top:1.5rem!important}}.carbon-responsive-wrap .carbon-img{border-radius:.25rem!important;overflow:hidden!important}@media (min-width:1024px){.carbon-responsive-wrap .carbon-img{flex:none!important}}.carbon-responsive-wrap .carbon-text{color:hsl(var(--muted-foreground))!important;font-size:.875rem!important;line-height:1.25rem!important}@media (min-width:1024px){.carbon-responsive-wrap .carbon-text{flex:none!important;text-align:center!important}}#carbonads .carbon-poweredby{background-color:hsl(var(--background))!important;color:hsl(var(--muted-foreground))!important;display:block!important;font-size:10px!important;text-align:right!important;text-decoration-line:none!important;text-transform:uppercase!important}</style>
<link rel="stylesheet" href="/_nuxt/entry.BZKa7Edp.css" crossorigin>
<style>:where(.i-lucide\:arrow-left){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m12 19l-7-7l7-7m7 7H5'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-right){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M5 12h14m-7-7l7 7l-7 7'/%3E%3C/svg%3E")}:where(.i-lucide\:arrow-up){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m5 12l7-7l7 7m-7 7V5'/%3E%3C/svg%3E")}:where(.i-lucide\:bot){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M12 8V4H8'/%3E%3Crect width='16' height='12' x='4' y='8' rx='2'/%3E%3Cpath d='M2 14h2m16 0h2m-7-1v2m-6-2v2'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:brain){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M12 5a3 3 0 1 0-5.997.125a4 4 0 0 0-2.526 5.77a4 4 0 0 0 .556 6.588A4 4 0 1 0 12 18Z'/%3E%3Cpath d='M12 5a3 3 0 1 1 5.997.125a4 4 0 0 1 2.526 5.77a4 4 0 0 1-.556 6.588A4 4 0 1 1 12 18Z'/%3E%3Cpath d='M15 13a4.5 4.5 0 0 1-3-4a4.5 4.5 0 0 1-3 4m8.599-6.5a3 3 0 0 0 .399-1.375m-11.995 0A3 3 0 0 0 6.401 6.5m-2.924 4.396a4 4 0 0 1 .585-.396m15.876 0a4 4 0 0 1 .585.396M6 18a4 4 0 0 1-1.967-.516m15.934 0A4 4 0 0 1 18 18'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:database){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cellipse cx='12' cy='5' rx='9' ry='3'/%3E%3Cpath d='M3 5v14a9 3 0 0 0 18 0V5'/%3E%3Cpath d='M3 12a9 3 0 0 0 18 0'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:download){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4m4-5l5 5l5-5m-5 5V3'/%3E%3C/svg%3E")}:where(.i-lucide\:flag){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 15s1-1 4-1s5 2 8 2s4-1 4-1V3s-1 1-4 1s-5-2-8-2s-4 1-4 1zm0 7v-7'/%3E%3C/svg%3E")}:where(.i-lucide\:github){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5c.08-1.25-.27-2.48-1-3.5c.28-1.15.28-2.35 0-3.5c0 0-1 0-3 1.5c-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.4 5.4 0 0 0 4 9c0 3.5 3 5.5 6 5.5c-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4'/%3E%3Cpath d='M9 18c-4.51 2-5-2-7-2'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:hand){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Cpath d='M18 11V6a2 2 0 0 0-2-2a2 2 0 0 0-2 2m0 4V4a2 2 0 0 0-2-2a2 2 0 0 0-2 2v2m0 4.5V6a2 2 0 0 0-2-2a2 2 0 0 0-2 2v8'/%3E%3Cpath d='M18 8a2 2 0 1 1 4 0v6a8 8 0 0 1-8 8h-2c-2.8 0-4.5-.86-5.99-2.34l-3.6-3.6a2 2 0 0 1 2.83-2.82L7 15'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-lucide\:menu){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 12h16M4 6h16M4 18h16'/%3E%3C/svg%3E")}:where(.i-lucide\:moon){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M12 3a6 6 0 0 0 9 9a9 9 0 1 1-9-9'/%3E%3C/svg%3E")}:where(.i-lucide\:paintbrush){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='m14.622 17.897l-10.68-2.913M18.376 2.622a1 1 0 1 1 3.002 3.002L17.36 9.643a.5.5 0 0 0 0 .707l.944.944a2.41 2.41 0 0 1 0 3.408l-.944.944a.5.5 0 0 1-.707 0L8.354 7.348a.5.5 0 0 1 0-.707l.944-.944a2.41 2.41 0 0 1 3.408 0l.944.944a.5.5 0 0 0 .707 0zM9 8c-1.804 2.71-3.97 3.46-6.583 3.948a.507.507 0 0 0-.302.819l7.32 8.883a1 1 0 0 0 1.185.204C12.735 20.405 16 16.792 16 15'/%3E%3C/svg%3E")}:where(.i-lucide\:sun){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cg fill='none' stroke='black' stroke-linecap='round' stroke-linejoin='round' stroke-width='2'%3E%3Ccircle cx='12' cy='12' r='4'/%3E%3Cpath d='M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41'/%3E%3C/g%3E%3C/svg%3E")}:where(.i-material-symbols\:water-drop-outline){display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' width='24' height='24'%3E%3Cpath fill='black' d='M12.275 19q.3-.025.513-.238T13 18.25q0-.35-.225-.562T12.2 17.5q-1.025.075-2.175-.562t-1.45-2.313q-.05-.275-.262-.45T7.825 14q-.35 0-.575.263t-.15.612q.425 2.275 2 3.25t3.175.875M12 22q-3.425 0-5.712-2.35T4 13.8q0-2.5 1.988-5.437T12 2q4.025 3.425 6.013 6.363T20 13.8q0 3.5-2.287 5.85T12 22m0-2q2.6 0 4.3-1.763T18 13.8q0-1.825-1.513-4.125T12 4.65Q9.025 7.375 7.513 9.675T6 13.8q0 2.675 1.7 4.438T12 20m0-8'/%3E%3C/svg%3E")}</style>
<link rel="preload" as="fetch" crossorigin="anonymous" href="/model-dataset-comp/robotics-datasets/_payload.json?57dc7828-7b5d-44e7-97ea-45198066b06c">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CMO6v0lR.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/I1dayEUe.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/Ce_fbWLk.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CWdOIeuz.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BixK3j1R.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CVdgv97E.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/Cxht6FSf.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DEDj_sCe.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BN3_wKYq.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CpZrzFZ1.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/D5B0EPYr.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ByDPZjQy.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CWmyRJOY.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/C7G5nwlh.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/D-TsS7E3.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CK623zo8.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CG72-2wh.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/f6r0BkpF.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DfYLpOUA.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/f5XyYqMS.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/CdaVelc5.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/ND5CHFs5.js">
<link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/57dc7828-7b5d-44e7-97ea-45198066b06c.json">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Deg-9YHY.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/C1lwRdxD.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/bNaE6FFb.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/BwOPSDqo.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/6j0gV1sp.js">
<meta property="og:image" content="https://cybernachos.github.io/__og-image__/static/model-dataset-comp/robotics-datasets/og.png">
<meta property="og:image:type" content="image/png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cybernachos.github.io/__og-image__/static/model-dataset-comp/robotics-datasets/og.png">
<meta name="twitter:image:src" content="https://cybernachos.github.io/__og-image__/static/model-dataset-comp/robotics-datasets/og.png">
<meta property="og:image:width" content="1200">
<meta name="twitter:image:width" content="1200">
<meta property="og:image:height" content="600">
<meta name="twitter:image:height" content="600">
<meta name="description" content="A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, Dobb·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.">
<meta property="og:description" content="A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, Dobb·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.">
<meta property="og:title" content="Robotics Dataset Comparison">
<script type="module" src="/_nuxt/CMO6v0lR.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body  class="theme-zinc" style="--radius:0.5rem"><div id="__nuxt"><!--[--><div class="nuxt-loading-indicator z-100 bg-primary/80" style="position:fixed;top:0;right:0;left:0;pointer-events:none;width:auto;height:3px;opacity:0;background-size:Infinity% auto;transform:scaleX(0%);transform-origin:left;transition:transform 0.1s, height 0.4s, opacity 0.4s;z-index:999999;"></div><!----><header class="sticky top-0 z-40 bg-background/80 backdrop-blur-lg"><div class="container max-w-screen-2xl flex h-14 items-center justify-between gap-2 px-4 md:px-8"><div class="hidden flex-1 md:flex"><a href="/" class="flex"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" data-nuxt-img srcset="/_ipx/_/transparent-logo.svg 1x, /_ipx/_/transparent-logo.svg 2x" class="h-7 dark:hidden" src="/_ipx/_/transparent-logo.svg"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" data-nuxt-img srcset="/_ipx/_/transparent-logo.svg 1x, /_ipx/_/transparent-logo.svg 2x" class="hidden h-7 dark:block" src="/_ipx/_/transparent-logo.svg"><span class="ml-3 self-center font-bold">Cyber Nachos</span></a></div><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 md:hidden" type="button" aria-haspopup="dialog" aria-expanded="false" data-state="closed"><!--[--><span class="iconify i-lucide:menu" aria-hidden="true" style="font-size:18px;"></span><!--]--></button><!----><!--]--><!--]--><!----><nav aria-label="Main" data-orientation="horizontal" dir="ltr" data-radix-navigation-menu class="relative z-10 max-w-max items-center justify-center hidden flex-1 lg:flex"><!--[--><!--[--><div style="position:relative;"><ul class="group flex flex-1 list-none items-center justify-center gap-x-1" data-orientation="horizontal"><!--[--><!--]--></ul></div><!--]--><div class="absolute left-0 top-full flex justify-center"><!----></div><!--]--></nav><div class="flex flex-1 justify-end gap-2"><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap text-sm ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent px-4 py-2 h-8 w-full self-center rounded-md pr-1.5 font-normal text-muted-foreground hover:text-accent-foreground md:w-40 lg:w-64"><!--[--><span class="mr-auto overflow-hidden">Search...</span><kbd class="pointer-events-none inline-flex h-5 select-none items-center gap-1 rounded border border-border bg-muted font-sans font-medium min-h-5 text-[11px] h-5 px-1 ml-auto hidden md:block"><!--[--><span class="text-xs">⌘</span>K <!--]--></kbd><!--]--></button><!--]--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><div class="flex"><!----><!--[--><!--[--><!--[--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls data-state="closed"><!--[--><span class="iconify i-lucide:paintbrush" aria-hidden="true" style="font-size:16px;"></span><!--]--></button><!----><!--]--><!--]--><!--]--><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10"><!--[--><span class="iconify i-lucide:sun rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0" aria-hidden="true" style="font-size:18px;"></span><span class="iconify i-lucide:moon absolute rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100" aria-hidden="true" style="font-size:18px;"></span><span class="sr-only">Toggle theme</span><!--]--></button><!--[--><a href="https://github.com/CyberNachos" rel="noopener noreferrer" target="_blank"><button class="items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 flex gap-2"><!--[--><span class="iconify i-lucide:github" aria-hidden="true" style="font-size:18px;"></span><!--]--></button></a><!--]--></div></div></div><!----></header><div class="min-h-screen border-b"><div class="container md:grid-cols-[220px_minmax(0,1fr)] lg:grid-cols-[240px_minmax(0,1fr)] flex-1 items-start px-4 md:grid md:gap-6 md:px-8 lg:gap-10"><aside class="fixed top-[102px] z-30 -ml-2 hidden h-[calc(100vh-3.5rem)] w-full shrink-0 overflow-y-auto md:sticky md:top-[60px] md:block"><div dir="ltr" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px;" class="relative h-full overflow-hidden py-6 pr-6 text-sm md:pr-4" orientation="vertical"><!--[--><!--[--><div data-radix-scroll-area-viewport style="overflow-x:hidden;overflow-y:hidden;" class="size-full rounded-[inherit]" tabindex="0"><div style=""><!--[--><!--[--><!----><!----><!----><ul class="flex flex-col gap-1 py-1 pt-1"><!--[--><li><a href="/cybernachos" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:hand min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Cyber Nachos</span><!--[--><!--]--><!--]--></a></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Published Products</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/published/cybernachos-gpt" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Cyber Nachos GPT</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Tutorial Isaac Lab</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/tutorial-isaaclab/introduction" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Introduction</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/installation" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:download min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Installation Guide</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/getting-started" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:flag min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Getting Started</span><!--[--><!--]--><!--]--></a></li><li><a href="/tutorial-isaaclab/enable-fluid" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-material-symbols:water-drop-outline min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Enabling fluid simulation</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Published Research Papers</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a href="/published-research/real-time-dexterous" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Real-time Dexterous Telemanipulation</span><!--[--><!--]--><!--]--></a></li><li><a href="/published-research/hallucination" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:brain min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Hallucination Prevention in LLMs</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><li><div><!--[--><div class="h-8 mt-2 flex items-center gap-2 rounded-md px-2 text-xs font-semibold text-foreground/70 outline-none"><!--[--><!----><span class="truncate text-nowrap">Model &amp; Dataset Comparisons</span><!--[--><!--]--><!--]--></div><ul class="flex flex-col gap-1 py-1"><!--[--><li><a aria-current="page" href="/model-dataset-comp/robotics-datasets" class="router-link-active router-link-exact-active flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary bg-muted !text-primary h-8"><!--[--><span class="iconify i-lucide:database min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">Robotics Dataset Comparison</span><!--[--><!--]--><!--]--></a></li><li><a href="/model-dataset-comp/robotics-models" class="flex items-center gap-2 rounded-md p-2 text-sm text-foreground/80 hover:bg-muted hover:text-primary h-8"><!--[--><span class="iconify i-lucide:bot min-w-4" aria-hidden="true" style="font-size:16px;"></span><span class="truncate text-nowrap">General-Purpose Robot Models Analysis</span><!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></div></li><!--]--></ul><!--]--><!--]--></div></div><style> /* Hide scrollbars cross-browser and enable momentum scroll for touch devices */ [data-radix-scroll-area-viewport] { scrollbar-width:none; -ms-overflow-style:none; -webkit-overflow-scrolling:touch; } [data-radix-scroll-area-viewport]::-webkit-scrollbar { display:none; } </style><!--]--><!----><!----><!--]--></div></aside><!--[--><main class="lg:grid lg:grid-cols-[1fr_220px] lg:gap-14 lg:py-8 relative py-6"><div class="mx-auto w-full min-w-0"><!----><div class="mb-6"><h1 class="scroll-m-20 text-4xl font-extrabold tracking-tight lg:text-5xl"><!--[-->Robotics Dataset Comparison<!--]--></h1><p class="pt-1 text-lg text-muted-foreground"></p><!----><!----></div><div class="docs-content"><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[-->Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table that highlights key characteristics, and second, detailed descriptions of each dataset&#39;s scope, technical features, advantages, and disadvantages.<!--]--></p><h2 id="summary-table" class="scroll-m-20 border-b pb-2 text-3xl font-semibold tracking-tight transition-colors [&amp;:not(:first-child)]:mt-10"><a href="#summary-table"><!--[-->Summary Table<!--]--></a></h2><div class="w-full overflow-y-auto [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6"><table class="w-full"><!--[--><thead><!--[--><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Dataset / Framework<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Scale &amp; Modalities<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Key Advantages<!--]--></strong><!--]--></th><th class="border px-4 py-2 text-left font-bold [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[--><strong class="font-semibold"><!--[-->Key Disadvantages<!--]--></strong><!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->1. <strong class="font-semibold"><!--[-->LeRobot<!--]--></strong> <br> (<a href="https://github.com/huggingface/lerobot" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->GitHub<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->End-to-end learning with community support; integrated simulation environments.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Complex setup; may require substantial computing and sensor calibration.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->2. <strong class="font-semibold"><!--[-->Open X-Embodiment<!--]--></strong> <br> (<a href="https://robotics-transformer-x.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Massive diversity enabling cross-robot transfer and positive knowledge sharing.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Heterogeneous quality and potential standardization issues across varied sources.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->3. <strong class="font-semibold"><!--[-->DROID<!--]--></strong> <br> (<a href="https://droid-dataset.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->In-the-wild robot manipulation for robust imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Diverse, large-scale manipulation data that improves policy robustness.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Mostly limited to manipulation with a specific hardware setup; less diversity in task types.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->4. <strong class="font-semibold"><!--[-->RoboTurk<!--]--></strong> <br> (<a href="https://roboturk.stanford.edu/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Leverages non-expert, scalable human demonstrations; supports collaborative tasks.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Variation in demonstration quality and potential limits in scale compared to fully automated data collection.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->5. <strong class="font-semibold"><!--[-->MIME<!--]--></strong> <br> (<a href="https://sites.google.com/view/mimedataset/dataset?authuser=0" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Google Sites<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Imitation learning for robot manipulation using human demonstrations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Focus on high-quality manipulation trajectories; well-suited for imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->May be smaller in scale and less diverse than some large-scale multi-robot datasets.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->6. <strong class="font-semibold"><!--[-->Meta-World<!--]--></strong> <br> (<a href="https://meta-world.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Benchmark for multi-task and meta-reinforcement learning in simulation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->50 distinct simulated manipulation environments; task variations with visual observations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Standardized benchmark for meta-RL; structured for evaluating generalization.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Limited to simulation and may not capture the full variability of real-world settings.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->7. <strong class="font-semibold"><!--[-->RoboNet<!--]--></strong> <br> (<a href="https://www.robonet.wiki/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Open database of real robotic experience for manipulation tasks across multiple platforms.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->~15M video frames, collected from 7 robot platforms with diverse camera viewpoints.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale, multi-platform real-world data that facilitates cross-robot generalization.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very high storage and processing requirements; complex data integration.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->8. <strong class="font-semibold"><!--[-->RoboSet<!--]--></strong> <br> (<a href="https://robopen.github.io/roboset/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-task dataset for household (kitchen) manipulation tasks, including language instructions.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Rich, multi-modal data in realistic home environments; supports language-guided sequencing.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Domain-specific (largely kitchens); may not generalize to non-domestic scenarios.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->9. <strong class="font-semibold"><!--[-->BridgeData V2<!--]--></strong> <br> (<a href="https://rail-berkeley.github.io/bridgedata/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale robotic manipulation across diverse environments and skills with language annotations.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->10. <strong class="font-semibold"><!--[-->RT-1<!--]--></strong> <br> (<a href="https://robotics-transformer1.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Real-world imitation learning for multi-task manipulation using transformer architectures.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Outstanding generalization and performance on diverse tasks; scalable transformer model.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->High training and computational requirements; system complexity may be a barrier.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->11. <strong class="font-semibold"><!--[-->Dobb·E<!--]--></strong> <br> (<a href="https://dobb-e.com/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Framework for home robotics: learning household manipulation tasks quickly in real homes.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->“HoNY” dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30 fps; also includes hardware (the “Stick”) for data collection.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Cost-effective, rapid task learning with real household data; designed for generalist home robots.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->12. <strong class="font-semibold"><!--[-->RH20T<!--]--></strong> <br> (<a href="https://rh20t.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Very large and complex; requires significant computational and storage resources; complex data processing pipeline.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->13. <strong class="font-semibold"><!--[-->BC-Z<!--]--></strong> <br> (<a href="https://sites.google.com/view/bc-z/home?pli=1" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Large-scale behavior cloning for robotic manipulation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->14. <strong class="font-semibold"><!--[-->MT-Opt<!--]--></strong> <br> (<a href="https://karolhausman.github.io/mt-opt/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Multi-task reinforcement learning at scale across many manipulation skills.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->15. <strong class="font-semibold"><!--[-->VIMA<!--]--></strong> <br> (<a href="https://vimalabs.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->General robot manipulation via multimodal prompts (combining language and vision) for unified task specification.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Unified formulation that “prompts” the robot to perform diverse tasks; highly scalable and sample-efficient.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation.<!--]--></td><!--]--></tr><tr class="m-0 border-t p-0 even:bg-muted/50"><!--[--><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->16. <strong class="font-semibold"><!--[-->SPOC<!--]--></strong> <br> (<a href="https://spoc-robot.github.io/" rel="nofollow" class="font-semibold underline underline-offset-4"><!--[-->Website<!--]--></a>)<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world).<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info.<!--]--></td><td class="border px-4 py-2 text-left [&amp;[align=center]]:text-center [&amp;[align=right]]:text-right"><!--[-->RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios.<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table></div><hr class="[&amp;:not(:first-child)]:mt-6"><h2 id="detailed-comparison" class="scroll-m-20 border-b pb-2 text-3xl font-semibold tracking-tight transition-colors [&amp;:not(:first-child)]:mt-10"><a href="#detailed-comparison"><!--[-->Detailed Comparison<!--]--></a></h2><h3 id="_1-lerobot" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_1-lerobot"><!--[-->1. LeRobot<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
LeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Built in PyTorch with modular dataset classes that support multi-frame temporal sampling.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Community-driven with active contributions and hosted on Hugging Face.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Facilitates rapid prototyping in robotics with an accessible codebase.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_2-open-x-embodiment" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_2-open-x-embodiment"><!--[-->2. Open X-Embodiment<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A collaborative effort pooling robot data from 21 institutions, it is aimed at training “generalist” policies across 22 different robot embodiments.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Aggregates 1M+ trajectories from diverse robots and tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Supports learning via transformer-based architectures that can generalize across different embodiments.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Unmatched diversity, which is ideal for studying cross-robot transfer.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Large scale increases the potential for generalization.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_3-droid" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_3-droid"><!--[-->3. DROID<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Focused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Multi-camera views (including wrist and exterior images) enable rich visual inputs.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Large, diverse dataset that significantly boosts policy performance and robustness.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Extensive coverage of real-world scenarios.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Being collected with a specific hardware platform, its applicability to other robots may be limited.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_4-roboturk" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_4-roboturk"><!--[-->4. RoboTurk<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset).<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes system features for low-latency teleoperation and human-in-the-loop interventions.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Proven effectiveness in enabling imitation learning on challenging tasks.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->The quality of demonstrations may vary due to differences in human teleoperation skills.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_5-mime" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_5-mime"><!--[-->5. MIME<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
MIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Focused on detailed manipulation tasks, making it ideal for imitation learning studies.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Generally smaller in scale compared to some of the largest datasets; might offer limited diversity.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_6-meta-world" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_6-meta-world"><!--[-->6. Meta-World<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Structured environments with varying goal positions and task variations to test generalization.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_7-robonet" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_7-robonet"><!--[-->7. RoboNet<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains over 15M video frames and data from multiple camera viewpoints.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Offers vast amounts of real-world data to study generalization across different robot hardware.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Requires heavy storage and processing; integrating multi-platform data can be challenging.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_8-roboset" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_8-roboset"><!--[-->8. RoboSet<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
A dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Rich multi-modal information (visual + language) supports language-guided robotic learning.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_9-bridgedata-v2" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_9-bridgedata-v2"><!--[-->9. BridgeData V2<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Designed to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Approximately 60K trajectories with multi-view RGB (and some depth) data.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes both teleoperated and scripted demonstrations.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->High diversity in environments and tasks; strong support for language-conditioned policy learning.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_10-rt-1" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_10-rt-1"><!--[-->10. RT-1<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Over 130K episodes covering more than 700 tasks collected from 13 robots.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Utilizes vision and natural language inputs to produce discretized action tokens.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates superior performance and generalization, including sim-to-real transfer.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Scalability through high-capacity transformer models.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demands extensive data, compute, and engineering expertise; system complexity is high.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_11-dobbe" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_11-dobbe"><!--[-->11. Dobb·E<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
Dobb·E focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->“HoNY” dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30 fps).<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Includes a low-cost hardware “Stick” for demonstration collection.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Cost-effective and designed for rapid task learning in domestic environments.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates strong real-world applicability in home settings.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_12-rh20t" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_12-rh20t"><!--[-->12. RH20T<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
RH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Detailed synchronization and calibration across multiple sensors.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Supports research on contact-rich and dexterous manipulation.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_13-bc-z" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_13-bc-z"><!--[-->13. BC-Z<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
BC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Serves as a standardized resource for evaluating behavior cloning algorithms.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_14-mt-opt" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_14-mt-opt"><!--[-->14. MT-Opt<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
MT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Effective at sharing experience across tasks, significantly boosting performance on rare tasks.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Demonstrates both zero-shot and rapid fine-tuning capabilities.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_15-vima" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_15-vima"><!--[-->15. VIMA<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
VIMA presents a novel formulation in which diverse robot manipulation tasks are “prompted” via interleaved language and visual tokens, unifying task specification.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Allows integration of various forms of task instructions (text + image).<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment.<!--]--></li><!--]--></ul><hr class="[&amp;:not(:first-child)]:mt-6"><h3 id="_16-spoc" class="scroll-m-20 text-2xl font-semibold tracking-tight [&amp;:not(:first-child)]:mt-8"><a href="#_16-spoc"><!--[-->16. SPOC<!--]--></a></h3><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Scope &amp; Application:<!--]--></strong><br>
SPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation.<!--]--></p><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Technical Features:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Advantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities.<!--]--></li><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->Trains entirely in simulation and transfers effectively.<!--]--></li><!--]--></ul><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[--><strong class="font-semibold"><!--[-->Disadvantages:<!--]--></strong><!--]--></p><ul class="ml-6 list-disc [&amp;:not(:first-child)]:mt-6 [&amp;:not(:last-child)]:mb-6 [&amp;&gt;li:not(:first-child)]:mt-2"><!--[--><li class="[&amp;&gt;ol]:!mt-2 [&amp;&gt;ul]:!mt-2"><!--[-->RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios.<!--]--></li><!--]--></ul></div><div class="mt-16"><div class="mb-6 flex w-full items-center justify-between"><!----><div class="w-fit"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 underline-offset-4 hover:underline h-10 px-4 py-2 text-sm font-semibold text-primary"><!--[--><div class="flex items-center gap-2"><span class="iconify i-lucide:arrow-up" aria-hidden="true" style="font-size:16px;"></span><span>Back to Top</span></div><!--]--></button></div></div><div class="border-t pt-6 lg:flex lg:flex-row"><a href="/published-research/hallucination" class="basis-1/3"><div class="mb-4 space-y-2 rounded-lg border p-4 transition-all hover:bg-muted/50"><div class="flex flex-row gap-3"><div class="flex size-6 min-w-6"><span class="iconify i-lucide:arrow-left mx-auto self-center" aria-hidden="true" style="font-size:20px;"></span></div><span class="space-y-2 self-center"><div class="text-lg font-semibold">Hallucination Prevention in LLMs</div><div class="text-sm text-muted-foreground">Zero-Resource Hallucination Prevention for Large Language Models</div></span><!----></div></div></a><span class="flex-1"></span><a href="/model-dataset-comp/robotics-models" class="basis-1/3"><div class="mb-4 space-y-2 rounded-lg border p-4 transition-all hover:bg-muted/50"><div class="flex flex-row gap-3"><!----><span class="space-y-2 self-center"><div class="text-lg font-semibold">General-Purpose Robot Models Analysis</div><div class="text-sm text-muted-foreground">Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.</div></span><div class="ml-auto flex size-6 min-w-6"><span class="iconify i-lucide:arrow-right mx-auto self-center" aria-hidden="true" style="font-size:20px;"></span></div></div></div></a></div><div class="flex"><!----></div></div></div><div class="hidden text-sm lg:block"><div class="sticky top-[90px] h-[calc(100vh-3.5rem)] overflow-hidden"><div dir="ltr" style="position:relative;--radix-scroll-area-corner-width:0px;--radix-scroll-area-corner-height:0px;" class="relative overflow-hidden z-30 hidden h-[calc(100vh-6.5rem)] overflow-y-auto md:block lg:block" orientation="vertical"><!--[--><!--[--><div data-radix-scroll-area-viewport style="overflow-x:hidden;overflow-y:hidden;" class="size-full rounded-[inherit]" tabindex="0"><div style=""><!--[--><!--[--><div class="flex h-[calc(100vh-6.5rem)] flex-col"><div><p class="mb-2 text-base font-semibold">On This Page</p><ul class=""><!--[--><li class="pt-2"><a href="#summary-table" class="text-muted-foreground transition-all hover:text-primary">Summary Table</a><!----></li><li class="pt-2"><a href="#detailed-comparison" class="text-muted-foreground transition-all hover:text-primary">Detailed Comparison</a><ul class="pl-4"><!--[--><li class="pt-2"><a href="#_1-lerobot" class="text-muted-foreground transition-all hover:text-primary">1. LeRobot</a><!----></li><li class="pt-2"><a href="#_2-open-x-embodiment" class="text-muted-foreground transition-all hover:text-primary">2. Open X-Embodiment</a><!----></li><li class="pt-2"><a href="#_3-droid" class="text-muted-foreground transition-all hover:text-primary">3. DROID</a><!----></li><li class="pt-2"><a href="#_4-roboturk" class="text-muted-foreground transition-all hover:text-primary">4. RoboTurk</a><!----></li><li class="pt-2"><a href="#_5-mime" class="text-muted-foreground transition-all hover:text-primary">5. MIME</a><!----></li><li class="pt-2"><a href="#_6-meta-world" class="text-muted-foreground transition-all hover:text-primary">6. Meta-World</a><!----></li><li class="pt-2"><a href="#_7-robonet" class="text-muted-foreground transition-all hover:text-primary">7. RoboNet</a><!----></li><li class="pt-2"><a href="#_8-roboset" class="text-muted-foreground transition-all hover:text-primary">8. RoboSet</a><!----></li><li class="pt-2"><a href="#_9-bridgedata-v2" class="text-muted-foreground transition-all hover:text-primary">9. BridgeData V2</a><!----></li><li class="pt-2"><a href="#_10-rt-1" class="text-muted-foreground transition-all hover:text-primary">10. RT-1</a><!----></li><li class="pt-2"><a href="#_11-dobbe" class="text-muted-foreground transition-all hover:text-primary">11. Dobb·E</a><!----></li><li class="pt-2"><a href="#_12-rh20t" class="text-muted-foreground transition-all hover:text-primary">12. RH20T</a><!----></li><li class="pt-2"><a href="#_13-bc-z" class="text-muted-foreground transition-all hover:text-primary">13. BC-Z</a><!----></li><li class="pt-2"><a href="#_14-mt-opt" class="text-muted-foreground transition-all hover:text-primary">14. MT-Opt</a><!----></li><li class="pt-2"><a href="#_15-vima" class="text-muted-foreground transition-all hover:text-primary">15. VIMA</a><!----></li><li class="pt-2"><a href="#_16-spoc" class="text-muted-foreground transition-all hover:text-primary">16. SPOC</a><!----></li><!--]--></ul></li><!--]--></ul><div class="pt-5 text-muted-foreground"><!--[--><!--]--></div></div><div class="flex-grow"></div><!----></div><!--]--><!--]--></div></div><style> /* Hide scrollbars cross-browser and enable momentum scroll for touch devices */ [data-radix-scroll-area-viewport] { scrollbar-width:none; -ms-overflow-style:none; -webkit-overflow-scrolling:touch; } [data-radix-scroll-area-viewport]::-webkit-scrollbar { display:none; } </style><!--]--><!----><!----><!--]--></div></div></div></main><!--]--></div></div><!--[--><!--[--><!--[--><!--]--><div role="region" aria-label="Notifications (F8)" tabindex="-1" style="pointer-events:none;"><!--[--><!----><ol tabindex="-1" class="fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]"><!--[--><!--]--></ol><!----><!--]--></div><!--]--><!--]--><footer class="py-6 text-muted-foreground md:px-8 md:py-0"><div class="container flex flex-col items-center justify-between gap-2 md:h-24 md:flex-row"><!--[--><div class="text-sm"><p class="leading-7 [&amp;:not(:first-child)]:mt-6"><!--[-->Copyright © 2025<!--]--></p></div><!--]--><span class="flex-1"></span><!--[--><a href="https://github.com/CyberNachos" rel="noopener noreferrer" target="_blank"><button class="items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground size-10 flex gap-2"><!--[--><span class="iconify i-lucide:github" aria-hidden="true" style="font-size:20px;"></span><!----><!--]--></button></a><!--]--></div></footer><!--]--></div><div id="teleports"></div>
<script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/model-dataset-comp/robotics-datasets/_payload.json?57dc7828-7b5d-44e7-97ea-45198066b06c">[{"state":1,"once":2095,"_errors":2096,"serverRendered":5,"path":11,"prerenderedAt":2099},["Reactive",2],{"$scolor-mode":3,"$sdd-pages":7,"$sdd-surrounds":2018,"$sdd-globals":2039,"$sdd-navigation":2041,"$sdocs-collapsed-map":2086,"$ssite-config":2087},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,["ShallowRef",8],["ShallowReactive",9],{"/model-dataset-comp/robotics-datasets":10},{"_path":11,"_dir":12,"_draft":6,"_partial":6,"_locale":13,"title":14,"description":15,"icon":16,"body":17,"_type":2011,"_id":2012,"_source":2013,"_file":2014,"_stem":2015,"_extension":2016,"layout":2017},"/model-dataset-comp/robotics-datasets","model-dataset-comp","","Robotics Dataset Comparison","A comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks, including LeRobot, Open X-Embodiment, DROID, RoboTurk, MIME, Meta-World, RoboNet, RoboSet, BridgeData V2, RT-1, Dobb·E, RH20T, BC-Z, MT-Opt, VIMA, and SPOC.","lucide:database",{"type":18,"children":19,"toc":1988},"root",[20,28,35,823,827,833,840,853,861,876,884,897,905,913,916,922,934,941,954,961,974,981,989,992,998,1010,1017,1030,1037,1050,1057,1065,1068,1074,1086,1093,1106,1113,1126,1133,1141,1144,1150,1162,1169,1177,1184,1192,1199,1207,1210,1216,1228,1235,1243,1250,1258,1265,1273,1276,1282,1294,1301,1309,1316,1324,1331,1339,1342,1348,1360,1367,1375,1382,1390,1397,1405,1408,1414,1426,1433,1446,1453,1461,1468,1476,1479,1485,1497,1504,1517,1524,1537,1544,1552,1555,1561,1573,1580,1593,1600,1613,1620,1628,1631,1637,1649,1656,1669,1676,1689,1696,1704,1707,1713,1725,1732,1740,1747,1755,1762,1770,1773,1779,1791,1798,1806,1813,1826,1833,1841,1844,1850,1862,1869,1877,1884,1897,1904,1912,1915,1921,1933,1940,1953,1960,1973,1980],{"type":21,"tag":22,"props":23,"children":24},"element","p",{},[25],{"type":26,"value":27},"text","Below is a comprehensive overview and comparative analysis of 16 mainstream robotics datasets and frameworks. The report is organized into two parts: first, a summary table that highlights key characteristics, and second, detailed descriptions of each dataset's scope, technical features, advantages, and disadvantages.",{"type":21,"tag":29,"props":30,"children":32},"h2",{"id":31},"summary-table",[33],{"type":26,"value":34},"Summary Table",{"type":21,"tag":36,"props":37,"children":38},"table",{},[39,89],{"type":21,"tag":40,"props":41,"children":42},"thead",{},[43],{"type":21,"tag":44,"props":45,"children":46},"tr",{},[47,57,65,73,81],{"type":21,"tag":48,"props":49,"children":50},"th",{},[51],{"type":21,"tag":52,"props":53,"children":54},"strong",{},[55],{"type":26,"value":56},"Dataset / Framework",{"type":21,"tag":48,"props":58,"children":59},{},[60],{"type":21,"tag":52,"props":61,"children":62},{},[63],{"type":26,"value":64},"Scope & Application",{"type":21,"tag":48,"props":66,"children":67},{},[68],{"type":21,"tag":52,"props":69,"children":70},{},[71],{"type":26,"value":72},"Scale & Modalities",{"type":21,"tag":48,"props":74,"children":75},{},[76],{"type":21,"tag":52,"props":77,"children":78},{},[79],{"type":26,"value":80},"Key Advantages",{"type":21,"tag":48,"props":82,"children":83},{},[84],{"type":21,"tag":52,"props":85,"children":86},{},[87],{"type":26,"value":88},"Key Disadvantages",{"type":21,"tag":90,"props":91,"children":92},"tbody",{},[93,146,192,237,282,328,373,418,463,508,553,598,643,688,733,778],{"type":21,"tag":44,"props":94,"children":95},{},[96,126,131,136,141],{"type":21,"tag":97,"props":98,"children":99},"td",{},[100,102,107,109,113,115,124],{"type":26,"value":101},"1. ",{"type":21,"tag":52,"props":103,"children":104},{},[105],{"type":26,"value":106},"LeRobot",{"type":26,"value":108}," ",{"type":21,"tag":110,"props":111,"children":112},"br",{},[],{"type":26,"value":114}," (",{"type":21,"tag":116,"props":117,"children":121},"a",{"href":118,"rel":119},"https://github.com/huggingface/lerobot",[120],"nofollow",[122],{"type":26,"value":123},"GitHub",{"type":26,"value":125},")",{"type":21,"tag":97,"props":127,"children":128},{},[129],{"type":26,"value":130},"Real-world robotics for imitation and reinforcement learning; supports both simulation and physical robots.",{"type":21,"tag":97,"props":132,"children":133},{},[134],{"type":26,"value":135},"Pretrained models and demo datasets; primarily visual and robot state data with temporal (multi-frame) context.",{"type":21,"tag":97,"props":137,"children":138},{},[139],{"type":26,"value":140},"End-to-end learning with community support; integrated simulation environments.",{"type":21,"tag":97,"props":142,"children":143},{},[144],{"type":26,"value":145},"Complex setup; may require substantial computing and sensor calibration.",{"type":21,"tag":44,"props":147,"children":148},{},[149,172,177,182,187],{"type":21,"tag":97,"props":150,"children":151},{},[152,154,159,160,163,164,171],{"type":26,"value":153},"2. ",{"type":21,"tag":52,"props":155,"children":156},{},[157],{"type":26,"value":158},"Open X-Embodiment",{"type":26,"value":108},{"type":21,"tag":110,"props":161,"children":162},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":165,"children":168},{"href":166,"rel":167},"https://robotics-transformer-x.github.io/",[120],[169],{"type":26,"value":170},"Website",{"type":26,"value":125},{"type":21,"tag":97,"props":173,"children":174},{},[175],{"type":26,"value":176},"Large-scale, multi-embodiment robotic manipulation; pooling data from many institutions.",{"type":21,"tag":97,"props":178,"children":179},{},[180],{"type":26,"value":181},"1M+ trajectories spanning 22 robot embodiments; heterogeneous real-world data.",{"type":21,"tag":97,"props":183,"children":184},{},[185],{"type":26,"value":186},"Massive diversity enabling cross-robot transfer and positive knowledge sharing.",{"type":21,"tag":97,"props":188,"children":189},{},[190],{"type":26,"value":191},"Heterogeneous quality and potential standardization issues across varied sources.",{"type":21,"tag":44,"props":193,"children":194},{},[195,217,222,227,232],{"type":21,"tag":97,"props":196,"children":197},{},[198,200,205,206,209,210,216],{"type":26,"value":199},"3. ",{"type":21,"tag":52,"props":201,"children":202},{},[203],{"type":26,"value":204},"DROID",{"type":26,"value":108},{"type":21,"tag":110,"props":207,"children":208},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":211,"children":214},{"href":212,"rel":213},"https://droid-dataset.github.io/",[120],[215],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":218,"children":219},{},[220],{"type":26,"value":221},"In-the-wild robot manipulation for robust imitation learning.",{"type":21,"tag":97,"props":223,"children":224},{},[225],{"type":26,"value":226},"76K demonstration trajectories (~350 hours) recorded with Franka Panda arms; multiple camera viewpoints.",{"type":21,"tag":97,"props":228,"children":229},{},[230],{"type":26,"value":231},"Diverse, large-scale manipulation data that improves policy robustness.",{"type":21,"tag":97,"props":233,"children":234},{},[235],{"type":26,"value":236},"Mostly limited to manipulation with a specific hardware setup; less diversity in task types.",{"type":21,"tag":44,"props":238,"children":239},{},[240,262,267,272,277],{"type":21,"tag":97,"props":241,"children":242},{},[243,245,250,251,254,255,261],{"type":26,"value":244},"4. ",{"type":21,"tag":52,"props":246,"children":247},{},[248],{"type":26,"value":249},"RoboTurk",{"type":26,"value":108},{"type":21,"tag":110,"props":252,"children":253},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":256,"children":259},{"href":257,"rel":258},"https://roboturk.stanford.edu/",[120],[260],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":263,"children":264},{},[265],{"type":26,"value":266},"Crowdsourced robotic skill learning via teleoperation; real-world demonstration collection.",{"type":21,"tag":97,"props":268,"children":269},{},[270],{"type":26,"value":271},"Pilot and real-world datasets (hundreds to thousands of demos, several hours of data) from teleoperated sessions.",{"type":21,"tag":97,"props":273,"children":274},{},[275],{"type":26,"value":276},"Leverages non-expert, scalable human demonstrations; supports collaborative tasks.",{"type":21,"tag":97,"props":278,"children":279},{},[280],{"type":26,"value":281},"Variation in demonstration quality and potential limits in scale compared to fully automated data collection.",{"type":21,"tag":44,"props":283,"children":284},{},[285,308,313,318,323],{"type":21,"tag":97,"props":286,"children":287},{},[288,290,295,296,299,300,307],{"type":26,"value":289},"5. ",{"type":21,"tag":52,"props":291,"children":292},{},[293],{"type":26,"value":294},"MIME",{"type":26,"value":108},{"type":21,"tag":110,"props":297,"children":298},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":301,"children":304},{"href":302,"rel":303},"https://sites.google.com/view/mimedataset/dataset?authuser=0",[120],[305],{"type":26,"value":306},"Google Sites",{"type":26,"value":125},{"type":21,"tag":97,"props":309,"children":310},{},[311],{"type":26,"value":312},"Imitation learning for robot manipulation using human demonstrations.",{"type":21,"tag":97,"props":314,"children":315},{},[316],{"type":26,"value":317},"Multi-modal data (visual, robot states, actions) collected via teleoperation; moderate number of trajectories.",{"type":21,"tag":97,"props":319,"children":320},{},[321],{"type":26,"value":322},"Focus on high-quality manipulation trajectories; well-suited for imitation learning.",{"type":21,"tag":97,"props":324,"children":325},{},[326],{"type":26,"value":327},"May be smaller in scale and less diverse than some large-scale multi-robot datasets.",{"type":21,"tag":44,"props":329,"children":330},{},[331,353,358,363,368],{"type":21,"tag":97,"props":332,"children":333},{},[334,336,341,342,345,346,352],{"type":26,"value":335},"6. ",{"type":21,"tag":52,"props":337,"children":338},{},[339],{"type":26,"value":340},"Meta-World",{"type":26,"value":108},{"type":21,"tag":110,"props":343,"children":344},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":347,"children":350},{"href":348,"rel":349},"https://meta-world.github.io/",[120],[351],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":354,"children":355},{},[356],{"type":26,"value":357},"Benchmark for multi-task and meta-reinforcement learning in simulation.",{"type":21,"tag":97,"props":359,"children":360},{},[361],{"type":26,"value":362},"50 distinct simulated manipulation environments; task variations with visual observations.",{"type":21,"tag":97,"props":364,"children":365},{},[366],{"type":26,"value":367},"Standardized benchmark for meta-RL; structured for evaluating generalization.",{"type":21,"tag":97,"props":369,"children":370},{},[371],{"type":26,"value":372},"Limited to simulation and may not capture the full variability of real-world settings.",{"type":21,"tag":44,"props":374,"children":375},{},[376,398,403,408,413],{"type":21,"tag":97,"props":377,"children":378},{},[379,381,386,387,390,391,397],{"type":26,"value":380},"7. ",{"type":21,"tag":52,"props":382,"children":383},{},[384],{"type":26,"value":385},"RoboNet",{"type":26,"value":108},{"type":21,"tag":110,"props":388,"children":389},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":392,"children":395},{"href":393,"rel":394},"https://www.robonet.wiki/",[120],[396],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":399,"children":400},{},[401],{"type":26,"value":402},"Open database of real robotic experience for manipulation tasks across multiple platforms.",{"type":21,"tag":97,"props":404,"children":405},{},[406],{"type":26,"value":407},"~15M video frames, collected from 7 robot platforms with diverse camera viewpoints.",{"type":21,"tag":97,"props":409,"children":410},{},[411],{"type":26,"value":412},"Large-scale, multi-platform real-world data that facilitates cross-robot generalization.",{"type":21,"tag":97,"props":414,"children":415},{},[416],{"type":26,"value":417},"Very high storage and processing requirements; complex data integration.",{"type":21,"tag":44,"props":419,"children":420},{},[421,443,448,453,458],{"type":21,"tag":97,"props":422,"children":423},{},[424,426,431,432,435,436,442],{"type":26,"value":425},"8. ",{"type":21,"tag":52,"props":427,"children":428},{},[429],{"type":26,"value":430},"RoboSet",{"type":26,"value":108},{"type":21,"tag":110,"props":433,"children":434},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":437,"children":440},{"href":438,"rel":439},"https://robopen.github.io/roboset/",[120],[441],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":444,"children":445},{},[446],{"type":26,"value":447},"Multi-task dataset for household (kitchen) manipulation tasks, including language instructions.",{"type":21,"tag":97,"props":449,"children":450},{},[451],{"type":26,"value":452},"28,500 trajectories (mix of ~9.5K teleop and ~19K kinesthetic demos), recorded with 4 camera views per frame.",{"type":21,"tag":97,"props":454,"children":455},{},[456],{"type":26,"value":457},"Rich, multi-modal data in realistic home environments; supports language-guided sequencing.",{"type":21,"tag":97,"props":459,"children":460},{},[461],{"type":26,"value":462},"Domain-specific (largely kitchens); may not generalize to non-domestic scenarios.",{"type":21,"tag":44,"props":464,"children":465},{},[466,488,493,498,503],{"type":21,"tag":97,"props":467,"children":468},{},[469,471,476,477,480,481,487],{"type":26,"value":470},"9. ",{"type":21,"tag":52,"props":472,"children":473},{},[474],{"type":26,"value":475},"BridgeData V2",{"type":26,"value":108},{"type":21,"tag":110,"props":478,"children":479},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":482,"children":485},{"href":483,"rel":484},"https://rail-berkeley.github.io/bridgedata/",[120],[486],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":489,"children":490},{},[491],{"type":26,"value":492},"Large-scale robotic manipulation across diverse environments and skills with language annotations.",{"type":21,"tag":97,"props":494,"children":495},{},[496],{"type":26,"value":497},"~60K trajectories, 24 environments, 13 skills; includes multi-view (fixed, wrist, randomized) RGB (and depth) data plus natural language.",{"type":21,"tag":97,"props":499,"children":500},{},[501],{"type":26,"value":502},"Very diverse and large-scale, ideal for cross-domain generalization and multi-modal learning.",{"type":21,"tag":97,"props":504,"children":505},{},[506],{"type":26,"value":507},"Often collected with a specific robot (e.g. WidowX); complex setup and annotation consistency challenges.",{"type":21,"tag":44,"props":509,"children":510},{},[511,533,538,543,548],{"type":21,"tag":97,"props":512,"children":513},{},[514,516,521,522,525,526,532],{"type":26,"value":515},"10. ",{"type":21,"tag":52,"props":517,"children":518},{},[519],{"type":26,"value":520},"RT-1",{"type":26,"value":108},{"type":21,"tag":110,"props":523,"children":524},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":527,"children":530},{"href":528,"rel":529},"https://robotics-transformer1.github.io/",[120],[531],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":534,"children":535},{},[536],{"type":26,"value":537},"Real-world imitation learning for multi-task manipulation using transformer architectures.",{"type":21,"tag":97,"props":539,"children":540},{},[541],{"type":26,"value":542},"Over 130K episodes covering 700+ tasks from 13 robots; uses visual and language inputs for closed-loop control.",{"type":21,"tag":97,"props":544,"children":545},{},[546],{"type":26,"value":547},"Outstanding generalization and performance on diverse tasks; scalable transformer model.",{"type":21,"tag":97,"props":549,"children":550},{},[551],{"type":26,"value":552},"High training and computational requirements; system complexity may be a barrier.",{"type":21,"tag":44,"props":554,"children":555},{},[556,578,583,588,593],{"type":21,"tag":97,"props":557,"children":558},{},[559,561,566,567,570,571,577],{"type":26,"value":560},"11. ",{"type":21,"tag":52,"props":562,"children":563},{},[564],{"type":26,"value":565},"Dobb·E",{"type":26,"value":108},{"type":21,"tag":110,"props":568,"children":569},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":572,"children":575},{"href":573,"rel":574},"https://dobb-e.com/",[120],[576],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":579,"children":580},{},[581],{"type":26,"value":582},"Framework for home robotics: learning household manipulation tasks quickly in real homes.",{"type":21,"tag":97,"props":584,"children":585},{},[586],{"type":26,"value":587},"“HoNY” dataset: 13 hours from 22 NYC homes, 5,620 trajectories, RGB and depth at 30 fps; also includes hardware (the “Stick”) for data collection.",{"type":21,"tag":97,"props":589,"children":590},{},[591],{"type":26,"value":592},"Cost-effective, rapid task learning with real household data; designed for generalist home robots.",{"type":21,"tag":97,"props":594,"children":595},{},[596],{"type":26,"value":597},"Domain-specific to domestic settings; quality and consistency can vary with non-expert demonstrations.",{"type":21,"tag":44,"props":599,"children":600},{},[601,623,628,633,638],{"type":21,"tag":97,"props":602,"children":603},{},[604,606,611,612,615,616,622],{"type":26,"value":605},"12. ",{"type":21,"tag":52,"props":607,"children":608},{},[609],{"type":26,"value":610},"RH20T",{"type":26,"value":108},{"type":21,"tag":110,"props":613,"children":614},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":617,"children":620},{"href":618,"rel":619},"https://rh20t.github.io/",[120],[621],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":624,"children":625},{},[626],{"type":26,"value":627},"Comprehensive dataset for contact-rich, multi-modal robot manipulation tasks in the real world.",{"type":21,"tag":97,"props":629,"children":630},{},[631],{"type":26,"value":632},"Millions of human-robot demonstration pairs; modalities include high-resolution RGB, depth, force/torque, audio, tactile, and high-frequency joint data.",{"type":21,"tag":97,"props":634,"children":635},{},[636],{"type":26,"value":637},"Extremely rich multi-modal data enabling detailed analysis and one-shot imitation learning.",{"type":21,"tag":97,"props":639,"children":640},{},[641],{"type":26,"value":642},"Very large and complex; requires significant computational and storage resources; complex data processing pipeline.",{"type":21,"tag":44,"props":644,"children":645},{},[646,668,673,678,683],{"type":21,"tag":97,"props":647,"children":648},{},[649,651,656,657,660,661,667],{"type":26,"value":650},"13. ",{"type":21,"tag":52,"props":652,"children":653},{},[654],{"type":26,"value":655},"BC-Z",{"type":26,"value":108},{"type":21,"tag":110,"props":658,"children":659},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":662,"children":665},{"href":663,"rel":664},"https://sites.google.com/view/bc-z/home?pli=1",[120],[666],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":669,"children":670},{},[671],{"type":26,"value":672},"Large-scale behavior cloning for robotic manipulation.",{"type":21,"tag":97,"props":674,"children":675},{},[676],{"type":26,"value":677},"(Details are sparser online but BC-Z is designed to support imitation learning with a large number of trajectories.)",{"type":21,"tag":97,"props":679,"children":680},{},[681],{"type":26,"value":682},"Provides a standardized dataset specifically aimed at behavior cloning; useful for benchmarking imitation algorithms.",{"type":21,"tag":97,"props":684,"children":685},{},[686],{"type":26,"value":687},"May offer less diversity outside manipulation tasks and less extensive documentation compared to other datasets.",{"type":21,"tag":44,"props":689,"children":690},{},[691,713,718,723,728],{"type":21,"tag":97,"props":692,"children":693},{},[694,696,701,702,705,706,712],{"type":26,"value":695},"14. ",{"type":21,"tag":52,"props":697,"children":698},{},[699],{"type":26,"value":700},"MT-Opt",{"type":26,"value":108},{"type":21,"tag":110,"props":703,"children":704},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":707,"children":710},{"href":708,"rel":709},"https://karolhausman.github.io/mt-opt/",[120],[711],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":714,"children":715},{},[716],{"type":26,"value":717},"Multi-task reinforcement learning at scale across many manipulation skills.",{"type":21,"tag":97,"props":719,"children":720},{},[721],{"type":26,"value":722},"Data collected from 7 robots over 9,600 robot hours spanning 12 tasks; continuous multi-task RL framework.",{"type":21,"tag":97,"props":724,"children":725},{},[726],{"type":26,"value":727},"Enables simultaneous learning across tasks; improves performance especially on underrepresented skills through shared experience.",{"type":21,"tag":97,"props":729,"children":730},{},[731],{"type":26,"value":732},"Demands large-scale infrastructure and careful task specification; complexity in multi-task coordination.",{"type":21,"tag":44,"props":734,"children":735},{},[736,758,763,768,773],{"type":21,"tag":97,"props":737,"children":738},{},[739,741,746,747,750,751,757],{"type":26,"value":740},"15. ",{"type":21,"tag":52,"props":742,"children":743},{},[744],{"type":26,"value":745},"VIMA",{"type":26,"value":108},{"type":21,"tag":110,"props":748,"children":749},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":752,"children":755},{"href":753,"rel":754},"https://vimalabs.github.io/",[120],[756],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":759,"children":760},{},[761],{"type":26,"value":762},"General robot manipulation via multimodal prompts (combining language and vision) for unified task specification.",{"type":21,"tag":97,"props":764,"children":765},{},[766],{"type":26,"value":767},"Benchmark with thousands of procedurally generated tabletop task instances; uses imitation learning data alongside transformer-based models.",{"type":21,"tag":97,"props":769,"children":770},{},[771],{"type":26,"value":772},"Unified formulation that “prompts” the robot to perform diverse tasks; highly scalable and sample-efficient.",{"type":21,"tag":97,"props":774,"children":775},{},[776],{"type":26,"value":777},"Primarily demonstrated in benchmark/simulated settings; real-world transfer may require additional adaptation.",{"type":21,"tag":44,"props":779,"children":780},{},[781,803,808,813,818],{"type":21,"tag":97,"props":782,"children":783},{},[784,786,791,792,795,796,802],{"type":26,"value":785},"16. ",{"type":21,"tag":52,"props":787,"children":788},{},[789],{"type":26,"value":790},"SPOC",{"type":26,"value":108},{"type":21,"tag":110,"props":793,"children":794},{},[],{"type":26,"value":114},{"type":21,"tag":116,"props":797,"children":800},{"href":798,"rel":799},"https://spoc-robot.github.io/",[120],[801],{"type":26,"value":170},{"type":26,"value":125},{"type":21,"tag":97,"props":804,"children":805},{},[806],{"type":26,"value":807},"Imitation learning for long-horizon navigation and manipulation using shortest path imitation (trained in simulation, deployed in the real world).",{"type":21,"tag":97,"props":809,"children":810},{},[811],{"type":26,"value":812},"Trained with RGB-only inputs in simulation; demonstrated on real robots for tasks such as object fetching and navigation.",{"type":21,"tag":97,"props":814,"children":815},{},[816],{"type":26,"value":817},"Robust long-horizon planning; effective sim-to-real transfer with minimal sensing (RGB only); no need for depth or privileged info.",{"type":21,"tag":97,"props":819,"children":820},{},[821],{"type":26,"value":822},"RGB-only perception can limit object recognition; some failure cases persist in challenging real-world scenarios.",{"type":21,"tag":824,"props":825,"children":826},"hr",{},[],{"type":21,"tag":29,"props":828,"children":830},{"id":829},"detailed-comparison",[831],{"type":26,"value":832},"Detailed Comparison",{"type":21,"tag":834,"props":835,"children":837},"h3",{"id":836},"_1-lerobot",[838],{"type":26,"value":839},"1. LeRobot",{"type":21,"tag":22,"props":841,"children":842},{},[843,848,851],{"type":21,"tag":52,"props":844,"children":845},{},[846],{"type":26,"value":847},"Scope & Application:",{"type":21,"tag":110,"props":849,"children":850},{},[],{"type":26,"value":852},"\nLeRobot is designed to lower the barrier for robotics research by providing an end-to-end learning framework with integrated pretrained models, diverse datasets, and simulation environments. It is well suited for imitation and reinforcement learning research on both simulated and real robots.",{"type":21,"tag":22,"props":854,"children":855},{},[856],{"type":21,"tag":52,"props":857,"children":858},{},[859],{"type":26,"value":860},"Technical Features:",{"type":21,"tag":862,"props":863,"children":864},"ul",{},[865,871],{"type":21,"tag":866,"props":867,"children":868},"li",{},[869],{"type":26,"value":870},"Built in PyTorch with modular dataset classes that support multi-frame temporal sampling.",{"type":21,"tag":866,"props":872,"children":873},{},[874],{"type":26,"value":875},"Offers pretrained policies (e.g. ACT, Diffusion, TDMPC) and supports various robot platforms and environments.",{"type":21,"tag":22,"props":877,"children":878},{},[879],{"type":21,"tag":52,"props":880,"children":881},{},[882],{"type":26,"value":883},"Advantages:",{"type":21,"tag":862,"props":885,"children":886},{},[887,892],{"type":21,"tag":866,"props":888,"children":889},{},[890],{"type":26,"value":891},"Community-driven with active contributions and hosted on Hugging Face.",{"type":21,"tag":866,"props":893,"children":894},{},[895],{"type":26,"value":896},"Facilitates rapid prototyping in robotics with an accessible codebase.",{"type":21,"tag":22,"props":898,"children":899},{},[900],{"type":21,"tag":52,"props":901,"children":902},{},[903],{"type":26,"value":904},"Disadvantages:",{"type":21,"tag":862,"props":906,"children":907},{},[908],{"type":21,"tag":866,"props":909,"children":910},{},[911],{"type":26,"value":912},"Complexity in data handling (various sensor streams and temporal dynamics) can demand significant compute and expertise.",{"type":21,"tag":824,"props":914,"children":915},{},[],{"type":21,"tag":834,"props":917,"children":919},{"id":918},"_2-open-x-embodiment",[920],{"type":26,"value":921},"2. Open X-Embodiment",{"type":21,"tag":22,"props":923,"children":924},{},[925,929,932],{"type":21,"tag":52,"props":926,"children":927},{},[928],{"type":26,"value":847},{"type":21,"tag":110,"props":930,"children":931},{},[],{"type":26,"value":933},"\nA collaborative effort pooling robot data from 21 institutions, it is aimed at training “generalist” policies across 22 different robot embodiments.",{"type":21,"tag":22,"props":935,"children":936},{},[937],{"type":21,"tag":52,"props":938,"children":939},{},[940],{"type":26,"value":860},{"type":21,"tag":862,"props":942,"children":943},{},[944,949],{"type":21,"tag":866,"props":945,"children":946},{},[947],{"type":26,"value":948},"Aggregates 1M+ trajectories from diverse robots and tasks.",{"type":21,"tag":866,"props":950,"children":951},{},[952],{"type":26,"value":953},"Supports learning via transformer-based architectures that can generalize across different embodiments.",{"type":21,"tag":22,"props":955,"children":956},{},[957],{"type":21,"tag":52,"props":958,"children":959},{},[960],{"type":26,"value":883},{"type":21,"tag":862,"props":962,"children":963},{},[964,969],{"type":21,"tag":866,"props":965,"children":966},{},[967],{"type":26,"value":968},"Unmatched diversity, which is ideal for studying cross-robot transfer.",{"type":21,"tag":866,"props":970,"children":971},{},[972],{"type":26,"value":973},"Large scale increases the potential for generalization.",{"type":21,"tag":22,"props":975,"children":976},{},[977],{"type":21,"tag":52,"props":978,"children":979},{},[980],{"type":26,"value":904},{"type":21,"tag":862,"props":982,"children":983},{},[984],{"type":21,"tag":866,"props":985,"children":986},{},[987],{"type":26,"value":988},"The heterogeneity of data can introduce inconsistencies; standardizing varied datasets is challenging.",{"type":21,"tag":824,"props":990,"children":991},{},[],{"type":21,"tag":834,"props":993,"children":995},{"id":994},"_3-droid",[996],{"type":26,"value":997},"3. DROID",{"type":21,"tag":22,"props":999,"children":1000},{},[1001,1005,1008],{"type":21,"tag":52,"props":1002,"children":1003},{},[1004],{"type":26,"value":847},{"type":21,"tag":110,"props":1006,"children":1007},{},[],{"type":26,"value":1009},"\nFocused on in-the-wild robot manipulation, DROID offers a vast dataset for robust imitation learning using Franka Panda robots.",{"type":21,"tag":22,"props":1011,"children":1012},{},[1013],{"type":21,"tag":52,"props":1014,"children":1015},{},[1016],{"type":26,"value":860},{"type":21,"tag":862,"props":1018,"children":1019},{},[1020,1025],{"type":21,"tag":866,"props":1021,"children":1022},{},[1023],{"type":26,"value":1024},"Contains 76K trajectories (~350 hours) across 564 scenes and 86 tasks.",{"type":21,"tag":866,"props":1026,"children":1027},{},[1028],{"type":26,"value":1029},"Multi-camera views (including wrist and exterior images) enable rich visual inputs.",{"type":21,"tag":22,"props":1031,"children":1032},{},[1033],{"type":21,"tag":52,"props":1034,"children":1035},{},[1036],{"type":26,"value":883},{"type":21,"tag":862,"props":1038,"children":1039},{},[1040,1045],{"type":21,"tag":866,"props":1041,"children":1042},{},[1043],{"type":26,"value":1044},"Large, diverse dataset that significantly boosts policy performance and robustness.",{"type":21,"tag":866,"props":1046,"children":1047},{},[1048],{"type":26,"value":1049},"Extensive coverage of real-world scenarios.",{"type":21,"tag":22,"props":1051,"children":1052},{},[1053],{"type":21,"tag":52,"props":1054,"children":1055},{},[1056],{"type":26,"value":904},{"type":21,"tag":862,"props":1058,"children":1059},{},[1060],{"type":21,"tag":866,"props":1061,"children":1062},{},[1063],{"type":26,"value":1064},"Being collected with a specific hardware platform, its applicability to other robots may be limited.",{"type":21,"tag":824,"props":1066,"children":1067},{},[],{"type":21,"tag":834,"props":1069,"children":1071},{"id":1070},"_4-roboturk",[1072],{"type":26,"value":1073},"4. RoboTurk",{"type":21,"tag":22,"props":1075,"children":1076},{},[1077,1081,1084],{"type":21,"tag":52,"props":1078,"children":1079},{},[1080],{"type":26,"value":847},{"type":21,"tag":110,"props":1082,"children":1083},{},[],{"type":26,"value":1085},"\nRoboTurk is a crowdsourcing platform that leverages teleoperation for collecting human demonstrations on both simulated and real robotic tasks.",{"type":21,"tag":22,"props":1087,"children":1088},{},[1089],{"type":21,"tag":52,"props":1090,"children":1091},{},[1092],{"type":26,"value":860},{"type":21,"tag":862,"props":1094,"children":1095},{},[1096,1101],{"type":21,"tag":866,"props":1097,"children":1098},{},[1099],{"type":26,"value":1100},"Provides datasets with hundreds to thousands of successful demonstrations (e.g. pilot dataset and real-world dataset).",{"type":21,"tag":866,"props":1102,"children":1103},{},[1104],{"type":26,"value":1105},"Includes system features for low-latency teleoperation and human-in-the-loop interventions.",{"type":21,"tag":22,"props":1107,"children":1108},{},[1109],{"type":21,"tag":52,"props":1110,"children":1111},{},[1112],{"type":26,"value":883},{"type":21,"tag":862,"props":1114,"children":1115},{},[1116,1121],{"type":21,"tag":866,"props":1117,"children":1118},{},[1119],{"type":26,"value":1120},"Enables scalable data collection from non-experts, lowering the cost of obtaining rich demonstrations.",{"type":21,"tag":866,"props":1122,"children":1123},{},[1124],{"type":26,"value":1125},"Proven effectiveness in enabling imitation learning on challenging tasks.",{"type":21,"tag":22,"props":1127,"children":1128},{},[1129],{"type":21,"tag":52,"props":1130,"children":1131},{},[1132],{"type":26,"value":904},{"type":21,"tag":862,"props":1134,"children":1135},{},[1136],{"type":21,"tag":866,"props":1137,"children":1138},{},[1139],{"type":26,"value":1140},"The quality of demonstrations may vary due to differences in human teleoperation skills.",{"type":21,"tag":824,"props":1142,"children":1143},{},[],{"type":21,"tag":834,"props":1145,"children":1147},{"id":1146},"_5-mime",[1148],{"type":26,"value":1149},"5. MIME",{"type":21,"tag":22,"props":1151,"children":1152},{},[1153,1157,1160],{"type":21,"tag":52,"props":1154,"children":1155},{},[1156],{"type":26,"value":847},{"type":21,"tag":110,"props":1158,"children":1159},{},[],{"type":26,"value":1161},"\nMIME targets imitation learning for manipulation, offering human demonstrations that capture complex manipulation behaviors.",{"type":21,"tag":22,"props":1163,"children":1164},{},[1165],{"type":21,"tag":52,"props":1166,"children":1167},{},[1168],{"type":26,"value":860},{"type":21,"tag":862,"props":1170,"children":1171},{},[1172],{"type":21,"tag":866,"props":1173,"children":1174},{},[1175],{"type":26,"value":1176},"Multi-modal data including visual inputs and robot state/action trajectories collected through teleoperation.",{"type":21,"tag":22,"props":1178,"children":1179},{},[1180],{"type":21,"tag":52,"props":1181,"children":1182},{},[1183],{"type":26,"value":883},{"type":21,"tag":862,"props":1185,"children":1186},{},[1187],{"type":21,"tag":866,"props":1188,"children":1189},{},[1190],{"type":26,"value":1191},"Focused on detailed manipulation tasks, making it ideal for imitation learning studies.",{"type":21,"tag":22,"props":1193,"children":1194},{},[1195],{"type":21,"tag":52,"props":1196,"children":1197},{},[1198],{"type":26,"value":904},{"type":21,"tag":862,"props":1200,"children":1201},{},[1202],{"type":21,"tag":866,"props":1203,"children":1204},{},[1205],{"type":26,"value":1206},"Generally smaller in scale compared to some of the largest datasets; might offer limited diversity.",{"type":21,"tag":824,"props":1208,"children":1209},{},[],{"type":21,"tag":834,"props":1211,"children":1213},{"id":1212},"_6-meta-world",[1214],{"type":26,"value":1215},"6. Meta-World",{"type":21,"tag":22,"props":1217,"children":1218},{},[1219,1223,1226],{"type":21,"tag":52,"props":1220,"children":1221},{},[1222],{"type":26,"value":847},{"type":21,"tag":110,"props":1224,"children":1225},{},[],{"type":26,"value":1227},"\nA simulation benchmark intended for meta-reinforcement learning and multi-task learning, Meta-World comprises 50 distinct manipulation environments.",{"type":21,"tag":22,"props":1229,"children":1230},{},[1231],{"type":21,"tag":52,"props":1232,"children":1233},{},[1234],{"type":26,"value":860},{"type":21,"tag":862,"props":1236,"children":1237},{},[1238],{"type":21,"tag":866,"props":1239,"children":1240},{},[1241],{"type":26,"value":1242},"Structured environments with varying goal positions and task variations to test generalization.",{"type":21,"tag":22,"props":1244,"children":1245},{},[1246],{"type":21,"tag":52,"props":1247,"children":1248},{},[1249],{"type":26,"value":883},{"type":21,"tag":862,"props":1251,"children":1252},{},[1253],{"type":21,"tag":866,"props":1254,"children":1255},{},[1256],{"type":26,"value":1257},"Standardized and well-documented benchmark that is widely used for evaluating meta-RL algorithms.",{"type":21,"tag":22,"props":1259,"children":1260},{},[1261],{"type":21,"tag":52,"props":1262,"children":1263},{},[1264],{"type":26,"value":904},{"type":21,"tag":862,"props":1266,"children":1267},{},[1268],{"type":21,"tag":866,"props":1269,"children":1270},{},[1271],{"type":26,"value":1272},"Limited to simulated settings; real-world complexities (e.g. sensor noise, dynamics variations) are not fully captured.",{"type":21,"tag":824,"props":1274,"children":1275},{},[],{"type":21,"tag":834,"props":1277,"children":1279},{"id":1278},"_7-robonet",[1280],{"type":26,"value":1281},"7. RoboNet",{"type":21,"tag":22,"props":1283,"children":1284},{},[1285,1289,1292],{"type":21,"tag":52,"props":1286,"children":1287},{},[1288],{"type":26,"value":847},{"type":21,"tag":110,"props":1290,"children":1291},{},[],{"type":26,"value":1293},"\nRoboNet is an open database of robotic experience collected from 7 different robot platforms, with an emphasis on visual data for manipulation.",{"type":21,"tag":22,"props":1295,"children":1296},{},[1297],{"type":21,"tag":52,"props":1298,"children":1299},{},[1300],{"type":26,"value":860},{"type":21,"tag":862,"props":1302,"children":1303},{},[1304],{"type":21,"tag":866,"props":1305,"children":1306},{},[1307],{"type":26,"value":1308},"Contains over 15M video frames and data from multiple camera viewpoints.",{"type":21,"tag":22,"props":1310,"children":1311},{},[1312],{"type":21,"tag":52,"props":1313,"children":1314},{},[1315],{"type":26,"value":883},{"type":21,"tag":862,"props":1317,"children":1318},{},[1319],{"type":21,"tag":866,"props":1320,"children":1321},{},[1322],{"type":26,"value":1323},"Offers vast amounts of real-world data to study generalization across different robot hardware.",{"type":21,"tag":22,"props":1325,"children":1326},{},[1327],{"type":21,"tag":52,"props":1328,"children":1329},{},[1330],{"type":26,"value":904},{"type":21,"tag":862,"props":1332,"children":1333},{},[1334],{"type":21,"tag":866,"props":1335,"children":1336},{},[1337],{"type":26,"value":1338},"Requires heavy storage and processing; integrating multi-platform data can be challenging.",{"type":21,"tag":824,"props":1340,"children":1341},{},[],{"type":21,"tag":834,"props":1343,"children":1345},{"id":1344},"_8-roboset",[1346],{"type":26,"value":1347},"8. RoboSet",{"type":21,"tag":22,"props":1349,"children":1350},{},[1351,1355,1358],{"type":21,"tag":52,"props":1352,"children":1353},{},[1354],{"type":26,"value":847},{"type":21,"tag":110,"props":1356,"children":1357},{},[],{"type":26,"value":1359},"\nA dataset focused on household (kitchen) manipulation tasks, RoboSet provides both kinesthetic and teleoperated demonstrations with language instructions.",{"type":21,"tag":22,"props":1361,"children":1362},{},[1363],{"type":21,"tag":52,"props":1364,"children":1365},{},[1366],{"type":26,"value":860},{"type":21,"tag":862,"props":1368,"children":1369},{},[1370],{"type":21,"tag":866,"props":1371,"children":1372},{},[1373],{"type":26,"value":1374},"28,500 trajectories captured with 4 camera views per frame; tasks are semantically grouped.",{"type":21,"tag":22,"props":1376,"children":1377},{},[1378],{"type":21,"tag":52,"props":1379,"children":1380},{},[1381],{"type":26,"value":883},{"type":21,"tag":862,"props":1383,"children":1384},{},[1385],{"type":21,"tag":866,"props":1386,"children":1387},{},[1388],{"type":26,"value":1389},"Rich multi-modal information (visual + language) supports language-guided robotic learning.",{"type":21,"tag":22,"props":1391,"children":1392},{},[1393],{"type":21,"tag":52,"props":1394,"children":1395},{},[1396],{"type":26,"value":904},{"type":21,"tag":862,"props":1398,"children":1399},{},[1400],{"type":21,"tag":866,"props":1401,"children":1402},{},[1403],{"type":26,"value":1404},"Domain-specific to kitchen and household scenes; may not generalize to industrial or outdoor scenarios.",{"type":21,"tag":824,"props":1406,"children":1407},{},[],{"type":21,"tag":834,"props":1409,"children":1411},{"id":1410},"_9-bridgedata-v2",[1412],{"type":26,"value":1413},"9. BridgeData V2",{"type":21,"tag":22,"props":1415,"children":1416},{},[1417,1421,1424],{"type":21,"tag":52,"props":1418,"children":1419},{},[1420],{"type":26,"value":847},{"type":21,"tag":110,"props":1422,"children":1423},{},[],{"type":26,"value":1425},"\nDesigned to boost generalization in robotic skills, BridgeData V2 spans 24 environments and 13 skills, with natural language annotations for goal conditioning.",{"type":21,"tag":22,"props":1427,"children":1428},{},[1429],{"type":21,"tag":52,"props":1430,"children":1431},{},[1432],{"type":26,"value":860},{"type":21,"tag":862,"props":1434,"children":1435},{},[1436,1441],{"type":21,"tag":866,"props":1437,"children":1438},{},[1439],{"type":26,"value":1440},"Approximately 60K trajectories with multi-view RGB (and some depth) data.",{"type":21,"tag":866,"props":1442,"children":1443},{},[1444],{"type":26,"value":1445},"Includes both teleoperated and scripted demonstrations.",{"type":21,"tag":22,"props":1447,"children":1448},{},[1449],{"type":21,"tag":52,"props":1450,"children":1451},{},[1452],{"type":26,"value":883},{"type":21,"tag":862,"props":1454,"children":1455},{},[1456],{"type":21,"tag":866,"props":1457,"children":1458},{},[1459],{"type":26,"value":1460},"High diversity in environments and tasks; strong support for language-conditioned policy learning.",{"type":21,"tag":22,"props":1462,"children":1463},{},[1464],{"type":21,"tag":52,"props":1465,"children":1466},{},[1467],{"type":26,"value":904},{"type":21,"tag":862,"props":1469,"children":1470},{},[1471],{"type":21,"tag":866,"props":1472,"children":1473},{},[1474],{"type":26,"value":1475},"Often tied to a particular hardware setup (e.g. WidowX 250), and the multi-view setup can complicate data preprocessing.",{"type":21,"tag":824,"props":1477,"children":1478},{},[],{"type":21,"tag":834,"props":1480,"children":1482},{"id":1481},"_10-rt-1",[1483],{"type":26,"value":1484},"10. RT-1",{"type":21,"tag":22,"props":1486,"children":1487},{},[1488,1492,1495],{"type":21,"tag":52,"props":1489,"children":1490},{},[1491],{"type":26,"value":847},{"type":21,"tag":110,"props":1493,"children":1494},{},[],{"type":26,"value":1496},"\nRT-1 is a state-of-the-art transformer-based model for real-world robotic control trained on a massive dataset of diverse tasks.",{"type":21,"tag":22,"props":1498,"children":1499},{},[1500],{"type":21,"tag":52,"props":1501,"children":1502},{},[1503],{"type":26,"value":860},{"type":21,"tag":862,"props":1505,"children":1506},{},[1507,1512],{"type":21,"tag":866,"props":1508,"children":1509},{},[1510],{"type":26,"value":1511},"Over 130K episodes covering more than 700 tasks collected from 13 robots.",{"type":21,"tag":866,"props":1513,"children":1514},{},[1515],{"type":26,"value":1516},"Utilizes vision and natural language inputs to produce discretized action tokens.",{"type":21,"tag":22,"props":1518,"children":1519},{},[1520],{"type":21,"tag":52,"props":1521,"children":1522},{},[1523],{"type":26,"value":883},{"type":21,"tag":862,"props":1525,"children":1526},{},[1527,1532],{"type":21,"tag":866,"props":1528,"children":1529},{},[1530],{"type":26,"value":1531},"Demonstrates superior performance and generalization, including sim-to-real transfer.",{"type":21,"tag":866,"props":1533,"children":1534},{},[1535],{"type":26,"value":1536},"Scalability through high-capacity transformer models.",{"type":21,"tag":22,"props":1538,"children":1539},{},[1540],{"type":21,"tag":52,"props":1541,"children":1542},{},[1543],{"type":26,"value":904},{"type":21,"tag":862,"props":1545,"children":1546},{},[1547],{"type":21,"tag":866,"props":1548,"children":1549},{},[1550],{"type":26,"value":1551},"Demands extensive data, compute, and engineering expertise; system complexity is high.",{"type":21,"tag":824,"props":1553,"children":1554},{},[],{"type":21,"tag":834,"props":1556,"children":1558},{"id":1557},"_11-dobbe",[1559],{"type":26,"value":1560},"11. Dobb·E",{"type":21,"tag":22,"props":1562,"children":1563},{},[1564,1568,1571],{"type":21,"tag":52,"props":1565,"children":1566},{},[1567],{"type":26,"value":847},{"type":21,"tag":110,"props":1569,"children":1570},{},[],{"type":26,"value":1572},"\nDobb·E focuses on home robotics, providing a full stack (hardware, dataset, models) for learning household manipulation tasks with minimal demonstration time.",{"type":21,"tag":22,"props":1574,"children":1575},{},[1576],{"type":21,"tag":52,"props":1577,"children":1578},{},[1579],{"type":26,"value":860},{"type":21,"tag":862,"props":1581,"children":1582},{},[1583,1588],{"type":21,"tag":866,"props":1584,"children":1585},{},[1586],{"type":26,"value":1587},"“HoNY” dataset includes 13 hours of data from 22 New York City homes (5,620 trajectories, RGB + depth at 30 fps).",{"type":21,"tag":866,"props":1589,"children":1590},{},[1591],{"type":26,"value":1592},"Includes a low-cost hardware “Stick” for demonstration collection.",{"type":21,"tag":22,"props":1594,"children":1595},{},[1596],{"type":21,"tag":52,"props":1597,"children":1598},{},[1599],{"type":26,"value":883},{"type":21,"tag":862,"props":1601,"children":1602},{},[1603,1608],{"type":21,"tag":866,"props":1604,"children":1605},{},[1606],{"type":26,"value":1607},"Cost-effective and designed for rapid task learning in domestic environments.",{"type":21,"tag":866,"props":1609,"children":1610},{},[1611],{"type":26,"value":1612},"Demonstrates strong real-world applicability in home settings.",{"type":21,"tag":22,"props":1614,"children":1615},{},[1616],{"type":21,"tag":52,"props":1617,"children":1618},{},[1619],{"type":26,"value":904},{"type":21,"tag":862,"props":1621,"children":1622},{},[1623],{"type":21,"tag":866,"props":1624,"children":1625},{},[1626],{"type":26,"value":1627},"Domain-specific and may not translate to other application areas; non-expert demonstrations can introduce variability.",{"type":21,"tag":824,"props":1629,"children":1630},{},[],{"type":21,"tag":834,"props":1632,"children":1634},{"id":1633},"_12-rh20t",[1635],{"type":26,"value":1636},"12. RH20T",{"type":21,"tag":22,"props":1638,"children":1639},{},[1640,1644,1647],{"type":21,"tag":52,"props":1641,"children":1642},{},[1643],{"type":26,"value":847},{"type":21,"tag":110,"props":1645,"children":1646},{},[],{"type":26,"value":1648},"\nRH20T is a comprehensive dataset aimed at learning diverse, contact-rich manipulation skills with extensive multi-modal sensor information.",{"type":21,"tag":22,"props":1650,"children":1651},{},[1652],{"type":21,"tag":52,"props":1653,"children":1654},{},[1655],{"type":26,"value":860},{"type":21,"tag":862,"props":1657,"children":1658},{},[1659,1664],{"type":21,"tag":866,"props":1660,"children":1661},{},[1662],{"type":26,"value":1663},"Contains millions of demonstration pairs with modalities including high-resolution RGB, depth, force/torque, audio, and tactile sensing.",{"type":21,"tag":866,"props":1665,"children":1666},{},[1667],{"type":26,"value":1668},"Detailed synchronization and calibration across multiple sensors.",{"type":21,"tag":22,"props":1670,"children":1671},{},[1672],{"type":21,"tag":52,"props":1673,"children":1674},{},[1675],{"type":26,"value":883},{"type":21,"tag":862,"props":1677,"children":1678},{},[1679,1684],{"type":21,"tag":866,"props":1680,"children":1681},{},[1682],{"type":26,"value":1683},"Extremely rich and diverse data ideal for advancing one-shot imitation learning and fine-grained sensor fusion.",{"type":21,"tag":866,"props":1685,"children":1686},{},[1687],{"type":26,"value":1688},"Supports research on contact-rich and dexterous manipulation.",{"type":21,"tag":22,"props":1690,"children":1691},{},[1692],{"type":21,"tag":52,"props":1693,"children":1694},{},[1695],{"type":26,"value":904},{"type":21,"tag":862,"props":1697,"children":1698},{},[1699],{"type":21,"tag":866,"props":1700,"children":1701},{},[1702],{"type":26,"value":1703},"Enormous data volume makes it challenging to store, process, and analyze; high complexity in data format and licensing.",{"type":21,"tag":824,"props":1705,"children":1706},{},[],{"type":21,"tag":834,"props":1708,"children":1710},{"id":1709},"_13-bc-z",[1711],{"type":26,"value":1712},"13. BC-Z",{"type":21,"tag":22,"props":1714,"children":1715},{},[1716,1720,1723],{"type":21,"tag":52,"props":1717,"children":1718},{},[1719],{"type":26,"value":847},{"type":21,"tag":110,"props":1721,"children":1722},{},[],{"type":26,"value":1724},"\nBC-Z is targeted at behavior cloning for robotic manipulation, providing a large-scale dataset that is useful as a benchmark for imitation learning approaches.",{"type":21,"tag":22,"props":1726,"children":1727},{},[1728],{"type":21,"tag":52,"props":1729,"children":1730},{},[1731],{"type":26,"value":860},{"type":21,"tag":862,"props":1733,"children":1734},{},[1735],{"type":21,"tag":866,"props":1736,"children":1737},{},[1738],{"type":26,"value":1739},"Although details are less extensively documented online, BC-Z is positioned alongside other large imitation learning datasets.",{"type":21,"tag":22,"props":1741,"children":1742},{},[1743],{"type":21,"tag":52,"props":1744,"children":1745},{},[1746],{"type":26,"value":883},{"type":21,"tag":862,"props":1748,"children":1749},{},[1750],{"type":21,"tag":866,"props":1751,"children":1752},{},[1753],{"type":26,"value":1754},"Serves as a standardized resource for evaluating behavior cloning algorithms.",{"type":21,"tag":22,"props":1756,"children":1757},{},[1758],{"type":21,"tag":52,"props":1759,"children":1760},{},[1761],{"type":26,"value":904},{"type":21,"tag":862,"props":1763,"children":1764},{},[1765],{"type":21,"tag":866,"props":1766,"children":1767},{},[1768],{"type":26,"value":1769},"May not offer as much diversity or multi-modal richness as some of the larger, more comprehensive datasets.",{"type":21,"tag":824,"props":1771,"children":1772},{},[],{"type":21,"tag":834,"props":1774,"children":1776},{"id":1775},"_14-mt-opt",[1777],{"type":26,"value":1778},"14. MT-Opt",{"type":21,"tag":22,"props":1780,"children":1781},{},[1782,1786,1789],{"type":21,"tag":52,"props":1783,"children":1784},{},[1785],{"type":26,"value":847},{"type":21,"tag":110,"props":1787,"children":1788},{},[],{"type":26,"value":1790},"\nMT-Opt is a framework for continuous multi-task reinforcement learning designed to learn a wide repertoire of manipulation skills concurrently.",{"type":21,"tag":22,"props":1792,"children":1793},{},[1794],{"type":21,"tag":52,"props":1795,"children":1796},{},[1797],{"type":26,"value":860},{"type":21,"tag":862,"props":1799,"children":1800},{},[1801],{"type":21,"tag":866,"props":1802,"children":1803},{},[1804],{"type":26,"value":1805},"Built on data collected from 7 robots over 9,600 hours, spanning 12 tasks with a scalable RL method.",{"type":21,"tag":22,"props":1807,"children":1808},{},[1809],{"type":21,"tag":52,"props":1810,"children":1811},{},[1812],{"type":26,"value":883},{"type":21,"tag":862,"props":1814,"children":1815},{},[1816,1821],{"type":21,"tag":866,"props":1817,"children":1818},{},[1819],{"type":26,"value":1820},"Effective at sharing experience across tasks, significantly boosting performance on rare tasks.",{"type":21,"tag":866,"props":1822,"children":1823},{},[1824],{"type":26,"value":1825},"Demonstrates both zero-shot and rapid fine-tuning capabilities.",{"type":21,"tag":22,"props":1827,"children":1828},{},[1829],{"type":21,"tag":52,"props":1830,"children":1831},{},[1832],{"type":26,"value":904},{"type":21,"tag":862,"props":1834,"children":1835},{},[1836],{"type":21,"tag":866,"props":1837,"children":1838},{},[1839],{"type":26,"value":1840},"Requires large-scale robotic infrastructure and sophisticated multi-task training pipelines.",{"type":21,"tag":824,"props":1842,"children":1843},{},[],{"type":21,"tag":834,"props":1845,"children":1847},{"id":1846},"_15-vima",[1848],{"type":26,"value":1849},"15. VIMA",{"type":21,"tag":22,"props":1851,"children":1852},{},[1853,1857,1860],{"type":21,"tag":52,"props":1854,"children":1855},{},[1856],{"type":26,"value":847},{"type":21,"tag":110,"props":1858,"children":1859},{},[],{"type":26,"value":1861},"\nVIMA presents a novel formulation in which diverse robot manipulation tasks are “prompted” via interleaved language and visual tokens, unifying task specification.",{"type":21,"tag":22,"props":1863,"children":1864},{},[1865],{"type":21,"tag":52,"props":1866,"children":1867},{},[1868],{"type":26,"value":860},{"type":21,"tag":862,"props":1870,"children":1871},{},[1872],{"type":21,"tag":866,"props":1873,"children":1874},{},[1875],{"type":26,"value":1876},"Transformer-based model that leverages multimodal prompts; benchmark includes thousands of procedurally generated tabletop task instances.",{"type":21,"tag":22,"props":1878,"children":1879},{},[1880],{"type":21,"tag":52,"props":1881,"children":1882},{},[1883],{"type":26,"value":883},{"type":21,"tag":862,"props":1885,"children":1886},{},[1887,1892],{"type":21,"tag":866,"props":1888,"children":1889},{},[1890],{"type":26,"value":1891},"Unified, scalable approach that achieves strong zero-shot generalization and high sample efficiency.",{"type":21,"tag":866,"props":1893,"children":1894},{},[1895],{"type":26,"value":1896},"Allows integration of various forms of task instructions (text + image).",{"type":21,"tag":22,"props":1898,"children":1899},{},[1900],{"type":21,"tag":52,"props":1901,"children":1902},{},[1903],{"type":26,"value":904},{"type":21,"tag":862,"props":1905,"children":1906},{},[1907],{"type":21,"tag":866,"props":1908,"children":1909},{},[1910],{"type":26,"value":1911},"Largely demonstrated in controlled (often simulated or tabletop) settings; additional work may be needed for full real-world deployment.",{"type":21,"tag":824,"props":1913,"children":1914},{},[],{"type":21,"tag":834,"props":1916,"children":1918},{"id":1917},"_16-spoc",[1919],{"type":26,"value":1920},"16. SPOC",{"type":21,"tag":22,"props":1922,"children":1923},{},[1924,1928,1931],{"type":21,"tag":52,"props":1925,"children":1926},{},[1927],{"type":26,"value":847},{"type":21,"tag":110,"props":1929,"children":1930},{},[],{"type":26,"value":1932},"\nSPOC focuses on long-horizon navigation and manipulation by imitating shortest paths. Trained entirely in simulation (using RGB-only inputs), it is deployed in the real world without extra sim-to-real adaptation.",{"type":21,"tag":22,"props":1934,"children":1935},{},[1936],{"type":21,"tag":52,"props":1937,"children":1938},{},[1939],{"type":26,"value":860},{"type":21,"tag":862,"props":1941,"children":1942},{},[1943,1948],{"type":21,"tag":866,"props":1944,"children":1945},{},[1946],{"type":26,"value":1947},"Uses a transformer-based action decoder conditioned on language instructions and sequential RGB frames.",{"type":21,"tag":866,"props":1949,"children":1950},{},[1951],{"type":26,"value":1952},"Emphasizes a minimalist sensory setup (RGB only) to drive exploration and task completion.",{"type":21,"tag":22,"props":1954,"children":1955},{},[1956],{"type":21,"tag":52,"props":1957,"children":1958},{},[1959],{"type":26,"value":883},{"type":21,"tag":862,"props":1961,"children":1962},{},[1963,1968],{"type":21,"tag":866,"props":1964,"children":1965},{},[1966],{"type":26,"value":1967},"Achieves robust long-horizon planning and recovery in real-world tasks despite minimal input modalities.",{"type":21,"tag":866,"props":1969,"children":1970},{},[1971],{"type":26,"value":1972},"Trains entirely in simulation and transfers effectively.",{"type":21,"tag":22,"props":1974,"children":1975},{},[1976],{"type":21,"tag":52,"props":1977,"children":1978},{},[1979],{"type":26,"value":904},{"type":21,"tag":862,"props":1981,"children":1982},{},[1983],{"type":21,"tag":866,"props":1984,"children":1985},{},[1986],{"type":26,"value":1987},"RGB-only perception can limit object detection accuracy; some failure cases persist in complex or cluttered real-world scenarios.",{"title":13,"searchDepth":1989,"depth":1989,"links":1990},2,[1991,1992],{"id":31,"depth":1989,"text":34},{"id":829,"depth":1989,"text":832,"children":1993},[1994,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010],{"id":836,"depth":1995,"text":839},3,{"id":918,"depth":1995,"text":921},{"id":994,"depth":1995,"text":997},{"id":1070,"depth":1995,"text":1073},{"id":1146,"depth":1995,"text":1149},{"id":1212,"depth":1995,"text":1215},{"id":1278,"depth":1995,"text":1281},{"id":1344,"depth":1995,"text":1347},{"id":1410,"depth":1995,"text":1413},{"id":1481,"depth":1995,"text":1484},{"id":1557,"depth":1995,"text":1560},{"id":1633,"depth":1995,"text":1636},{"id":1709,"depth":1995,"text":1712},{"id":1775,"depth":1995,"text":1778},{"id":1846,"depth":1995,"text":1849},{"id":1917,"depth":1995,"text":1920},"markdown","content:5.model-dataset-comp:1.robotics-datasets.md","content","5.model-dataset-comp/1.robotics-datasets.md","5.model-dataset-comp/1.robotics-datasets","md","default",["ShallowRef",2019],["ShallowReactive",2020],{"/model-dataset-comp/robotics-datasets":2021},[2022,2031],{"_path":2023,"_dir":2024,"_draft":6,"_partial":6,"_locale":13,"title":2025,"description":2026,"icon":2027,"_type":2011,"_id":2028,"_source":2013,"_file":2029,"_stem":2030,"_extension":2016},"/published-research/hallucination","published-research","Hallucination Prevention in LLMs","Zero-Resource Hallucination Prevention for Large Language Models","lucide:brain","content:4.published-research:2.hallucination.md","4.published-research/2.hallucination.md","4.published-research/2.hallucination",{"_path":2032,"_dir":12,"_draft":6,"_partial":6,"_locale":13,"title":2033,"description":2034,"icon":2035,"_type":2011,"_id":2036,"_source":2013,"_file":2037,"_stem":2038,"_extension":2016},"/model-dataset-comp/robotics-models","General-Purpose Robot Models Analysis","Overview of recent works on general-purpose robot models, comparing key technical aspects and hardware/time requirements for training, fine-tuning, or distillation.","lucide:bot","content:5.model-dataset-comp:2.robotics-models.md","5.model-dataset-comp/2.robotics-models.md","5.model-dataset-comp/2.robotics-models",["ShallowRef",2040],{},[2042,2046,2053,2072,2080],{"title":2043,"_path":2044,"icon":2045},"Cyber Nachos","/cybernachos","lucide:hand",{"title":2047,"_path":2048,"children":2049},"Published Products","/published",[2050],{"title":2051,"_path":2052,"icon":2027},"Cyber Nachos GPT","/published/cybernachos-gpt",{"title":2054,"_path":2055,"children":2056},"Tutorial Isaac Lab","/tutorial-isaaclab",[2057,2060,2064,2068],{"title":2058,"_path":2059,"icon":2035},"Introduction","/tutorial-isaaclab/introduction",{"title":2061,"_path":2062,"icon":2063},"Installation Guide","/tutorial-isaaclab/installation","lucide:download",{"title":2065,"_path":2066,"icon":2067},"Getting Started","/tutorial-isaaclab/getting-started","lucide:flag",{"title":2069,"_path":2070,"icon":2071},"Enabling fluid simulation","/tutorial-isaaclab/enable-fluid","material-symbols:water-drop-outline",{"title":2073,"_path":2074,"children":2075},"Published Research Papers","/published-research",[2076,2079],{"title":2077,"_path":2078,"icon":2035},"Real-time Dexterous Telemanipulation","/published-research/real-time-dexterous",{"title":2025,"_path":2023,"icon":2027},{"title":2081,"_path":2082,"children":2083},"Model & Dataset Comparisons","/model-dataset-comp",[2084,2085],{"title":14,"_path":11,"icon":16},{"title":2033,"_path":2032,"icon":2035},["Map"],{"_priority":2088,"env":2092,"name":2093,"url":2094},{"name":2089,"env":2090,"url":2091},-10,-15,-3,"production","shadcn-docs-nuxt-starter","https://cybernachos.github.io",["Set"],["ShallowReactive",2097],{"XFVaCZk9MC":2098},null,1743201604563]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{mdc:{components:{prose:true,map:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},content:{locales:[],defaultLocale:"",integrity:1743201587050,experimental:{stripQueryParameters:false,advanceQuery:false,clientDB:false},respectPathCase:false,api:{baseURL:"/api/_content"},navigation:{fields:["icon","navBadges","navTruncate","badges","toc","sidebar","collapse","editLink","prevNext","breadcrumb","fullpage","layout"]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"ProseCodeInline",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:{theme:{default:"github-light",dark:"github-dark"},preload:["json","js","ts","html","css","vue","diff","shell","markdown","mdc","yaml","bash","ini","dotenv"]},wsUrl:"",documentDriven:{page:true,navigation:true,surround:true,globals:{},layoutFallbacks:["theme"],injectPage:true},host:"",trailingSlash:false,search:{indexed:true,ignoredTags:["script","style","pre"],filterQuery:{_draft:false,_partial:false},options:{fields:["title","content","titles"],storeFields:["title","content","titles"],searchOptions:{prefix:true,fuzzy:.2,boost:{title:4,content:2,titles:1}}}},contentHead:true,anchorLinks:{depth:4,exclude:[1]}},"nuxt-scripts":{version:"",defaultScriptOptions:{trigger:"onNuxtReady"}}},app:{baseURL:"/",buildId:"57dc7828-7b5d-44e7-97ea-45198066b06c",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>